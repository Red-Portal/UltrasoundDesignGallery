
@inproceedings{__,
  type = {Inproceedings}
}

@article{_autonomic_2015,
  title = {An Autonomic Performance Environment for Exascale},
  year = {2015},
  month = jul,
  volume = {2},
  journal = {Supercomput. Front. Innov.},
  number = {3}
}

@incollection{_fdivergences_,
  title = {F-Divergences},
  file = {/home/msca8h/Zotero/storage/4WEVGPFU/f-divergences.pdf}
}

@techreport{_openmp_2015,
  title = {{{OpenMP}} Application Programming Interface},
  year = {2015},
  month = nov,
  institution = {{OpenMP Architecture Review Board}},
  number = {Version 4.5}
}

@article{:/content/journals/10.1049/ip-rsn_19990255,
  title = {Improved Particle Filter for Nonlinear Problems},
  author = {Carpenter, J. and Clifford, P. and Fearnhead, P.},
  year = {1999},
  month = feb,
  volume = {146},
  pages = {2-7(5)},
  abstract = {The Kalman filter provides an effective solution to the linear Gaussian filtering problem. However where there is nonlinearity, either in the model specification or the observation process, other methods are required. Methods known generically as `particle filters' are considered. These include the condensation algorithm and the Bayesian bootstrap or sampling importance resampling (SIR) filter. These filters represent the posterior distribution of the state variables by a system of particles which evolves and adapts recursively as new information becomes available. In practice, large numbers of particles may be required to provide adequate approximations and for certain applications, after a sequence of updates, the particle system will often collapse to a single point. A method of monitoring the efficiency of these filters is introduced which provides a simple quantitative assessment of sample impoverishment and the authors show how to construct improved particle filters that are both structurally efficient in terms of preventing the collapse of the particle system and computationally efficient in their implementation. This is illustrated with the classic bearings-only tracking problem.},
  copyright = {\textcopyright{} IEE},
  journal = {IEE Proc. - Radar Sonar Navig.},
  keywords = {approximations,Bayesian bootstrap,bearings-only tracking problem,condensation algorithm,efficiency,improved particle filter,monitoring,nonlinear problems,posterior distribution,sample impoverishment,sampling importance resampling filter,state variables,updates},
  language = {English},
  number = {1}
}

@inproceedings{10.1007/978-3-030-00934-2_56,
  title = {High-Dimensional Bayesian Optimization of Personalized Cardiac Model Parameters via an Embedded Generative Model},
  booktitle = {Medical Image Computing and Computer Assisted Intervention \textendash{} {{MICCAI}} 2018},
  author = {Dhamala, Jwala and Ghimire, Sandesh and Sapp, John L. and Hor{\'a}{\v c}ek, B. Milan and Wang, Linwei},
  year = {2018},
  pages = {499--507},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {The estimation of patient-specific tissue properties in the form of model parameters is important for personalized physiological models. However, these tissue properties are spatially varying across the underlying anatomical model, presenting a significance challenge of high-dimensional (HD) optimization at the presence of limited measurement data. A common solution to reduce the dimension of the parameter space is to explicitly partition the anatomical mesh, either into a fixed small number of segments or a multi-scale hierarchy. This anatomy-based reduction of parameter space presents a fundamental bottleneck to parameter estimation, resulting in solutions that are either too low in resolution to reflect tissue heterogeneity, or too high in dimension to be reliably estimated within feasible computation. In this paper, we present a novel concept that embeds a generative variational auto-encoder (VAE) into the objective function of Bayesian optimization, providing an implicit low-dimensional (LD) search space that represents the generative code of the HD spatially-varying tissue properties. In addition, the VAE-encoded knowledge about the generative code is further used to guide the exploration of the search space. The presented method is applied to estimating tissue excitability in a cardiac electrophysiological model. Synthetic and real-data experiments demonstrate its ability to improve the accuracy of parameter estimation with more than 10x gain in efficiency.}
}

@article{10.1007/s11222-008-9110-y,
  title = {A Tutorial on Adaptive {{MCMC}}},
  author = {Andrieu, Christophe and Thoms, Johannes},
  year = {2008},
  month = dec,
  volume = {18},
  pages = {343--373},
  publisher = {{Kluwer Academic Publishers}},
  address = {{USA}},
  abstract = {We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.},
  issue_date = {December 2008},
  journal = {Stat. Comput.},
  keywords = {Adaptive MCMC,Controlled Markov chain,MCMC,Stochastic approximation},
  number = {4}
}

@article{10.1007/s11222-018-9809-3,
  title = {{{GPU}}-{{Accelerated}} Gibbs Sampling: {{A}} Case Study of the Horseshoe Probit Model},
  author = {Terenin, Alexander and Dong, Shawfeng and Draper, David},
  year = {2019},
  month = mar,
  volume = {29},
  pages = {301--310},
  publisher = {{Kluwer Academic Publishers}},
  address = {{USA}},
  issue_date = {March 2019},
  journal = {Stat. Comput.},
  keywords = {Bayesian generalized linear models,Big data,Graphics processing units,High-dimensional statistical modeling,Markov chain Monte Carlo,Parallel computing},
  number = {2}
}

@article{10.1023/A:1020711129064,
  title = {Conditional Simulation from Highly Structured Gaussian Systems, with Application to Blocking-Mcmc for the Bayesian Analysis of Very Large Linear Models},
  author = {Wilkinson, Darren J. and Yeung, Stephen K. H.},
  year = {2002},
  month = jul,
  volume = {12},
  pages = {287--300},
  publisher = {{Kluwer Academic Publishers}},
  address = {{USA}},
  issue_date = {July 2002},
  journal = {Stat. Comput.},
  keywords = {block sampling,DAG propagation,linear Bayes models,local computation,nested hierarchical random effects},
  number = {3}
}

@inproceedings{10.1145/1102351.1102369,
  title = {Preference Learning with Gaussian Processes},
  booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
  author = {Chu, Wei and Ghahramani, Zoubin},
  year = {2005},
  pages = {137--144},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  series = {{{ICML}} '05}
}

@article{10.1145/2010324.1964963,
  title = {Local {{Laplacian}} Filters: {{Edge}}-Aware Image Processing with a {{Laplacian}} Pyramid},
  author = {Paris, Sylvain and Hasinoff, Samuel W. and Kautz, Jan},
  year = {2011},
  month = jul,
  volume = {30},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {The Laplacian pyramid is ubiquitous for decomposing images into multiple scales and is widely used for image analysis. However, because it is constructed with spatially invariant Gaussian kernels, the Laplacian pyramid is widely believed as being unable to represent edges well and as being ill-suited for edge-aware operations such as edge-preserving smoothing and tone mapping. To tackle these tasks, a wealth of alternative techniques and representations have been proposed, e.g., anisotropic diffusion, neighborhood filtering, and specialized wavelet bases. While these methods have demonstrated successful results, they come at the price of additional complexity, often accompanied by higher computational cost or the need to post-process the generated results. In this paper, we show state-of-the-art edge-aware processing using standard Laplacian pyramids. We characterize edges with a simple threshold on pixel values that allows us to differentiate large-scale edges from small-scale details. Building upon this result, we propose a set of image filters to achieve edge-preserving smoothing, detail enhancement, tone mapping, and inverse tone mapping. The advantage of our approach is its simplicity and flexibility, relying only on simple point-wise nonlinearities and small Gaussian convolutions; no optimization or post-processing is required. As we demonstrate, our method produces consistently high-quality results, without degrading edges or introducing halos.},
  articleno = {68},
  issue_date = {July 2011},
  journal = {ACM Trans. Graph.},
  keywords = {edge-aware image processing,image pyramids},
  number = {4},
  series = {{{SIGGRAPH}}'11}
}

@inproceedings{10.1145/2063384.2063405,
  title = {Parallel Random Numbers: {{As}} Easy as 1, 2, 3},
  booktitle = {Proceedings of {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Salmon, John K. and Moraes, Mark A. and Dror, Ron O. and Shaw, David E.},
  year = {2011},
  publisher = {{ACM Press}},
  address = {{Seattle, WA, USA}},
  abstract = {Most pseudorandom number generators (PRNGs) scale poorly to massively parallel high-performance computation because they are designed as sequentially dependent state transformations. We demonstrate that independent, keyed transformations of counters produce a large alternative class of PRNGs with excellent statistical properties (long period, no discernable structure or correlation). These counter-based PRNGs are ideally suited to modern multi-core CPUs, GPUs, clusters, and special-purpose hardware because they vectorize and parallelize well, and require little or no memory for state. We introduce several counter-based PRNGs: some based on cryptographic standards (AES, Threefish) and some completely new (Philox). All our PRNGs pass rigorous statistical tests (including TestU01's BigCrush) and produce at least 264 unique parallel streams of random numbers, each with period 2128 or more. In addition to essentially unlimited parallel scalability, our PRNGs offer excellent single-chip performance: Philox is faster than the CURAND library on a single NVIDIA GPU.},
  articleno = {16},
  series = {{{SC}} '11}
}

@inproceedings{10.1145/258734.258887,
  title = {Design Galleries: {{A}} General Approach to Setting Parameters for Computer Graphics and Animation},
  booktitle = {Proc. {{Annu}}. {{Conf}}. {{Comput}}. {{Graph}}. {{Interact}}. {{Techn}}.},
  author = {Marks, J. and Andalman, B. and Beardsley, P. A. and Freeman, W. and Gibson, S. and Hodgins, J. and Kang, T. and Mirtich, B. and Pfister, H. and Ruml, W. and Ryall, K. and Seims, J. and Shieber, S.},
  year = {1997},
  pages = {389--400},
  publisher = {{ACM Press/Addison-Wesley Publishing Co.}},
  address = {{USA}},
  keywords = {animation,computer-aided design,image rendering,lighting,motion synthesis,particle systems,physical modeling,visualization,volume rendering},
  series = {{{SIGGRAPH}}'97}
}

@inproceedings{10.1145/2593882.2593900,
  title = {Probabilistic Programming},
  booktitle = {Future of Software Engineering Proceedings},
  author = {Gordon, Andrew D. and Henzinger, Thomas A. and Nori, Aditya V. and Rajamani, Sriram K.},
  year = {2014},
  pages = {167--181},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {Machine learning,Probabilistic programming,Program analysis},
  series = {{{FOSE}} 2014}
}

@inproceedings{10.1145/2737924.2737969,
  title = {Autotuning Algorithmic Choice for Input Sensitivity},
  booktitle = {Proceedings of the 36th {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {Ding, Yufei and Ansel, Jason and Veeramachaneni, Kalyan and Shen, Xipeng and O'Reilly, Una-May and Amarasinghe, Saman},
  year = {2015},
  pages = {379--390},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {Algorithmic Selection,Autotuning,Input Sensitivity,Variable Accuracy},
  series = {{{PLDI}} '15}
}

@inproceedings{10.1145/2751205.2751214,
  title = {{{FAST}}: {{A}} Fast Stencil Autotuning Framework Based on an Optimal-Solution Space Model},
  booktitle = {Proceedings of the 29th {{ACM}} on International Conference on Supercomputing},
  author = {Luo, Yulong and Tan, Guangming and Mo, Zeyao and Sun, Ninghui},
  year = {2015},
  pages = {187--196},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {autotuning,oss,stencil},
  series = {{{ICS}} '15}
}

@inproceedings{10.1145/2858036.2858111,
  title = {{{SelPh}}: {{Progressive}} Learning and Support of Manual Photo Color Enhancement},
  booktitle = {Proceedings of the {{CHI}} Conference on Human Factors in Computing Systems},
  author = {Koyama, Yuki and Sakamoto, Daisuke and Igarashi, Takeo},
  year = {2016},
  pages = {2520--2532},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {design support,photo enhancement,self-reinforcement},
  series = {{{CHI}} '16}
}

@inproceedings{10.1145/2872362.2872411,
  title = {Architecture-Adaptive Code Variant Tuning},
  booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
  author = {Muralidharan, Saurav and Roy, Amit and Hall, Mary and Garland, Michael and Rai, Piyush},
  year = {2016},
  pages = {325--338},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {autotuning,cross-architectural tuning,device feature selection,input-adaptive,multi-task learning},
  series = {{{ASPLOS}} '16}
}

@inproceedings{10.1145/2939672.2939785,
  title = {{{XGBoost}}: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  pages = {785--794},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {large-scale machine learning},
  series = {{{KDD}} '16}
}

@article{10.1145/3072959.3073598,
  title = {Sequential Line Search for Efficient Visual Design Optimization by Crowds},
  author = {Koyama, Yuki and Sato, Issei and Sakamoto, Daisuke and Igarashi, Takeo},
  year = {2017},
  month = jul,
  volume = {36},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Parameter tweaking is a common task in various design scenarios. For example, in color enhancement of photographs, designers tweak multiple parameters such as "brightness" and "contrast" to obtain the best visual impression. Adjusting one parameter is easy; however, if there are multiple correlated parameters, the task becomes much more complex, requiring many trials and a large cognitive load. To address this problem, we present a novel extension of Bayesian optimization techniques, where the system decomposes the entire parameter tweaking task into a sequence of one-dimensional line search queries that are easy for human to perform by manipulating a single slider. In addition, we present a novel concept called crowd-powered visual design optimizer, which queries crowd workers, and provide a working implementation of this concept. Our single-slider manipulation microtask design for crowdsourcing accelerates the convergence of the optimization relative to existing comparison-based microtask designs. We applied our framework to two different design domains: photo color enhancement and material BRDF design, and thereby showed its applicability to various design domains.},
  articleno = {48},
  issue_date = {July 2017},
  journal = {ACM Trans. Graph.},
  keywords = {bayesian optimization,computational design,crowdsourcing,human computation},
  number = {4}
}

@inproceedings{10.1145/3205289.3205321,
  title = {Bootstrapping Parameter Space Exploration for Fast Tuning},
  booktitle = {Proceedings of the 2018 International Conference on Supercomputing},
  author = {Thiagarajan, Jayaraman J. and Jain, Nikhil and Anirudh, Rushil and Gimenez, Alfredo and Sridhar, Rahul and Marathe, Aniruddha and Wang, Tao and Emani, Murali and Bhatele, Abhinav and Gamblin, Todd},
  year = {2018},
  pages = {385--395},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {autotuning,performance,sampling,semi-supervised learning},
  series = {{{ICS}} '18}
}

@article{10.1145/3218823,
  title = {Design and Implementation of Adaptive {{SpMV}} Library for Multicore and Many-Core Architecture},
  author = {Tan, Guangming and Liu, Junhong and Li, Jiajia},
  year = {2018},
  month = aug,
  volume = {44},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  articleno = {46},
  issue_date = {August 2018},
  journal = {ACM Trans Math Softw},
  keywords = {auto-tuning,machine learning,multicore,Sparse matrix vector multiplication},
  number = {4}
}

@inproceedings{10.1145/3243176.3243198,
  title = {Log({{Graph}}): {{A}} near-Optimal High-Performance Graph Representation},
  booktitle = {Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques},
  author = {Besta, Maciej and Stanojevic, Dimitri and Zivic, Tijana and Singh, Jagpreet and Hoerold, Maurice and Hoefler, Torsten},
  year = {2018},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Today's graphs used in domains such as machine learning or social network analysis may contain hundreds of billions of edges. Yet, they are not necessarily stored efficiently, and standard graph representations such as adjacency lists waste a significant number of bits while graph compression schemes such as WebGraph often require time-consuming decompression. To address this, we propose Log(Graph): a graph representation that combines high compression ratios with very low-overhead decompression to enable cheaper and faster graph processing. The key idea is to encode a graph so that the parts of the representation approach or match the respective storage lower bounds. We call our approach "graph logarithmization" because these bounds are usually logarithmic. Our high-performance Log(Graph) implementation based on modern bitwise operations and state-of-the-art succinct data structures achieves high compression ratios as well as performance. For example, compared to the tuned Graph Algorithm Processing Benchmark Suite (GAPBS), it reduces graph sizes by 20-35\% while matching GAPBS' performance or even delivering speedups due to reducing amounts of transferred data. It approaches the compression ratio of the established WebGraph compression library while enabling speedups of up to more than 2\texttimes. Log(Graph) can improve the design of various graph processing engines or libraries on single NUMA nodes as well as distributed-memory systems.},
  articleno = {7},
  keywords = {graph compression,graph layout,graph representation,ILP,parallel graph algorithms,succinct data structures},
  series = {{{PACT}} '18}
}

@inproceedings{10.1145/3295500.3356181,
  title = {Red-Blue Pebbling Revisited: {{Near}} Optimal Parallel Matrix-Matrix Multiplication},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  author = {Kwasniewski, Grzegorz and Kabi{\'c}, Marko and Besta, Maciej and VandeVondele, Joost and Solc{\`a}, Raffaele and Hoefler, Torsten},
  year = {2019},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {We propose COSMA: a parallel matrix-matrix multiplication algorithm that is near communication-optimal for all combinations of matrix dimensions, processor counts, and memory sizes. The key idea behind COSMA is to derive an optimal (up to a factor of 0.03\% for 10MB of fast memory) sequential schedule and then parallelize it, preserving I/O optimality. To achieve this, we use the red-blue pebble game to precisely model MMM dependencies and derive a constructive and tight sequential and parallel I/O lower bound proofs. Compared to 2D or 3D algorithms, which fix processor decomposition upfront and then map it to the matrix dimensions, it reduces communication volume by up to {$\surd$} times. COSMA outperforms the established ScaLAPACK, CARMA, and CTF algorithms in all scenarios up to 12.8x (2.2x on average), achieving up to 88\% of Piz Daint's peak performance. Our work does not require any hand tuning and is maintained as an open source implementation.},
  articleno = {24},
  series = {{{SC}} '19}
}

@article{10.1145/3386569.3392409,
  title = {Human-in-the-Loop Differential Subspace Search in High-Dimensional Latent Space},
  author = {Chiu, Chia-Hsing and Koyama, Yuki and Lai, Yu-Chi and Igarashi, Takeo and Yue, Yonghao},
  year = {2020},
  month = jul,
  volume = {39},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Generative models based on deep neural networks often have a high-dimensional latent space, ranging sometimes to a few hundred dimensions or even higher, which typically makes them hard for a user to explore directly. We propose differential subspace search to allow efficient iterative user exploration in such a space, without relying on domain- or data-specific assumptions. We develop a general framework to extract low-dimensional subspaces based on a local differential analysis of the generative model, such that a small change in such a subspace would provide enough change in the resulting data. We do so by applying singular value decomposition to the Jacobian of the generative model and forming a subspace with the desired dimensionality spanned by a given number of singular vectors stochastically selected on the basis of their singular values, to maintain ergodicity. We use our framework to present 1D subspaces to the user via a 1D slider interface. Starting from an initial location, the user finds a new candidate in the presented 1D subspace, which is in turn updated at the new candidate location. This process is repeated until no further improvement can be made. Numerical simulations show that our method can better optimize synthetic black-box objective functions than the alternatives that we tested. Furthermore, we conducted a user study using complex generative models and the results show that our method enables more efficient exploration of high-dimensional latent spaces than the alternatives.},
  articleno = {85},
  issue_date = {July 2020},
  journal = {ACM Trans. Graph.},
  keywords = {dimensionality reduction,generative models,human-in-the-loop optimization},
  number = {4},
  series = {{{SIGGRAPH}}'20}
}

@article{10.1214/17-STS611,
  title = {Importance Sampling: {{Intrinsic}} Dimension and Computational Cost},
  author = {Agapiou, S. and Papaspiliopoulos, O. and {Sanz-Alonso}, D. and Stuart, A. M.},
  year = {2017},
  volume = {32},
  pages = {405--431},
  publisher = {{Institute of Mathematical Statistics}},
  journal = {Stat. Sci.},
  keywords = {Absolute continuity,Filtering,importance sampling,Inverse problems,notions of dimension,small noise},
  number = {3}
}

@article{10.2307/2242610,
  title = {Rates of Convergence of the Hastings and Metropolis Algorithms},
  author = {Mengersen, K. L. and Tweedie, R. L.},
  year = {1996},
  volume = {24},
  pages = {101--121},
  publisher = {{Institute of Mathematical Statistics}},
  abstract = {We apply recent results in Markov chain theory to Hastings and Metropolis algorithms with either independent or symmetric candidate distributions, and provide necessary and sufficient conditions for the algorithms to converge at a geometric rate to a prescribed distribution {$\pi$}. In the independence case (in Rk) these indicate that geometric convergence essentially occurs if and only if the candidate density is bounded below by a multiple of {$\pi$}; in the symmetric case (in R only) we show geometric convergence essentially occurs if and only if {$\pi$} has geometric tails. We also evaluate recently developed computable bounds on the rates of convergence in this context: examples show that these theoretical bounds can be inherently extremely conservative, although when the chain is stochastically monotone the bounds may well be effective.},
  journal = {Ann. Stat.},
  number = {1}
}

@article{10.2307/24308995,
  title = {Adaptively Scaling the {{Metropolis}} Algorithm Using Expected Squared Jumped Distance},
  author = {Pasarica, Cristian and Gelman, Andrew},
  year = {2010},
  volume = {20},
  pages = {343--364},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  abstract = {A good choice of the proposal distribution is crucial for the rapid convergence of the Metropolis algorithm. In this paper, given a family of parametric Markovian kernels, we develop an adaptive algorithm for selecting the best kernel that maximizes the expected squared jumped distance, an objective function that characterizes the Markov chain. We demonstrate the effectiveness of our method in several examples.},
  journal = {Stat. Sin.},
  number = {1}
}

@article{10.2307/25662473,
  title = {{{DOES WASTE RECYCLING REALLY IMPROVE THE MULTI}}-{{PROPOSAL METROPOLIS}}\textendash{{HASTINGS ALGORITHM}}? {{AN ANALYSIS BASED ON CONTROL VARIATES}}},
  author = {Delmas, Jean-Fran{\c c}ois and Jourdain, Benjamin},
  year = {2009},
  volume = {46},
  pages = {938--959},
  publisher = {{Applied Probability Trust}},
  abstract = {The waste-recycling Monte Carlo (WRMC) algorithm introduced by physicists is a modification of the (multi-proposal) Metropolis\textendash Hastings algorithm, which makes use of all the proposals in the empirical mean, whereas the standard (multi-proposal) Metropolis\textendash Hastings algorithm uses only the accepted proposals. In this paper we extend the WRMC algorithm to a general control variate technique and exhibit the optimal choice of the control variate in terms of the asymptotic variance. We also give an example which shows that, in contradiction to the intuition of physicists, the WRMC algorithm can have an asymptotic variance larger than that of the Metropolis\textendash Hastings algorithm. However, in the particular case of the Metropolis\textendash Hastings algorithm called the Boltzmann algorithm, we prove that the WRMC algorithm is asymptotically better than the Metropolis\textendash Hastings algorithm. This last property is also true for the multiproposal Metropolis\textendash Hastings algorithm. In this last framework we consider a linear parametric generalization of WRMC, and we propose an estimator of the explicit optimal parameter using the proposals.},
  journal = {J. Appl. Probab.},
  number = {4}
}

@article{10.2307/27595854,
  title = {Coupling and Ergodicity of Adaptive Markov Chain Monte Carlo Algorithms},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2007},
  volume = {44},
  pages = {458--475},
  publisher = {{Applied Probability Trust}},
  abstract = {We consider basic ergodicity properties of adaptive Markov chain Monte Carlo algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counterexamples to demonstrate that the assumptions we make are not redundant.},
  journal = {J. Appl. Probab.},
  number = {2}
}

@article{10.2307/27821441,
  title = {Preference Uncertainty, Preference Learning, and Paired Comparison Experiments},
  author = {Kingsley, David C. and Brown, Thomas C.},
  year = {2010},
  volume = {86},
  pages = {530--544},
  publisher = {{[Board of Regents of the University of Wisconsin System, University of Wisconsin Press]}},
  abstract = {Results from paired comparison experiments suggest that as respondents progress through a sequence of binary choices they become more consistent, apparently fine-tuning their preferences. Consistency may be indicated by the variance of the estimated valuation distribution measured by the error term in the random utility model. A significant reduction in the variance is shown to be consistent with a model of preference uncertainty allowing for preference learning. Respondents become more adept at discriminating among items as they gain experience considering and comparing them, suggesting that methods allowing for such experience may obtain more well founded values.},
  journal = {Land Econ.},
  number = {3}
}

@inproceedings{10.5555/2074022.2074067,
  title = {Expectation Propagation for Approximate Bayesian Inference},
  booktitle = {Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence ({{UAI}})},
  author = {Minka, Thomas P.},
  year = {2001},
  pages = {362--369},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation," unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining expectations, such as mean and varitmce, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be donvincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.}
}

@article{10.5555/235610.235641,
  title = {Exact Sampling with Coupled Markov Chains and Applications to Statistical Mechanics},
  author = {Propp, James Gary and Wilson, David Bruce},
  year = {1996},
  month = aug,
  volume = {9},
  pages = {223--252},
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{USA}},
  issue_date = {Aug./Sept. 1996},
  journal = {Random Struct. Algorithms},
  number = {1\textendash 2}
}

@inproceedings{10.5555/2540128.2540383,
  title = {Bayesian Optimization in High Dimensions via Random Embeddings},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  author = {Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David and De Freitas, Nando},
  year = {2013},
  pages = {1778--1784},
  publisher = {{AAAI Press}},
  address = {{Beijing, China}},
  abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. The experiments demonstrate that REMBO can effectively solve high-dimensional problems, including automatic parameter configuration of a popular mixed integer linear programming solver.},
  series = {{{IJCAI}} '13}
}

@article{10.5555/2627435.2638586,
  title = {The No-u-Turn Sampler: {{Adaptively}} Setting Path Lengths in Hamiltonian Monte Carlo},
  author = {Homan, Matthew D. and Gelman, Andrew},
  year = {2014},
  month = jan,
  volume = {15},
  pages = {1593--1623},
  publisher = {{JMLR.org}},
  issue_date = {January 2014},
  journal = {J. Mach. Learn. Res.},
  keywords = {adaptive Monte Carlo,Bayesian inference,dual averaging,Hamiltonian Monte Carlo,Markov chain Monte Carlo},
  number = {1}
}

@inproceedings{10.5555/3020751.3020755,
  title = {Accelerating {{MCMC}} via Parallel Predictive Prefetching},
  booktitle = {Proc. 30th {{Conf}}. {{Uncertainty Artif}}. {{Intell}}.},
  author = {Angelino, Elaine and Kohler, Eddie and Waterland, Amos and Seltzer, Margo and Adams, Ryan P.},
  year = {2014},
  pages = {22--31},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}},
  abstract = {Parallel predictive prefetching is a new framework for accelerating a large class of widely-used Markov chain Monte Carlo (MCMC) algorithms. It speculatively evaluates many potential steps of an MCMC chain in parallel while exploiting fast, iterative approximations to the target density. This can accelerate sampling from target distributions in Bayesian inference problems. Our approach takes advantage of whatever parallel resources are available, but produces results exactly equivalent to standard serial execution. In the initial burn-in phase of chain evaluation, we achieve speedup close to linear in the number of available cores.},
  series = {{{UAI}}'14}
}

@inproceedings{10.5555/3020751.3020816,
  title = {Asymptotically Exact, Embarrassingly Parallel {{MCMC}}},
  booktitle = {Proc. 30th {{Conf}}. {{Uncertainty Artif}}. {{Intell}}.},
  author = {Neiswanger, Willie and Wang, Chong and Xing, Eric P.},
  year = {2014},
  pages = {623--632},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}},
  series = {{{UAI}}'14}
}

@inproceedings{10.5555/3042817.3043056,
  title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
  author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
  year = {2013},
  pages = {III--1067--III--1075},
  publisher = {{JMLR.org}},
  address = {{Atlanta, GA, USA}},
  series = {{{ICML}}'13}
}

@article{10.5555/3122009.3208005,
  title = {Robust and Scalable Bayes via a Median of Subset Posterior Measures},
  author = {Minsker, Stanislav and Srivastava, Sanvesh and Lin, Lizhen and Dunson, David B.},
  year = {2017},
  month = jan,
  volume = {18},
  pages = {4488--4527},
  publisher = {{JMLR.org}},
  issue_date = {January 2017},
  journal = {J. Mach. Learn. Res.},
  keywords = {big data,distributed computing,geometric median,parallel MCMC,Wasserstein distance},
  number = {1}
}

@inproceedings{10.5555/3305381.3305515,
  title = {Measuring Sample Quality with Kernels},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  author = {Gorham, Jackson and Mackey, Lester},
  year = {2017},
  pages = {1292--1301},
  publisher = {{JMLR.org}},
  address = {{Sydney, NSW, Australia}},
  series = {{{ICML}}'17}
}

@article{10.5555/944919.944937,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  month = mar,
  volume = {3},
  pages = {993--1022},
  publisher = {{JMLR.org}},
  issue_date = {3/1/2003},
  journal = {J. Mach. Learn. Res.},
  number = {null}
}

@article{41849,
  title = {Bayes and Big Data: {{The}} Consensus Monte Carlo Algorithm},
  author = {Scott, Steven L. and Blocker, Alexander W. and Bonassi, Fernando V. and Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  year = {2016},
  volume = {11},
  pages = {78--88},
  journal = {Int. J. Manag. Sci. Eng. Manag.}
}

@inproceedings{6713531,
  title = {Modeling of Motion Artifacts in Contactless Heart Rate Measurements},
  booktitle = {Computing in Cardiology},
  author = {Wartzek, T and Br{\"u}ser, C and Schlebusch, T and Brendle, C and Santos, S and Kerekes, A and {Gerlach-Hahn}, K and Weyer, S and Lunze, K and Hoog Antink, C and Leonhardt, S},
  year = {2013},
  pages = {931--934},
  publisher = {{IEEE}},
  address = {{Zaragoza, Spain}}
}

@article{abd-elmoniem_realtime_2002,
  title = {Real-Time Speckle Reduction and Coherence Enhancement in Ultrasound Imaging via Nonlinear Anisotropic Diffusion},
  author = {{Abd-Elmoniem}, K.Z. and Youssef, A.-B.M. and Kadah, Y.M.},
  year = {2002},
  month = sep,
  volume = {49},
  pages = {997--1014},
  file = {/home/msca8h/Zotero/storage/J7NM8HBL/Abd-Elmoniem et al. - 2002 - Real-time speckle reduction and coherence enhancem.pdf},
  journal = {IEEE Trans. Biomed. Eng.},
  language = {en},
  number = {9}
}

@article{adedinsewo_artificial_2020,
  title = {Artificial Intelligence-Enabled {{ECG}} Algorithm to Identify Patients with Left Ventricular Systolic Dysfunction Presenting to the Emergency Department with Dyspnea},
  author = {Adedinsewo, Demilade and Carter, Rickey E. and Attia, Zachi and Johnson, Patrick and Kashou, Anthony H. and Dugan, Jennifer L. and Albus, Michael and Sheele, Johnathan M. and Bellolio, Fernanda and Friedman, Paul A. and {Lopez-Jimenez}, Francisco and Noseworthy, Peter A.},
  year = {2020},
  month = aug,
  volume = {13},
  abstract = {Background:               Identification of systolic heart failure among patients presenting to the emergency department (ED) with acute dyspnea is challenging. The reasons for dyspnea are often multifactorial. A focused physical evaluation and diagnostic testing can lack sensitivity and specificity. The objective of this study was to assess the accuracy of an artificial intelligence-enabled ECG to identify patients presenting with dyspnea who have left ventricular systolic dysfunction (LVSD).                                         Methods:               We retrospectively applied a validated artificial intelligence-enabled ECG algorithm for the identification of LVSD (defined as LV ejection fraction {$\leq$}35\%) to a cohort of patients aged {$\geq$}18 years who were evaluated in the ED at a Mayo Clinic site with dyspnea. Patients were included if they had at least one standard 12-lead ECG acquired on the date of the ED visit and an echocardiogram performed within 30 days of presentation. Patients with prior LVSD were excluded. We assessed the model performance using area under the receiver operating characteristic curve, accuracy, sensitivity, and specificity.                                         Results:               A total of 1606 patients were included. Median time from ECG to echocardiogram was 1 day (Q1: 1, Q3: 2). The artificial intelligence-enabled ECG algorithm identified LVSD with an area under the receiver operating characteristic curve of 0.89 (95\% CI, 0.86\textendash 0.91) and accuracy of 85.9\%. Sensitivity, specificity, negative predictive value, and positive predictive value were 74\%, 87\%, 97\%, and 40\%, respectively. To identify an ejection fraction {$<$}50\%, the area under the receiver operating characteristic curve, accuracy, sensitivity, and specificity were 0.85 (95\% CI, 0.83\textendash 0.88), 86\%, 63\%, and 91\%, respectively. NT-proBNP (N-terminal pro-B-type natriuretic peptide) alone at a cutoff of {$>$}800 identified LVSD with an area under the receiver operating characteristic curve of 0.80 (95\% CI, 0.76\textendash 0.84).                                         Conclusions:               The ECG is an inexpensive, ubiquitous, painless test which can be quickly obtained in the ED. It effectively identifies LVSD in selected patients presenting to the ED with dyspnea when analyzed with artificial intelligence and outperforms NT-proBNP.                                         Graphic Abstract:                                A                 graphic abstract                 is available for this article.},
  file = {/home/msca8h/Zotero/storage/8A84NK47/Adedinsewo et al. - 2020 - Artificial Intelligence-Enabled ECG Algorithm to I.pdf},
  journal = {Circ: Arrhythmia and Electrophysiology},
  language = {en},
  number = {8}
}

@inproceedings{adve_influence_1993,
  title = {The Influence of Random Delays on Parallel Execution Times},
  booktitle = {Proc. 1993 {{ACM SIGMETRICS Conf}}. {{Meas}}. {{Model}}. {{Comp}}. {{Syst}}.},
  author = {Adve, Vikram S. and Vernon, Mary K.},
  year = {1993},
  pages = {61--73},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  series = {{{SIGMETRICS}}'93}
}

@article{aja-fernandez_estimation_2006,
  title = {On the Estimation of the Coefficient of Variation for Anisotropic Diffusion Speckle Filtering},
  author = {{Aja-Fernandez}, S. and {Alberola-Lopez}, C.},
  year = {2006},
  month = sep,
  volume = {15},
  pages = {2694--2701},
  journal = {IEEE Trans. Image Process.},
  number = {9}
}

@article{akyildiz_convergence_2021,
  title = {Convergence Rates for Optimised Adaptive Importance Samplers},
  author = {Akyildiz, {\"O}mer Deniz and M{\'i}guez, Joaqu{\'i}n},
  year = {2021},
  month = mar,
  volume = {31},
  pages = {12},
  abstract = {Abstract                            Adaptive importance samplers are adaptive Monte Carlo algorithms to estimate expectations with respect to some target distribution which               adapt               themselves to obtain better estimators over a sequence of iterations. Although it is straightforward to show that they have the same                                                   \$\$\textbackslash mathcal \{O\}(1/\textbackslash sqrt\{N\})\$\$                                                               O                       (                       1                       /                                                N                                              )                                                                                       convergence rate as standard importance samplers, where               N               is the number of Monte Carlo samples, the behaviour of adaptive importance samplers over the number of iterations has been left relatively unexplored. In this work, we investigate an adaptation strategy based on convex optimisation which leads to a class of adaptive importance samplers termed               optimised adaptive importance samplers               (OAIS). These samplers rely on the iterative minimisation of the                                                   \$\$\textbackslash chi \^2\$\$                                                               {$\chi$}                       2                                                                                       -divergence between an exponential family proposal and the target. The analysed algorithms are closely related to the class of adaptive importance samplers which minimise the variance of the weight function. We first prove non-asymptotic error bounds for the mean squared errors (MSEs) of these algorithms, which explicitly depend on the number of iterations and the number of samples together. The non-asymptotic bounds derived in this paper imply that when the target belongs to the exponential family, the                                                   \$\$L\_2\$\$                                                               L                       2                                                                                       errors of the optimised samplers converge to the optimal rate of                                                   \$\$\textbackslash mathcal \{O\}(1/\textbackslash sqrt\{N\})\$\$                                                               O                       (                       1                       /                                                N                                              )                                                                                       and the rate of convergence in the number of iterations are explicitly provided. When the target does               not               belong to the exponential family, the rate of convergence is the same but the asymptotic                                                   \$\$L\_2\$\$                                                               L                       2                                                                                       error increases by a factor                                                   \$\$\textbackslash sqrt\{\textbackslash rho \^\textbackslash star \} {$>$} 1\$\$                                                                                                                   {$\rho$}                           {$\star$}                                                                       {$>$}                       1                                                                                       , where                                                   \$\$\textbackslash rho \^\textbackslash star - 1\$\$                                                                                        {$\rho$}                         {$\star$}                                              -                       1                                                                                       is the minimum                                                   \$\$\textbackslash chi \^2\$\$                                                               {$\chi$}                       2                                                                                       -divergence between the target and an exponential family proposal.},
  file = {/home/msca8h/Zotero/storage/6ZUPK5ZZ/Akyildiz and Míguez - 2021 - Convergence rates for optimised adaptive importanc.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@misc{alexanderfabisch_cmaespp_2011,
  title = {{{CMA}}-{{ESpp}}},
  author = {Alexander Fabisch},
  year = {2011},
  abstract = {A C++ implementation of the derivative-free optimization algorithm CMA-ES.}
}

@article{alhinai_deep_2021,
  title = {Deep Learning Analysis of Resting Electrocardiograms for the Detection of Myocardial Dysfunction, Hypertrophy, and Ischaemia: A Systematic Review},
  shorttitle = {Deep Learning Analysis of Resting Electrocardiograms for the Detection of Myocardial Dysfunction, Hypertrophy, and Ischaemia},
  author = {Al Hinai, Ghalib and Jammoul, Samer and Vajihi, Zara and Afilalo, Jonathan},
  year = {2021},
  month = aug,
  pages = {ztab048},
  abstract = {Abstract                            Aims               To assess the evidence for deep learning (DL) analysis of resting electrocardiograms (ECG) to predict structural cardiac pathologies such as left ventricular systolic dysfunction, myocardial hypertrophy, and ischaemic heart disease.                                         Methods and Results               A systematic review was conducted to identify published original articles on end-to-end DL analysis of resting ECG signals for the detection of structural cardiac pathologies. Studies were excluded if the ECG was acquired by ambulatory, stress, intracardiac, or implantable devices, and if the pathology of interest was arrhythmic in nature. After duplicate reviewers screened search results, 12 articles met the inclusion criteria and were included. Three articles used DL ECG to detect left ventricular systolic dysfunction, achieving an area under the curve (AUC) of 0.89-0.93 and accuracy of 98\%. One study used DL ECG to detect left ventricular hypertrophy, achieving an AUC of 0.87 and accuracy of 87\%. Six articles used DL ECG to detect acute myocardial infarction, achieving an AUC of 0.88-1.00 and accuracy of 83-99.9\%. Two articles used DL ECG to detect stable ischaemic heart disease, achieving an accuracy of 95-99.9\%. DL algorithms, particularly those that used convolutional neural networks, outperformed rules-based algorithms and other machine learning algorithms.                                         Conclusions               DL is a promising technique to analyze resting ECG signals for the detection of structural cardiac pathologies, which has clinical applicability for more effective screening of asymptomatic populations and expedited diagnostic work-up of symptomatic patients at risk for cardiovascular disease.},
  file = {/home/msca8h/Zotero/storage/EIYAWAEP/Al Hinai et al. - 2021 - Deep learning analysis of resting electrocardiogra.pdf},
  journal = {Eur. Heart J. - Digit. Health},
  language = {en}
}

@inproceedings{alipourfard_cherrypick_2017,
  title = {{{CherryPick}}: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics},
  booktitle = {Proc. 14th {{USENIX Symp}}. {{Networked Syst}}. {{Des}}. {{Implementation}}},
  author = {Alipourfard, Omid and Liu, Hongqiang Harry and Chen, Jianshu and Venkataraman, Shivaram and Yu, Minlan and Zhang, Ming},
  year = {2017},
  pages = {469--482},
  publisher = {{USENIX Association}},
  address = {{Boston, MA}},
  file = {/home/msca8h/Documents/bayesian_optimization/Alipourfard et al. - 2017 - CherryPick Adaptively Unearthing the Best Cloud C.pdf},
  series = {{{NSDI}}'17}
}

@article{altekar_parallel_2004,
  title = {Parallel {{Metropolis}} Coupled {{Markov}} Chain {{Monte Carlo}} for {{Bayesian}} Phylogenetic Inference},
  author = {Altekar, G. and Dwarkadas, S. and Huelsenbeck, J. P. and Ronquist, F.},
  year = {2004},
  month = feb,
  volume = {20},
  pages = {407--415},
  file = {/home/msca8h/Zotero/storage/YQ92IHIC/Altekar et al. - 2004 - Parallel Metropolis coupled Markov chain Monte Car.pdf},
  journal = {Bioinformatics},
  language = {en},
  number = {3}
}

@inproceedings{amdahl_validity_1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, {{Spring Joint Computer Conference}}},
  author = {Amdahl, Gene M.},
  year = {1967},
  pages = {483},
  publisher = {{ACM Press}},
  address = {{Atlantic City, New Jersey}},
  language = {en}
}

@inproceedings{amiri_computation_2019,
  title = {Computation {{Scheduling}} for {{Distributed Machine Learning}} with {{Straggling Workers}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Amiri, Mohammad Mohammadi and Gunduz, Deniz},
  year = {2019},
  month = may,
  pages = {8177--8181},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  file = {/home/msca8h/Zotero/storage/9B8TBEGA/Amiri and Gunduz - 2019 - Computation Scheduling for Distributed Machine Lea.pdf}
}

@article{andrieu_ergodicity_2006,
  title = {On the Ergodicity Properties of Some Adaptive {{MCMC}} Algorithms},
  author = {Andrieu, Christophe and Moulines, {\'E}ric},
  year = {2006},
  month = aug,
  volume = {16},
  file = {/home/msca8h/Zotero/storage/BWPKTXKR/Andrieu and Moulines - 2006 - On the ergodicity properties of some adaptive MCMC.pdf},
  journal = {Ann. Appl. Probab.},
  number = {3}
}

@article{andrieu_introduction_2003,
  title = {An Introduction to {{MCMC}} for Machine Learning},
  author = {Andrieu, Christophe and {de Freitas}, Nando and Doucet, Arnaud and Jordan, Michael I.},
  year = {2003},
  volume = {50},
  pages = {5--43},
  file = {/home/msca8h/Zotero/storage/JRMGPMFB/Andrieu et al. - 2003 - [No title found].pdf},
  journal = {Mach. Learn.},
  number = {1/2}
}

@article{andrieu_particle_2010,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  shorttitle = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  month = jun,
  volume = {72},
  pages = {269--342},
  file = {/home/msca8h/Zotero/storage/UNBH88PQ/Andrieu et al. - 2010 - Particle Markov chain Monte Carlo methods Particl.pdf},
  journal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  language = {en},
  number = {3}
}

@article{andrieu_uniform_2018,
  title = {Uniform Ergodicity of the Iterated Conditional {{SMC}} and Geometric Ergodicity of Particle {{Gibbs}} Samplers},
  author = {Andrieu, Christophe and Lee, Anthony and Vihola, Matti},
  year = {2018},
  month = may,
  volume = {24},
  file = {/home/msca8h/Zotero/storage/AAKZYF9Z/Andrieu et al. - 2018 - Uniform ergodicity of the iterated conditional SMC.pdf},
  journal = {Bernoulli},
  number = {2}
}

@article{ang_optimal_1992,
  title = {Optimal Importance-sampling Density Estimator},
  author = {Ang, George L. and Ang, Alfredo H.-S. and Tang, Wilson H.},
  year = {1992},
  month = jun,
  volume = {118},
  pages = {1146--1163},
  journal = {Journal of Engineering Mechanics},
  language = {en},
  number = {6}
}

@article{angelino_patterns_2016,
  title = {Patterns of Scalable {{Bayesian}} Inference},
  author = {Angelino, Elaine and Johnson, Matthew James and Adams, Ryan P.},
  year = {2016},
  volume = {9},
  pages = {119--247},
  file = {/home/msca8h/Zotero/storage/K48JPUMZ/Angelino et al. - 2016 - Patterns of Scalable Bayesian Inference.pdf},
  journal = {Found. Trends. Mach. Learn.},
  language = {en},
  number = {2-3}
}

@inproceedings{ansel_opentuner_2014,
  title = {{{OpenTuner}}: An Extensible Framework for Program Autotuning},
  shorttitle = {{{OpenTuner}}},
  booktitle = {Proceedings of the 23rd International Conference on {{Parallel}} Architectures and Compilation - {{PACT}} '14},
  author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and {Ragan-Kelley}, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
  year = {2014},
  pages = {303--316},
  publisher = {{ACM Press}},
  address = {{Edmonton, AB, Canada}},
  language = {en}
}

@article{article,
  title = {{{JAGS}}: {{A}} Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling},
  author = {Plummer, Martyn},
  year = {2003},
  month = apr,
  volume = {124},
  journal = {3rd Int. Workshop Distrib. Stat. Comput. DSC 2003 Vienna Austria}
}

@article{ashikuzzaman_low_2020,
  title = {Low {{Rank}} and {{Sparse Decomposition}} of {{Ultrasound Color Flow Images}} for {{Suppressing Clutter}} in {{Real}}-{{Time}}},
  author = {Ashikuzzaman, Md and Belasso, Clyde and Kibria, Md. Golam and Bergdahl, Andreas and Gauthier, Claudine J. and Rivaz, Hassan},
  year = {2020},
  month = apr,
  volume = {39},
  pages = {1073--1084},
  journal = {IEEE Trans. Med. Imaging},
  number = {4}
}

@article{ashouri_survey_2018,
  title = {A {{Survey}} on {{Compiler Autotuning}} Using {{Machine Learning}}},
  author = {Ashouri, Amir H. and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
  year = {2018},
  month = sep,
  volume = {51},
  pages = {1--42},
  file = {/home/msca8h/Zotero/storage/K2MMSXTC/Ashouri et al. - 2018 - A Survey on Compiler Autotuning using Machine Lear.pdf},
  journal = {ACM Comput. Surv.},
  language = {en},
  number = {5}
}

@article{asl_lowcomplexity_2012,
  title = {A Low-Complexity Adaptive Beamformer for Ultrasound Imaging Using Structured Covariance Matrix},
  author = {Asl, B. M. and Mahloojifar, A.},
  year = {2012},
  month = apr,
  volume = {59},
  pages = {660--667},
  journal = {IEEE Trans. Ultrason., Ferroelect., Freq. Contr.},
  number = {4}
}

@inproceedings{ath_bayesian_2021,
  title = {How {{Bayesian}} Should {{Bayesian}} Optimisation Be?},
  booktitle = {Proc.  {{Genetic Evol}}. {{Comput}}. {{Conf}}. {{Companion}}},
  author = {George De Ath, Richard Everson, Jonathan Fieldsend},
  year = {2021},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  series = {{{GECCO}} '20}
}

@article{attia_screening_2019,
  title = {Screening for Cardiac Contractile Dysfunction Using an Artificial Intelligence\textendash Enabled Electrocardiogram},
  author = {Attia, Zachi I. and Kapa, Suraj and {Lopez-Jimenez}, Francisco and McKie, Paul M. and Ladewig, Dorothy J. and Satam, Gaurav and Pellikka, Patricia A. and {Enriquez-Sarano}, Maurice and Noseworthy, Peter A. and Munger, Thomas M. and Asirvatham, Samuel J. and Scott, Christopher G. and Carter, Rickey E. and Friedman, Paul A.},
  year = {2019},
  month = jan,
  volume = {25},
  pages = {70--74},
  journal = {Nat Med},
  language = {en},
  number = {1}
}

@phdthesis{austad_parallel_2007,
  title = {Parallel {{Multiple Proposal MCMC Algorithms}}},
  author = {Austad, Haakon Michael},
  year = {2007},
  month = jun,
  school = {Norwegian University of Science and Technology},
  type = {Master Thesis}
}

@inproceedings{axelsen_evaluation_2010,
  title = {Evaluation of Automatic Time Gain Compensated In-Vivo Ultrasound Sequences},
  booktitle = {{{IEEE International Ultrasonics Symposium}}},
  author = {Axelsen, Martin Christian and Roeboe, Kristian Frostholm and Hemmsen, Martin Christian and Nikolov, Svetoslav Ivanov and Pedersen, Mads Moller and Nielsen, Michael Bachmann and Jensen, Jorgen Arendt},
  year = {2010},
  month = oct,
  pages = {1640--1643},
  publisher = {{IEEE}},
  address = {{San Diego, CA}},
  file = {/home/msca8h/Zotero/storage/KWKSCNCV/Axelsen et al. - 2010 - Evaluation of automatic time gain compensated in-v.pdf}
}

@inproceedings{backstrom_group_2006,
  title = {Group Formation in Large Social Networks: Membership, Growth, and Evolution},
  booktitle = {Proc. 12th {{ACM SIGKDD Int}}. {{Conf}}. {{Knowl}}. {{Discovery Data Mining}}},
  author = {Backstrom, Lars and Huttenlocher, Dan and Kleinberg, Jon and Lan, Xiangyang},
  year = {2006},
  pages = {44--54},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {diffusion of innovations,on-line communities,social networks},
  series = {{{KDD}}'06}
}

@article{bai_containment_2011,
  title = {On the Containment Condition for Adaptive Markov Chain Monte Carlo Algorithms},
  author = {Bai, Yan and Roberts, Gareth and Rosenthal, Jeffrey},
  year = {2011},
  month = jan,
  volume = {21},
  journal = {Adv Appl Stat}
}

@inproceedings{bailey_nas_1991,
  title = {The {{NAS}} Parallel Benchmarks---Summary and Preliminary Results},
  booktitle = {Proceedings of the 1991 {{ACM}}/{{IEEE}} Conference on {{Supercomputing}}  - {{Supercomputing}} '91},
  author = {Bailey, D. H. and Schreiber, R. S. and Simon, H. D. and Venkatakrishnan, V. and Weeratunga, S. K. and Barszcz, E. and Barton, J. T. and Browning, D. S. and Carter, R. L. and Dagum, L. and Fatoohi, R. A. and Frederickson, P. O. and Lasinski, T. A.},
  year = {1991},
  pages = {158--165},
  publisher = {{ACM Press}},
  address = {{Albuquerque, New Mexico, United States}},
  language = {en}
}

@incollection{bailey_nas_2011,
  title = {{{NAS}} Parallel Benchmarks},
  booktitle = {Encyclopedia of {{Parallel Computing}}},
  author = {Bailey, David H.},
  year = {2011},
  pages = {1254--1259},
  publisher = {{Springer US}},
  address = {{Boston, MA}}
}

@inproceedings{bak_optimized_2019,
  title = {Optimized Execution of Parallel Loops via User-Defined Scheduling Policies},
  booktitle = {Proc. 48th {{Int}}. {{Conf}}. {{Parallel Process}}.},
  author = {Bak, Seonmyeong and Guo, Yanfei and Balaji, Pavan and Sarkar, Vivek},
  year = {2019},
  month = aug,
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Kyoto Japan}},
  language = {en},
  series = {{{ICPP}}'19}
}

@article{balakrishnan_onepass_2006,
  title = {A One-Pass Sequential {{Monte Carlo}} Method for {{Bayesian}} Analysis of Massive Datasets},
  author = {Balakrishnan, Suhrid and Madigan, David},
  year = {2006},
  month = jun,
  volume = {1},
  pages = {345--361},
  file = {/home/msca8h/Zotero/storage/WVSSS4I2/Balakrishnan and Madigan - 2006 - A one-pass sequential Monte Carlo method for Bayes.pdf},
  journal = {Bayesian Anal.},
  language = {en},
  number = {2}
}

@inproceedings{balaprakash_activelearningbased_2013,
  title = {Active-Learning-Based Surrogate Models for Empirical Performance Tuning},
  booktitle = {2013 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Balaprakash, Prasanna and Gramacy, Robert B. and Wild, Stefan M.},
  year = {2013},
  month = sep,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Indianapolis, IN, USA}},
  file = {/home/msca8h/Zotero/storage/JTYE3NWJ/Balaprakash et al. - 2013 - Active-learning-based surrogate models for empiric.pdf}
}

@article{balaprakash_autotuning_2018,
  title = {Autotuning in {{High}}-{{Performance Computing Applications}}},
  author = {Balaprakash, Prasanna and Dongarra, Jack and Gamblin, Todd and Hall, Mary and Hollingsworth, Jeffrey K. and Norris, Boyana and Vuduc, Richard},
  year = {2018},
  month = nov,
  volume = {106},
  pages = {2068--2083},
  journal = {Proc. IEEE},
  number = {11}
}

@inproceedings{banicescu_balancing_1995,
  title = {Balancing {{Processor Loads}} and {{Exploiting Data Locality}} in {{N}}-{{Body Simulations}}},
  booktitle = {Supercomputing '95:{{Proceedings}} of the 1995 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  author = {Banicescu, I. and Hummel, S. F.},
  year = {1995},
  month = dec,
  pages = {43--43},
  file = {/home/msca8h/Documents/parallel_scheduling/Banicescu and Hummel - 1995 - Balancing Processor Loads and Exploiting Data Loca.pdf},
  keywords = {Delay,Distributed computing,Dynamic scheduling,exploiting locality,factoring scheduling,Fractals,Load management,multiprocessor load balancing,N-body simulations,Parallel machines,Performance gain,Permission,Processor scheduling,Scheduling algorithm,tiling}
}

@inproceedings{banicescu_load_2002,
  title = {Load Balancing Highly Irregular Computations with the Adaptive Factoring},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}.},
  author = {Banicescu, I. and Velusamy, V.},
  year = {2002},
  pages = {12 pp},
  address = {{Ft. Lauderdale, FL}},
  file = {/home/msca8h/Documents/parallel_scheduling/Banicescu and Velusamy - 2002 - Load balancing highly irregular computations with .pdf},
  series = {{{IPDPS}}'02}
}

@article{banterle_accelerating_2019,
  title = {Accelerating {{Metropolis}}-{{Hastings}} Algorithms by {{Delayed Acceptance}}},
  author = {Banterle, Marco and Grazian, Clara and Lee, Anthony and P. Robert, Christian},
  year = {2019},
  volume = {1},
  pages = {103--128},
  file = {/home/msca8h/Zotero/storage/EQTZFSHG/Banterle et al. - 2019 - Accelerating Metropolis-Hastings algorithms by Del.pdf},
  journal = {Found. Data Sci.},
  language = {en},
  number = {2}
}

@inbook{barbakh_cross_2009,
  title = {Cross Entropy Methods},
  booktitle = {Non-{{Standard Parameter Adaptation}} for {{Exploratory Data Analysis}}},
  author = {Barbakh, Wesam Ashour and Wu, Ying and Fyfe, Colin},
  year = {2009},
  volume = {249},
  pages = {151--174},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  collaborator = {Barbakh, Wesam Ashour and Wu, Ying and Fyfe, Colin}
}

@article{barker_monte_1965,
  title = {Monte {{Carlo}} Calculations of the Radial Distribution Functions for a Proton Electron Plasma},
  shorttitle = {Monte {{Carlo Calculations}} of the {{Radial Distribution Functions}} for a {{Proton}}?},
  author = {Barker, Anthony A.},
  year = {1965},
  volume = {18},
  pages = {119},
  abstract = {A general method is presented for computation of radial distribution functions for plasmas over a wide range of temperatures and densities. The method uses the Monte Carlo technique applied by Wood and Parker, and extends this to long-range forces using results borrowed from crystal lattice theory. The approach is then used to calculate the radial distribution functions for a proton-electron plasma of density 1018 electrons/cm3 at a temperature of 104 OK. The results show the usefulness of the method if sufficient computing facilities are available.},
  file = {/home/msca8h/Zotero/storage/7TZDEJCY/Barker - 1965 - Monte Carlo Calculations of the Radial Distributio.pdf},
  journal = {Aust. J.  Phys.},
  language = {en},
  number = {2}
}

@article{bast_scheduling_2000,
  title = {On Scheduling Parallel Tasks at Twilight},
  author = {Bast, H.},
  year = {2000},
  month = dec,
  volume = {33},
  pages = {489--563},
  file = {/home/msca8h/Documents/parallel_scheduling/Bast - 2000 - On Scheduling Parallel Tasks at Twilight.pdf},
  journal = {Theory Comput. Syst.},
  language = {en},
  number = {5-6}
}

@phdthesis{basthannah_provably_2000,
  title = {Provably Optimal Scheduling of Similar Tasks},
  author = {{Bast, Hannah}},
  year = {2000},
  file = {/home/msca8h/Documents/parallel_scheduling/Bast, Hannah - 2000 - Provably Optimal Scheduling of Similar Tasks.pdf},
  school = {Universit\"at des Saarlandes, Saarbr\"ucken},
  type = {Ph.{{D Thesis}}}
}

@article{basu_making_2013,
  title = {Towards Making Autotuning Mainstream},
  author = {Basu, Protonu and Hall, Mary and Khan, Malik and Maindola, Suchit and Muralidharan, Saurav and Ramalingam, Shreyas and Rivera, Axel and Shantharam, Manu and Venkat, Anand},
  year = {2013},
  month = nov,
  volume = {27},
  pages = {379--393},
  journal = {The International Journal of High Performance Computing Applications},
  language = {en},
  number = {4}
}

@inproceedings{bayat_concurrent_2018,
  title = {Concurrent {{Clutter}} and {{Noise Suppression}} via {{Low Rank Plus Sparse Optimization}} for {{Non}}-{{Contrast Ultrasound Flow Doppler Processing}} in {{Microvasculature}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bayat, Mahdi and Fatemi, Mostafa},
  year = {2018},
  month = apr,
  pages = {1080--1084},
  publisher = {{IEEE}},
  address = {{Calgary, AB}}
}

@inproceedings{baydin_etalumis_2019,
  title = {Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale},
  shorttitle = {Etalumis},
  booktitle = {Proc.  {{Int}}. {{Conf}}. {{High Perform}}. {{Comput}}. {{Networking}}, {{Storage}}, {{Anal}}.},
  author = {Baydin, Atilim G{\"u}ne{\c s} and Shao, Lei and Bhimji, Wahid and Heinrich, Lukas and Meadows, Lawrence and Liu, Jialin and Munk, Andreas and Naderiparizi, Saeid and {Gram-Hansen}, Bradley and Louppe, Gilles and Ma, Mingfei and Zhao, Xiaohui and Torr, Philip and Lee, Victor and Cranmer, Kyle and {Prabhat} and Wood, Frank},
  year = {2019},
  month = nov,
  pages = {1--24},
  address = {{Denver Colorado}},
  file = {/home/msca8h/Zotero/storage/7KAL9GUM/Baydin et al. - 2019 - Etalumis bringing probabilistic programming to sc.pdf},
  language = {en},
  series = {{{SC}}'19}
}

@article{beamer_gap_2017,
  title = {The {{GAP}} Benchmark Suite},
  author = {Beamer, Scott and Asanovi{\'c}, Krste and Patterson, David},
  year = {2017},
  month = may,
  abstract = {We present a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing.},
  archiveprefix = {arXiv},
  eprint = {1508.03619},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/SLDCD2QR/Beamer et al. - 2017 - The GAP Benchmark Suite.pdf;/home/msca8h/Zotero/storage/YRQLAWQG/1508.html},
  journal = {arXiv:1508.03619 [cs]},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Distributed; Parallel; and Cluster Computing},
  primaryclass = {cs}
}

@inproceedings{beckingsale_apollo_2017,
  title = {Apollo: {{Reusable Models}} for {{Fast}}, {{Dynamic Tuning}} of {{Input}}-{{Dependent Code}}},
  shorttitle = {Apollo},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Beckingsale, David and Pearce, Olga and Laguna, Ignacio and Gamblin, Todd},
  year = {2017},
  month = may,
  pages = {307--316},
  publisher = {{IEEE}},
  address = {{Orlando, FL, USA}}
}

@inproceedings{beckingsale_raja_2019,
  title = {{{RAJA}}: {{Portable Performance}} for {{Large}}-{{Scale Scientific Applications}}},
  shorttitle = {{{RAJA}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Performance}}, {{Portability}} and {{Productivity}} in {{HPC}} ({{P3HPC}})},
  author = {Beckingsale, David A. and Scogland, Thomas RW and Burmark, Jason and Hornung, Rich and Jones, Holger and Killian, William and Kunen, Adam J. and Pearce, Olga and Robinson, Peter and Ryujin, Brian S.},
  year = {2019},
  month = nov,
  pages = {71--81},
  publisher = {{IEEE}},
  address = {{Denver, CO, USA}},
  file = {/home/msca8h/Zotero/storage/QL5ZBNKA/Beckingsale et al. - 2019 - RAJA Portable Performance for Large-Scale Scienti.pdf}
}

@inproceedings{behzad_dynamic_2015,
  title = {Dynamic {{Model}}-{{Driven Parallel I}}/{{O Performance Tuning}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Behzad, Babak and Byna, Surendra and Wild, Stefan M. and {Prabhat} and Snir, Marc},
  year = {2015},
  month = sep,
  pages = {184--193},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}}
}

@article{behzad_optimizing_2019,
  title = {Optimizing {{I}}/{{O Performance}} of {{HPC Applications}} with {{Autotuning}}},
  author = {Behzad, Babak and Byna, Surendra and {Prabhat} and Snir, Marc},
  year = {2019},
  month = mar,
  volume = {5},
  pages = {1--27},
  journal = {ACM Trans. Parallel Comput.},
  language = {en},
  number = {4}
}

@inproceedings{belilovsky_greedy_2019,
  title = {Greedy {{Layerwise Learning Can Scale To ImageNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {583--593},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Shallow supervised 1-hidden layer neural networks have a number of favorable properties that make them easier to interpret, analyze, and optimize than their deep counterparts, but lack their representational power. Here we use 1-hidden layer learning problems to sequentially build deep networks layer by layer, which can inherit properties from shallow networks. Contrary to previous approaches using shallow networks, we focus on problems where deep learning is reported as critical for success. We thus study CNNs on image classification tasks using the large-scale ImageNet dataset and the CIFAR-10 dataset. Using a simple set of ideas for architecture and training we find that solving sequential 1-hidden-layer auxiliary problems lead to a CNN that exceeds AlexNet performance on ImageNet. Extending this training methodology to construct individual layers by solving 2-and-3-hidden layer auxiliary problems, we obtain an 11-layer network that exceeds several members of the VGG model family on ImageNet, and can train a VGG-11 model to the same accuracy as end-to-end learning. To our knowledge, this is the first competitive alternative to end-to-end training of CNNs that can scale to ImageNet. We illustrate several interesting properties of these models and conduct a range of experiments to study the properties this training induces on the intermediate layers.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@inproceedings{bell_challenge_2020,
  title = {Challenge on {{Ultrasound Beamforming}} with {{Deep Learning}} ({{CUBDL}})},
  booktitle = {2020 {{IEEE International Ultrasonics Symposium}} ({{IUS}})},
  author = {Bell, Muyinatu A. Lediju and Huang, Jiaqi and Hyun, Dongwoon and Eldar, Yonina C. and {van Sloun}, Ruud and Mischi, Massimo},
  year = {2020},
  month = sep,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}}
}

@article{ben-nun_demystifying_2018,
  title = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}: {{An In}}-{{Depth Concurrency Analysis}}},
  shorttitle = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}},
  author = {{Ben-Nun}, Tal and Hoefler, Torsten},
  year = {2018},
  month = feb,
  abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  archiveprefix = {arXiv},
  eprint = {1802.09941},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/P87CUQF8/Ben-Nun and Hoefler - 2018 - Demystifying Parallel and Distributed Deep Learnin.pdf;/home/msca8h/Zotero/storage/Y96D3EKI/1802.html},
  journal = {ArXiv180209941 Cs},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryclass = {cs}
}

@article{ben-nun_demystifying_2018a,
  title = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}: {{An In}}-{{Depth Concurrency Analysis}}},
  shorttitle = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}},
  author = {{Ben-Nun}, Tal and Hoefler, Torsten},
  year = {2018},
  month = feb,
  abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  archiveprefix = {arXiv},
  eprint = {1802.09941},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/FC6L9TTY/Ben-Nun and Hoefler - 2018 - Demystifying Parallel and Distributed Deep Learnin.pdf;/home/msca8h/Zotero/storage/SS9CEJVR/1802.html},
  journal = {ArXiv180209941 Cs},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryclass = {cs}
}

@incollection{benassi_bayesian_2012,
  title = {Bayesian {{Optimization Using Sequential Monte Carlo}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Benassi, Romain and Bect, Julien and Vazquez, Emmanuel},
  year = {2012},
  volume = {7219},
  pages = {339--342},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Zotero/storage/PAI2I9NZ/Benassi et al. - 2012 - Bayesian Optimization Using Sequential Monte Carlo.pdf},
  language = {en}
}

@incollection{bengio_greedy_2007,
  title = {Greedy {{Layer}}-{{Wise Training}} of {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2007},
  pages = {153--160},
  publisher = {{MIT Press}}
}

@inproceedings{bergstra_machine_2012,
  title = {Machine Learning for Predictive Auto-Tuning with Boosted Regression Trees},
  booktitle = {2012 {{Innovative Parallel Computing}} ({{InPar}})},
  author = {Bergstra, James and Pinto, Nicolas and Cox, David},
  year = {2012},
  month = may,
  pages = {1--9},
  publisher = {{IEEE}},
  address = {{San Jose, CA, USA}}
}

@article{berkenkamp_noregret_2019,
  title = {No-{{Regret Bayesian Optimization}} with {{Unknown Hyperparameters}}},
  author = {Berkenkamp, Felix and Schoellig, Angela P and Krause, Andreas},
  year = {2019},
  volume = {20},
  pages = {50},
  journal = {J. Mach. Learn. Res.}
}

@article{berkenkamp_noregret_2019a,
  title = {No-{{Regret Bayesian Optimization}} with {{Unknown Hyperparameters}}},
  author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
  year = {2019},
  volume = {20},
  pages = {1--24},
  journal = {J. Mach. Learn. Res.},
  number = {50}
}

@article{berkenkamp_noregret_2019b,
  title = {No-{{Regret Bayesian Optimization}} with {{Unknown Hyperparameters}}},
  author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
  year = {2019},
  volume = {20},
  pages = {1--24},
  journal = {J. Mach. Learn. Res.},
  number = {50}
}

@article{bernton_locally_2015,
  title = {Locally Weighted {{Markov}} Chain {{Monte Carlo}}},
  author = {Bernton, Espen and Yang, Shihao and Chen, Yang and Shephard, Neil and Liu, Jun S.},
  year = {2015},
  month = jun,
  abstract = {We propose a weighting scheme for the proposals within Markov chain Monte Carlo algorithms and show how this can improve statistical efficiency at no extra computational cost. These methods are most powerful when combined with multi-proposal MCMC algorithms such as multiple-try Metropolis, which can efficiently exploit modern computer architectures with large numbers of cores. The locally weighted Markov chain Monte Carlo method also improves upon a partial parallelization of the Metropolis-Hastings algorithm via Rao-Blackwellization. We derive the effective sample size of the output of our algorithm and show how to estimate this in practice. Illustrations and examples of the method are given and the algorithm is compared in theory and applications with existing methods.},
  archiveprefix = {arXiv},
  eprint = {1506.08852},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/M2W2VEPJ/Bernton et al. - 2015 - Locally weighted Markov chain Monte Carlo.pdf;/home/msca8h/Zotero/storage/ZPUC7D2R/1506.html},
  journal = {ArXiv150608852 Stat},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@article{beskos_convergence_2016,
  title = {On the Convergence of Adaptive Sequential {{Monte Carlo}} Methods},
  author = {Beskos, Alexandros and Jasra, Ajay and Kantas, Nikolas and Thiery, Alexandre},
  year = {2016},
  month = apr,
  volume = {26},
  pages = {1111--1146},
  file = {/home/msca8h/Zotero/storage/CVPC6354/Beskos et al. - 2016 - On the convergence of adaptive sequential Monte Ca.pdf},
  journal = {Ann. Appl. Probab.},
  language = {en},
  number = {2}
}

@article{beskos_optimal_2013,
  title = {Optimal Tuning of the Hybrid {{Monte Carlo}} Algorithm},
  author = {Beskos, Alexandros and Pillai, Natesh and Roberts, Gareth and {Sanz-Serna}, Jesus-Maria and Stuart, Andrew},
  year = {2013},
  month = nov,
  volume = {19},
  pages = {1501--1534},
  file = {/home/msca8h/Zotero/storage/TWGYDNM6/Beskos et al. - 2013 - Optimal tuning of the hybrid Monte Carlo algorithm.pdf},
  journal = {Bernoulli},
  language = {en},
  number = {5A}
}

@misc{betancourt_bayes_2018,
  title = {Bayes {{Sparse Regression}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = mar,
  abstract = {As complex measurements convolve meaningful phenomena with more and more extraneous phenomena, sparsity is becoming an increasingly prevalent objective in statistical analyses. Much of this zeitgeist has been driven by the success of frequentist methods like compressed sensing and LASSO regression, and it is often naively assumed that these properties immediately carry over to the corresponding Bayesian analyses. Sparsity in a Bayesian analysis, however, is induced by fundamentally different properties than those that induce sparsity in a frequentist analysis. In this case study I'll review how sparsity arises in frequentist and Bayesian analyses and discuss the often subtle challenges in implementing sparsity in practical Bayesian analyses.}
}

@article{betancourt_conceptual_2017,
  title = {A Conceptual Introduction to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2017},
  month = jan,
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/P6BHN8X4/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;/home/msca8h/Zotero/storage/KZ3MH7EI/1701.html},
  journal = {ArXiv170102434 Stat},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@article{betancourt_fundamental_2015,
  title = {The {{Fundamental Incompatibility}} of {{Hamiltonian Monte Carlo}} and {{Data Subsampling}}},
  author = {Betancourt, M. J.},
  year = {2015},
  month = feb,
  abstract = {Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the efficient exploration of Hamiltonian flow and hence the scalable performance of Hamiltonian Monte Carlo itself.},
  archiveprefix = {arXiv},
  eprint = {1502.01510},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/74MTU2DC/Betancourt - 2015 - The Fundamental Incompatibility of Hamiltonian Mon.pdf;/home/msca8h/Zotero/storage/UULTNRI9/1502.html},
  journal = {ArXiv150201510 Stat},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@misc{betancourt_hierarchical_2020,
  title = {Hierarchical {{Modeling}}},
  author = {Betancourt, Michael},
  year = {2020},
  month = nov
}

@article{betancourt_identifying_2016,
  title = {Identifying the Optimal Integration Time in {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2016},
  month = jan,
  abstract = {By leveraging the natural geometry of a smooth probabilistic system, Hamiltonian Monte Carlo yields computationally efficient Markov Chain Monte Carlo estimation. At least provided that the algorithm is sufficiently well-tuned. In this paper I show how the geometric foundations of Hamiltonian Monte Carlo implicitly identify the optimal choice of these parameters, especially the integration time. I then consider the practical consequences of these principles in both existing algorithms and a new implementation called \textbackslash emph\{Exhaustive Hamiltonian Monte Carlo\} before demonstrating the utility of these ideas in some illustrative examples.},
  archiveprefix = {arXiv},
  eprint = {1601.00225},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/6I836MJZ/Betancourt - 2016 - Identifying the Optimal Integration Time in Hamilt.pdf;/home/msca8h/Zotero/storage/82GALAQV/1601.html},
  journal = {ArXiv160100225 Stat},
  keywords = {Statistics - Computation,Statistics - Methodology},
  primaryclass = {stat}
}

@misc{betancourt_identity_2020,
  title = {Identity Crisis},
  author = {Betancourt, Michael},
  year = {2020},
  month = jun,
  abstract = {Under ideal conditions only a small neighborhood of model configurations will be consistent with both the observed data and the domain expertise we encode in our prior model, resulting in a posterior distribution that strongly concentrates along each parameter. This not only yields precise inferences and but also facilitates accurate estimation of those inferences. Under more realistic conditions, however, our measurements and domain expertise can be much less informative, allowing our posterior distribution to stretch across more expansive, complex neighborhoods of the model configuration space. These intricate uncertainties complicate not only the utility of our inferences but also our ability to quantify those inferences computationally. In this case study we will explore the theoretical concept of identifiability and its more geometric counterpart degeneracy that better generalizes to applied statistical practice. We will also discuss the principled ways in which we can identify and then compensate for degenerate inferences before demonstrating these strategies in a series of pedagogical examples.}
}

@article{betancourt_optimizing_2015,
  title = {Optimizing the Integrator Step Size for {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, M. J. and Byrne, Simon and Girolami, Mark},
  year = {2015},
  month = feb,
  abstract = {Hamiltonian Monte Carlo can provide powerful inference in complex statistical problems, but ultimately its performance is sensitive to various tuning parameters. In this paper we use the underlying geometry of Hamiltonian Monte Carlo to construct a universal optimization criteria for tuning the step size of the symplectic integrator crucial to any implementation of the algorithm as well as diagnostics to monitor for any signs of invalidity. An immediate outcome of this result is that the suggested target average acceptance probability of 0.651 can be relaxed to \$0.6 \textbackslash lesssim a \textbackslash lesssim 0.9\$ with larger values more robust in practice.},
  archiveprefix = {arXiv},
  eprint = {1411.6669},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/KSFYXYRW/Betancourt et al. - 2015 - Optimizing The Integrator Step Size for Hamiltonia.pdf;/home/msca8h/Zotero/storage/6A8M6UH9/1411.html},
  journal = {ArXiv14116669 Math Stat},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  primaryclass = {math, stat}
}

@article{bezanson_julia_2017,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  year = {2017},
  volume = {59},
  pages = {65--98},
  journal = {SIAM Rev.},
  number = {1}
}

@article{bhatia_better_2000,
  title = {A Better Bound on the Variance},
  author = {Bhatia, Rajendra and Davis, Chandler},
  year = {2000},
  month = apr,
  volume = {107},
  pages = {353--357},
  journal = {The American Mathematical Monthly},
  language = {en},
  number = {4}
}

@inproceedings{bhattacharyya_pemogen_2014,
  title = {{{PEMOGEN}}: Automatic Adaptive Performance Modeling during Program Runtime},
  shorttitle = {{{PEMOGEN}}},
  booktitle = {Proceedings of the 23rd International Conference on {{Parallel}} Architectures and Compilation - {{PACT}} '14},
  author = {Bhattacharyya, Arnamoy and Hoefler, Torsten},
  year = {2014},
  pages = {393--404},
  publisher = {{ACM Press}},
  address = {{Edmonton, AB, Canada}},
  language = {en}
}

@article{bini_despeckling_2014,
  title = {Despeckling Low {{SNR}}, Low Contrast Ultrasound Images via Anisotropic Level Set Diffusion},
  author = {Bini, A. A. and Bhat, M. S.},
  year = {2014},
  month = jan,
  volume = {25},
  pages = {41--65},
  journal = {Multidim Syst Sign Process},
  language = {en},
  number = {1}
}

@article{blei_variational_2017,
  title = {Variational Inference: {{A}} Review for Statisticians},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  volume = {112},
  pages = {859--877},
  file = {/home/msca8h/Zotero/storage/NM6WQLJH/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {518}
}

@article{botev_markov_2013,
  title = {Markov Chain Importance Sampling with Applications to Rare Event Probability Estimation},
  author = {Botev, Zdravko I. and L'Ecuyer, Pierre and Tuffin, Bruno},
  year = {2013},
  month = mar,
  volume = {23},
  pages = {271--285},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@article{bottenus_resolution_2021,
  title = {Resolution and Speckle Reduction in Cardiac Imaging},
  author = {Bottenus, Nick and LeFevre, Melissa and Cleve, Jayne and Crowley, Anna Lisa and Trahey, Gregg},
  year = {2021},
  month = apr,
  volume = {68},
  pages = {1131--1143},
  journal = {IEEE Trans. Ultrason. Ferroelect. Freq. Contr.},
  number = {4}
}

@incollection{bottou_online_1999,
  title = {On-Line Learning and Stochastic Approximations},
  booktitle = {On-{{Line Learning}} in {{Neural Networks}}},
  author = {Bottou, L{\'e}on},
  year = {1999},
  month = jan,
  edition = {First},
  pages = {9--42},
  publisher = {{Cambridge University Press}},
  file = {/home/msca8h/Zotero/storage/F9CTXQGD/Bottou - 1999 - On-line Learning and Stochastic Approximations.pdf}
}

@article{bottou_optimization_2018,
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2018},
  month = jan,
  volume = {60},
  pages = {223--311},
  file = {/home/msca8h/Zotero/storage/B5Q4DBVW/Bottou et al. - 2018 - Optimization Methods for Large-Scale Machine Learn.pdf},
  journal = {SIAM Rev.},
  language = {en},
  number = {2}
}

@article{braak_markov_2006,
  title = {A {{Markov Chain Monte Carlo}} Version of the Genetic Algorithm {{Differential Evolution}}: Easy {{Bayesian}} Computing for Real Parameter Spaces},
  shorttitle = {A {{Markov Chain Monte Carlo}} Version of the Genetic Algorithm {{Differential Evolution}}},
  author = {Braak, Cajo J. F. Ter},
  year = {2006},
  month = sep,
  volume = {16},
  pages = {239--249},
  journal = {Stat Comput},
  language = {en},
  number = {3}
}

@article{bratley_algorithm_1988,
  title = {{{ALGORITHM}} 659: Implementing {{Sobol}}'s Quasirandom Sequence Generator},
  shorttitle = {{{ALGORITHM}} 659},
  author = {Bratley, Paul and Fox, Bennett L.},
  year = {1988},
  month = mar,
  volume = {14},
  pages = {88--100},
  journal = {ACM Trans. Math. Softw.},
  number = {1}
}

@article{breivik_realtime_2017,
  title = {Real-{{Time}} Nonlocal Means-Based Despeckling},
  author = {Breivik, Lars Hofsoy and Snare, Sten Roar and Steen, Erik Normann and Solberg, Anne H. Schistad},
  year = {2017},
  month = jun,
  volume = {64},
  pages = {959--977},
  journal = {IEEE Trans. Ultrason., Ferroelect., Freq. Contr.},
  number = {6}
}

@inproceedings{brochu_bayesian_2010,
  title = {A {{Bayesian Interactive Optimization Approach}} to {{Procedural Animation Design}}},
  booktitle = {Proc. {{ACM SIGGRAPH}}/{{Eurographics Symp}}. {{Comput}}. {{Animation}}},
  author = {Brochu, Eric and Brochu, Tyson and {de Freitas}, Nando},
  year = {2010},
  pages = {103--112},
  publisher = {{Eurographics Association}},
  address = {{Goslar, DEU}},
  series = {{{SCA}} '10}
}

@article{brochu_tutorial_2010,
  title = {A Tutorial on {{Bayesian}} Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
  author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
  year = {2010},
  volume = {abs/1012.2599},
  file = {/home/msca8h/Documents/bayesian_optimization/Brochu et al. - 2010 - A Tutorial on Bayesian Optimization of Expensive C.pdf},
  journal = {CoRR}
}

@inproceedings{brock_freezeout_2017,
  title = {{{FreezeOut}}: {{Accelerate Training}} by {{Progressively Freezing Layers}}},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, James Millar and Weston, Nicholas J.},
  year = {2017},
  month = dec,
  abstract = {The early layers of a deep neural net have the fewest parameters, but take up themost computation. In this extended abstract, we propose to only train the hidden layers for a set portion of the training run, freezing them out one-by-one and excluding them from the backward pass. We empirically demonstrate that FreezeOut yields savings of up to 20\% wall-clock time during training with 3\% loss in accuracy for DenseNets on CIFAR.},
  keywords = {Machine learning,Neural network},
  language = {English}
}

@article{brockwell_identification_2005,
  title = {Identification of {{Regeneration Times}} in {{MCMC Simulation}}, {{With Application}} to {{Adaptive Schemes}}},
  author = {Brockwell, Anthony E and Kadane, Joseph B},
  year = {2005},
  month = jun,
  volume = {14},
  pages = {436--458},
  file = {/home/msca8h/Zotero/storage/PYU7V8YA/Brockwell and Kadane - 2005 - Identification of Regeneration Times in MCMC Simul.pdf},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {2}
}

@article{brockwell_parallel_2006,
  title = {Parallel {{Markov}} Chain {{Monte Carlo}} Simulation by Pre-Fetching},
  author = {Brockwell, A. E},
  year = {2006},
  month = mar,
  volume = {15},
  pages = {246--261},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {1}
}

@article{broquedis_forestgomp_2010,
  title = {{{ForestGOMP}}: {{An Efficient OpenMP Environment}} for {{NUMA Architectures}}},
  shorttitle = {{{ForestGOMP}}},
  author = {Broquedis, Fran{\c c}ois and Furmento, Nathalie and Goglin, Brice and Wacrenier, Pierre-Andr{\'e} and Namyst, Raymond},
  year = {2010},
  month = oct,
  volume = {38},
  pages = {418--439},
  file = {/home/msca8h/Zotero/storage/H9IAWHUD/Broquedis et al. - 2010 - ForestGOMP An Efficient OpenMP Environment for NU.pdf},
  journal = {Int J Parallel Prog},
  language = {en},
  number = {5-6}
}

@article{bruel_autotuning_2017,
  title = {Autotuning {{CUDA}} Compiler Parameters for Heterogeneous Applications Using the {{OpenTuner}} Framework: {{WSCAD15}}},
  shorttitle = {Autotuning {{CUDA}} Compiler Parameters for Heterogeneous Applications Using the {{OpenTuner}} Framework},
  author = {Bruel, Pedro and Amar{\'i}s, Marcos and Goldman, Alfredo},
  year = {2017},
  month = nov,
  volume = {29},
  pages = {e3973},
  journal = {Concurrency Computat.: Pract. Exper.},
  language = {en},
  number = {22}
}

@inproceedings{bruel_autotuning_2019,
  title = {Autotuning {{Under Tight Budget Constraints}}: {{A Transparent Design}} of {{Experiments Approach}}},
  shorttitle = {Autotuning {{Under Tight Budget Constraints}}},
  booktitle = {2019 19th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGRID}})},
  author = {Bruel, Pedro and Quinito Masnada, Steven and Videau, Brice and Legrand, Arnaud and Vincent, Jean-Marc and Goldman, Alfredo},
  year = {2019},
  month = may,
  pages = {147--156},
  publisher = {{IEEE}},
  address = {{Larnaca, Cyprus}},
  file = {/home/msca8h/Zotero/storage/YAP4WEYQ/Bruel et al. - 2019 - Autotuning Under Tight Budget Constraints A Trans.pdf}
}

@article{buchholz_adaptive_2018,
  title = {Adaptive {{Tuning Of Hamiltonian Monte Carlo Within Sequential Monte Carlo}}},
  author = {Buchholz, Alexander and Chopin, Nicolas and Jacob, Pierre E.},
  year = {2018},
  month = aug,
  abstract = {Sequential Monte Carlo (SMC) samplers form an attractive alternative to MCMC for Bayesian computation. However, their performance depends strongly on the Markov kernels used to re- juvenate particles. We discuss how to calibrate automatically (using the current particles) Hamiltonian Monte Carlo kernels within SMC. To do so, we build upon the adaptive SMC ap- proach of Fearnhead and Taylor (2013), and we also suggest alternative methods. We illustrate the advantages of using HMC kernels within an SMC sampler via an extensive numerical study.},
  archiveprefix = {arXiv},
  eprint = {1808.07730},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/P74SRNF6/Buchholz et al. - 2018 - Adaptive Tuning Of Hamiltonian Monte Carlo Within .pdf;/home/msca8h/Zotero/storage/JYD9MGZA/1808.html},
  journal = {ArXiv180807730 Stat},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@article{buchholz_adaptive_2020,
  title = {Adaptive {{Tuning}} of {{Hamiltonian Monte Carlo Within Sequential Monte Carlo}}},
  author = {Buchholz, Alexander and Chopin, Nicolas and Jacob, Pierre E.},
  year = {2020},
  month = jul,
  file = {/home/msca8h/Zotero/storage/ZSWJFAKX/Buchholz et al. - 2020 - Adaptive Tuning of Hamiltonian Monte Carlo Within .pdf},
  journal = {Bayesian Anal.},
  language = {en}
}

@book{buder_lapesd_,
  title = {{{LaPeSD LibGOMP}}},
  author = {Buder, Patrick A.}
}

@article{bugallo_adaptive_2017,
  title = {Adaptive Importance Sampling: The Past, the Present, and the Future},
  shorttitle = {Adaptive {{Importance Sampling}}},
  author = {Bugallo, Monica F. and Elvira, Victor and Martino, Luca and Luengo, David and Miguez, Joaquin and Djuric, Petar M.},
  year = {2017},
  month = jul,
  volume = {34},
  pages = {60--79},
  journal = {IEEE Signal Process. Mag.},
  number = {4}
}

@incollection{bui_streaming_2017,
  title = {Streaming {{Sparse Gaussian Process Approximations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Bui, Thang D and Nguyen, Cuong and Turner, Richard E},
  year = {2017},
  pages = {3299--3307},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{bull_feedback_1998,
  title = {Feedback Guided Dynamic Loop Scheduling: {{Algorithms}} and Experiments},
  shorttitle = {Feedback Guided Dynamic Loop Scheduling},
  booktitle = {Euro-{{Par}}'98 {{Parallel Processing}}},
  author = {Bull, J. Mark},
  year = {1998},
  volume = {1470},
  pages = {377--382},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Documents/parallel_scheduling/bull1998.pdf;/home/msca8h/Zotero/storage/QSJV2E7S/Bull - 1998 - Feedback guided dynamic loop scheduling Algorithm.pdf}
}

@inproceedings{bull_measuring_1999,
  title = {Measuring Synchronisation and Scheduling Overheads in {{OpenMP}}},
  booktitle = {Proc. 1st {{Eur}}. {{Workshop OpenMP}}},
  author = {Bull, J. M.},
  year = {1999},
  pages = {99--105},
  file = {/home/msca8h/Documents/parallel_scheduling/Bull - 1999 - Measuring Synchronisation and Scheduling Overheads.pdf},
  series = {{{IWOMP}}'99}
}

@article{burgess_understanding_2018,
  title = {Understanding Disentangling in \$\textbackslash beta\$-{{VAE}}},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  year = {2018},
  month = apr,
  abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in \$\textbackslash beta\$-VAE, as training progresses. From these insights, we propose a modification to the training regime of \$\textbackslash beta\$-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \$\textbackslash beta\$-VAE, without the previous trade-off in reconstruction accuracy.},
  archiveprefix = {arXiv},
  eprint = {1804.03599},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/PGMMNJYJ/Burgess et al. - 2018 - Understanding disentangling in $beta$-VAE.pdf;/home/msca8h/Zotero/storage/QSU2LBVW/1804.html},
  journal = {ArXiv180403599 Cs Stat},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{burke_learning_2020,
  title = {Learning Rewards for Robotic Ultrasound Scanning Using Probabilistic Temporal Ranking},
  author = {Burke, Michael and Lu, Katie and Angelov, Daniel and Strai{\v z}ys, Art{\=u}ras and Innes, Craig and Subr, Kartic and Ramamoorthy, Subramanian},
  year = {2020},
  month = may,
  abstract = {This paper addresses a common class of problems where a robot learns to perform a discovery task based on example solutions, or \textbackslash emph\{human demonstrations\}. As an example, this work considers the problem of ultrasound scanning, where a demonstration involves an expert adaptively searching for a satisfactory view of internal organs, vessels or tissue and potential anomalies while maintaining optimal contact between the probe and surface tissue. Such problems are often solved by inferring notional \textbackslash emph\{rewards\} that, when optimised for, result in a plan that mimics demonstrations. A pivotal assumption, that plans with higher reward should be exponentially more likely, leads to the de facto approach for reward inference in robotics. While this approach of maximum entropy inverse reinforcement learning leads to a general and elegant formulation, it struggles to cope with frequently encountered sub-optimal demonstrations. In this paper, we propose an alternative approach to cope with the class of problems where sub-optimal demonstrations occur frequently. We hypothesise that, in tasks which require discovery, successive states of any demonstration are progressively more likely to be associated with a higher reward. We formalise this \textbackslash emph\{temporal ranking\} approach and show that it improves upon maximum-entropy approaches to perform reward inference for autonomous ultrasound scanning, a novel application of learning from demonstration in medical imaging.},
  archiveprefix = {arXiv},
  eprint = {2002.01240},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/FXSDY8YH/Burke et al. - 2020 - Learning rewards for robotic ultrasound scanning u.pdf;/home/msca8h/Zotero/storage/UQX3PSYE/2002.html},
  journal = {ArXiv200201240 Cs},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  primaryclass = {cs}
}

@article{burt_laplacian_1983,
  title = {The {{Laplacian}} Pyramid as a Compact Image Code},
  author = {Burt, P. and Adelson, E.},
  year = {1983},
  month = apr,
  volume = {31},
  pages = {532--540},
  journal = {IEEE Trans. Commun.},
  language = {en},
  number = {4}
}

@inproceedings{byrd_parallelisation_2010,
  title = {On the Parallelisation of {{MCMC}} by Speculative Chain Execution},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}. {{Workshop}}  {{Phd Forum}}},
  author = {Byrd, Jonathan M R and Jarvis, Stephen A and Bhalerao, Abhir H},
  year = {2010},
  month = apr,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Atlanta, GA}},
  file = {/home/msca8h/Zotero/storage/AEW2NJKI/Byrd et al. - 2010 - On the parallelisation of MCMC by speculative chai.pdf},
  series = {{{IPDPSW}}'10}
}

@inproceedings{byrd_reducing_2008,
  title = {Reducing the Run-Time of {{MCMC}} Programs by Multithreading on {{SMP}} Architectures},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}.},
  author = {Byrd, Jonathan M. R. and Jarvis, Stephen A. and Bhalerao, Abhir H.},
  year = {2008},
  month = apr,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Miami, FL, USA}},
  file = {/home/msca8h/Zotero/storage/GTUHTAJZ/Byrd et al. - 2008 - Reducing the run-time of MCMC programs by multithr.pdf},
  series = {{{IPDPS}}'08}
}

@inproceedings{cadena_how_2019,
  title = {How Well Do Deep Neural Networks Trained on Object Recognition Characterize the Mouse Visual System?},
  booktitle = {{{NeurIPS Neuro AI Workshop}}},
  author = {Cadena, S. A. and Sinz, F. H. and Muhammad, T. and Froudarakis, E. and Cobos, E. and Walker, E. Y. and Reimer, J. and Bethge, M. and Tolias, A. and Ecker, A. S.},
  year = {2019},
  keywords = {deep neural networks,goal-driven modeling,hierarchical organization,mouse visual cortex,object recognition}
}

@article{calderhead_general_2014,
  title = {A General Construction for Parallelizing {{Metropolis}}-{{Hastings}} Algorithms},
  author = {Calderhead, Ben},
  year = {2014},
  month = dec,
  volume = {111},
  pages = {17408--17413},
  journal = {Proc. Nat. Acad. Sci.},
  language = {en},
  number = {49},
  series = {{{PNAS}}}
}

@article{cappe_adaptive_2008,
  title = {Adaptive Importance Sampling in General Mixture Classes},
  author = {Capp{\'e}, Olivier and Douc, Randal and Guillin, Arnaud and Marin, Jean-Michel and Robert, Christian P.},
  year = {2008},
  month = dec,
  volume = {18},
  pages = {447--459},
  file = {/home/msca8h/Zotero/storage/QI9CG3WZ/Cappé et al. - 2008 - Adaptive importance sampling in general mixture cl.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {4}
}

@article{cappe_overview_2007,
  title = {An {{Overview}} of {{Existing Methods}} and {{Recent Advances}} in {{Sequential Monte Carlo}}},
  author = {Capp{\'e}, Olivier and Godsill, Simon J and Moulines, Eric},
  year = {2007},
  volume = {95},
  pages = {899--924},
  file = {/home/msca8h/Zotero/storage/7BQ8YWN3/Cappé et al. - 2007 - An overview of existing methods and recent advance.pdf},
  journal = {Proc. IEEE},
  number = {5}
}

@article{cappe_overview_2007a,
  title = {An {{Overview}} of {{Existing Methods}} and {{Recent Advances}} in {{Sequential Monte Carlo}}},
  author = {Cappe, O. and Godsill, S. J. and Moulines, E.},
  year = {2007},
  month = may,
  volume = {95},
  pages = {899--924},
  file = {/home/msca8h/Zotero/storage/KQWQP35W/Cappe et al. - 2007 - An Overview of Existing Methods and Recent Advance.pdf},
  journal = {Proc. IEEE},
  keywords = {and smoothing,Bayesian dynamical model,Bayesian dynamical models,Computer vision,Computerized monitoring,filtering,Filtering,filtering theory,hidden Markov models,Hidden Markov models,image processing,Monte Carlo methods,parameter estimation,particle filter,Pollution,prediction,Predictive models,Probability density function,sequential Monte Carlo,signal processing,Signal processing,Sliding mode control,SMC algorithms,state-space model,tracking},
  number = {5}
}

@article{cappe_population_2004,
  title = {Population {{Monte Carlo}}},
  author = {Capp{\'e}, O and Guillin, A and Marin, J. M and Robert, C. P},
  year = {2004},
  month = dec,
  volume = {13},
  pages = {907--929},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {4}
}

@inproceedings{carastan-santos_obtaining_2017,
  title = {Obtaining Dynamic Scheduling Policies with Simulation and Machine Learning},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} on - {{SC}} '17},
  author = {{Carastan-Santos}, Danilo and {de Camargo}, Raphael Y.},
  year = {2017},
  pages = {1--13},
  publisher = {{ACM Press}},
  address = {{Denver, Colorado}},
  file = {/home/msca8h/Zotero/storage/LBXXK2IW/Carastan-Santos and de Camargo - 2017 - Obtaining dynamic scheduling policies with simulat.pdf},
  language = {en}
}

@inproceedings{carino_dynamic_2002,
  title = {Dynamic Scheduling Parallel Loops with Variable Iterate Execution Times},
  booktitle = {Proceedings 16th {{International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Carino, R.L. and Banicescu, I.},
  year = {2002},
  pages = {8 pp},
  publisher = {{IEEE}},
  address = {{Ft. Lauderdale, FL}},
  file = {/home/msca8h/Zotero/storage/EKD9L7GR/Carino and Banicescu - 2002 - Dynamic scheduling parallel loops with variable it.pdf}
}

@article{carino_dynamic_2008,
  title = {Dynamic Load Balancing with Adaptive Factoring Methods in Scientific Applications},
  author = {Cari{\~n}o, Ricolindo L. and Banicescu, Ioana},
  year = {2008},
  month = apr,
  volume = {44},
  pages = {41--63},
  abstract = {To improve the performance of scientific applications with parallel loops, dynamic loop scheduling methods have been proposed. Such methods address performance degradations due to load imbalance caused by predictable phenomena like nonuniform data distribution or algorithmic variance, and unpredictable phenomena such as data access latency or operating system interference. In particular, methods such as factoring, weighted factoring, adaptive weighted factoring, and adaptive factoring have been developed based on a probabilistic analysis of parallel loop iterates with variable running times. These methods have been successfully implemented in a number of applications such as: N-Body and Monte Carlo simulations, computational fluid dynamics, and radar signal processing.},
  file = {/home/msca8h/Documents/parallel_scheduling/Cariño and Banicescu - 2008 - Dynamic load balancing with adaptive factoring met.pdf},
  journal = {J. Supercomput.},
  number = {1}
}

@article{carpenter_stan_2017,
  title = {Stan: {{A}} Probabilistic Programming Language},
  shorttitle = {{\emph{Stan}}},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  volume = {76},
  file = {/home/msca8h/Zotero/storage/5KD8YE7R/Carpenter et al. - 2017 - Stan  A Probabilistic Programming Language.pdf},
  journal = {J. Stat. Soft.},
  language = {en},
  number = {1}
}

@article{carvalho_particle_2010,
  title = {Particle {{Learning}} and {{Smoothing}}},
  author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
  year = {2010},
  volume = {25},
  pages = {88--106},
  abstract = {Particle learning (PL) provides state filtering, sequential parameter learning and smoothing in a general class of state space models. Our approach extends existing particle methods by incorporating the estimation of static parameters via a fully-adapted filter that utilizes conditional sufficient statistics for parameters and/or states as particles. State smoothing in the presence of parameter uncertainty is also solved as a by-product of PL. In a number of examples, we show that PL outperforms existing particle filtering alternatives and proves to be a competitor to MCMC.},
  journal = {Stat. Sci.},
  number = {1}
}

@article{carvalho_particle_2010a,
  title = {Particle {{Learning}} and {{Smoothing}}},
  author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
  year = {2010},
  month = feb,
  volume = {25},
  pages = {88--106},
  file = {/home/msca8h/Zotero/storage/QF2GT4LF/Carvalho et al. - 2010 - Particle Learning and Smoothing.pdf},
  journal = {Statist. Sci.},
  language = {en},
  number = {1}
}

@article{carvalho_particle_2010b,
  title = {Particle {{Learning}} and {{Smoothing}}},
  author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
  year = {2010},
  month = feb,
  volume = {25},
  pages = {88--106},
  file = {/home/msca8h/Zotero/storage/YLZTBG3Q/Carvalho et al. - 2010 - Particle Learning and Smoothing.pdf},
  journal = {Statist. Sci.},
  language = {en},
  number = {1}
}

@book{casella_statistical_1990,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  year = {1990},
  publisher = {{Brooks/Cole Pub. Co}},
  address = {{Pacific Grove, Calif}},
  keywords = {Mathematical statistics,Probabilities},
  lccn = {QA276 .C37 1990},
  series = {The {{Wadsworth}} \& {{Brooks}}/{{Cole Statistics}}/{{Probability}} Series}
}

@inproceedings{chan_autotuning_2009,
  title = {Autotuning Multigrid with {{PetaBricks}}},
  booktitle = {Proceedings of the {{Conference}} on {{High Performance Computing Networking}}, {{Storage}} and {{Analysis}} - {{SC}} '09},
  author = {Chan, Cy and Ansel, Jason and Wong, Yee Lok and Amarasinghe, Saman and Edelman, Alan},
  year = {2009},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon}},
  file = {/home/msca8h/Zotero/storage/S8LD2X2V/Chan et al. - 2009 - Autotuning multigrid with PetaBricks.pdf},
  language = {en}
}

@inproceedings{chang_domainspecific_2019,
  title = {Domain-{{Specific Batch Normalization}} for {{Unsupervised Domain Adaptation}}},
  booktitle = {The {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chang, Woong-Gi and You, Tackgeun and Seo, Seonguk and Kwak, Suha and Han, Bohyung},
  year = {2019},
  month = jun
}

@inproceedings{chang_multiple_2005,
  title = {Multiple Object Tracking with Kernel Particle Filter},
  booktitle = {Proceedings of the {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern}}},
  author = {Chang, Cheng and Ansari, R. and Khokhar, A.},
  year = {2005},
  volume = {1},
  pages = {566--573},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}}
}

@article{chang_usefulness_2021,
  title = {Usefulness of Machine Learning-Based Detection and Classification of Cardiac Arrhythmias with 12-Lead Electrocardiograms},
  author = {Chang, Kuan-Cheng and Hsieh, Po-Hsin and Wu, Mei-Yao and Wang, Yu-Chen and Chen, Jan-Yow and Tsai, Fuu-Jen and Shih, Edward S.C. and Hwang, Ming-Jing and Huang, Tzung-Chi},
  year = {2021},
  month = jan,
  volume = {37},
  pages = {94--104},
  journal = {Canadian Journal of Cardiology},
  language = {en},
  number = {1}
}

@article{chatterjee_sample_2018,
  title = {The Sample Size Required in Importance Sampling},
  author = {Chatterjee, Sourav and Diaconis, Persi},
  year = {2018},
  month = apr,
  volume = {28},
  file = {/home/msca8h/Zotero/storage/B7CLIKC7/Chatterjee and Diaconis - 2018 - The sample size required in importance sampling.pdf},
  journal = {Ann. Appl. Probab.},
  number = {2}
}

@inproceedings{che_characterization_2010,
  title = {A Characterization of the {{Rodinia}} Benchmark Suite with Comparison to Contemporary {{CMP}} Workloads},
  booktitle = {Proc. {{IEEE Int}}. {{Symp}}. {{Workload Characterization}}},
  author = {Che, Shuai and Sheaffer, Jeremy W. and Boyer, Michael and Szafaryn, Lukasz G. and {Liang Wang} and Skadron, Kevin},
  year = {2010},
  month = dec,
  pages = {1--11},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  file = {/home/msca8h/Zotero/storage/SFWW88D6/Che et al. - 2010 - A characterization of the Rodinia benchmark suite .pdf},
  series = {{{IISWC}}'10}
}

@article{chen_accelerated_2016,
  title = {Accelerated {{Dimension}}-{{Independent Adaptive Metropolis}}},
  author = {Chen, Yuxin and Keyes, David and Law, Kody J. H. and Ltaief, Hatem},
  year = {2016},
  month = jan,
  volume = {38},
  pages = {S539-S565},
  file = {/home/msca8h/Zotero/storage/53VNYW45/Chen et al. - 2016 - Accelerated Dimension-Independent Adaptive Metropo.pdf},
  journal = {SIAM J. Sci. Comput.},
  language = {en},
  number = {5}
}

@article{chen_adaptive_2013,
  title = {Adaptive {{Cache Aware Bitier Work}}-{{Stealing}} in {{Multisocket Multicore Architectures}}},
  author = {Chen, Quan and Guo, Minyi and Huang, Zhiyi},
  year = {2013},
  month = dec,
  volume = {24},
  pages = {2334--2343},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {12}
}

@article{chen_contention_2018,
  title = {Contention and {{Locality}}-{{Aware Work}}-{{Stealing}} for {{Iterative Applications}} in {{Multi}}-{{Socket Computers}}},
  author = {Chen, Quan and Guo, Minyi},
  year = {2018},
  month = jun,
  volume = {67},
  pages = {784--798},
  file = {/home/msca8h/Documents/parallel_scheduling/Chen and Guo - 2018 - Contention and Locality-Aware Work-Stealing for It.pdf;/home/msca8h/Documents/parallel_scheduling/Chen and Guo - 2018 - Contention and Locality-Aware Work-Stealing for It.pdf},
  journal = {IEEE Trans. Comput.},
  number = {6}
}

@article{chen_fast_2019,
  title = {Fast Mixing of {{Metropolized Hamiltonian Monte Carlo}}: {{Benefits}} of Multi-Step Gradients},
  shorttitle = {Fast Mixing of {{Metropolized Hamiltonian Monte Carlo}}},
  author = {Chen, Yuansi and Dwivedi, Raaz and Wainwright, Martin J. and Yu, Bin},
  year = {2019},
  month = may,
  abstract = {Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo sampling algorithm for drawing samples from smooth probability densities over continuous spaces. We study the variant most widely used in practice, Metropolized HMC with the St\textbackslash "\{o\}rmer-Verlet or leapfrog integrator, and make two primary contributions. First, we provide a non-asymptotic upper bound on the mixing time of the Metropolized HMC with explicit choices of stepsize and number of leapfrog steps. This bound gives a precise quantification of the faster convergence of Metropolized HMC relative to simpler MCMC algorithms such as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we provide a general framework for sharpening mixing time bounds Markov chains initialized at a substantial distance from the target distribution over continuous spaces. We apply this sharpening device to the Metropolized random walk and Langevin algorithms, thereby obtaining improved mixing time bounds from a non-warm initial distribution.},
  archiveprefix = {arXiv},
  eprint = {1905.12247},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/BS8WR7KS/Chen et al. - 2019 - Fast mixing of Metropolized Hamiltonian Monte Carl.pdf;/home/msca8h/Zotero/storage/MPFZESKL/1905.html},
  journal = {ArXiv190512247 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{chen_pairwise_2013,
  title = {Pairwise Ranking Aggregation in a Crowdsourced Setting},
  booktitle = {Proceedings of the Sixth {{ACM}} International Conference on {{Web}} Search and Data Mining},
  author = {Chen, Xi and Bennett, Paul N. and {Collins-Thompson}, Kevyn and Horvitz, Eric},
  year = {2013},
  month = feb,
  pages = {193--202},
  publisher = {{Association for Computing Machinery}},
  address = {{Rome, Italy}},
  abstract = {Inferring rankings over elements of a set of objects, such as documents or images, is a key learning problem for such important applications as Web search and recommender systems. Crowdsourcing services provide an inexpensive and efficient means to acquire preferences over objects via labeling by sets of annotators. We propose a new model to predict a gold-standard ranking that hinges on combining pairwise comparisons via crowdsourcing. In contrast to traditional ranking aggregation methods, the approach learns about and folds into consideration the quality of contributions of each annotator. In addition, we minimize the cost of assessment by introducing a generalization of the traditional active learning scenario to jointly select the annotator and pair to assess while taking into account the annotator quality, the uncertainty over ordering of the pair, and the current model uncertainty. We formalize this as an active learning strategy that incorporates an exploration-exploitation tradeoff and implement it using an efficient online Bayesian updating scheme. Using simulated and real-world data, we demonstrate that the active learning strategy achieves significant reductions in labeling cost while maintaining accuracy.},
  keywords = {crowdsourcing,pairwise preference,ranking},
  series = {{{WSDM}} '13}
}

@article{ching_transitional_2007,
  title = {Transitional {{Markov Chain Monte Carlo Method}} for {{Bayesian Model Updating}}, {{Model Class Selection}}, and {{Model Averaging}}},
  author = {Ching, Jianye and Chen, Yi-Chu},
  year = {2007},
  month = jul,
  volume = {133},
  pages = {816--832},
  journal = {J. Eng. Mech.},
  language = {en},
  number = {7}
}

@inproceedings{choi_modeldriven_2010,
  title = {Model-Driven Autotuning of Sparse Matrix-Vector Multiply on {{GPUs}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN}} Symposium on {{Principles}} and Practice of Parallel Programming - {{PPoPP}} '10},
  author = {Choi, Jee W. and Singh, Amik and Vuduc, Richard W.},
  year = {2010},
  pages = {115},
  publisher = {{ACM Press}},
  address = {{Bangalore, India}},
  file = {/home/msca8h/Zotero/storage/TZTYX998/Choi et al. - 2010 - Model-driven autotuning of sparse matrix-vector mu.pdf},
  language = {en}
}

@article{chopin_central_2004,
  title = {Central Limit Theorem for Sequential {{Monte Carlo}} Methods and Its Application to {{Bayesian}} Inference},
  author = {Chopin, Nicolas},
  year = {2004},
  month = dec,
  volume = {32},
  pages = {2385--2411},
  file = {/home/msca8h/Zotero/storage/KZWFGFEG/Chopin - 2004 - Central limit theorem for sequential Monte Carlo m.pdf},
  journal = {Ann. Statist.},
  language = {en},
  number = {6}
}

@book{chopin_introduction_2020,
  title = {An {{Introduction}} to {{Sequential Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  year = {2020},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  language = {en},
  series = {Springer {{Series}} in {{Statistics}}}
}

@article{chopin_sequential_2002,
  title = {A Sequential Particle Filter Method for Static Models},
  author = {Chopin, Nicolas},
  year = {2002},
  month = aug,
  volume = {89},
  pages = {539--552},
  abstract = {Particle filter methods are complex inference procedures, which combine importance sampling and Monte Carlo schemes in order to explore consistently a sequence of multiple distributions of interest. We show that such methods can also offer an efficient estimation tool in `static' set-ups, in which case {$\pi$}(\texttheta{} | y1, \ldots, yN) (n \&lt; N) is the only posterior distribution of interest but the preliminary exploration of partial posteriors {$\pi$}(\texttheta{} | y1, \ldots, yn) makes it possible to save computing time. A complete algorithm is proposed for independent or Markov models. Our method is shown to challenge other common estimation procedures in terms of robustness and execution time, especially when the sample size is important. Two classes of examples, mixture models and discrete generalised linear models, are discussed and illustrated by numerical results.},
  journal = {Biometrika},
  number = {3}
}

@article{chow_review_2016,
  title = {Review of Medical Image Quality Assessment},
  author = {Chow, Li Sze and Paramesran, Raveendran},
  year = {2016},
  month = may,
  volume = {27},
  pages = {145--154},
  journal = {Biomed. Signal Process. Control},
  language = {en}
}

@article{christen_general_2010,
  title = {A General Purpose Sampling Algorithm for Continuous Distributions (the t-Walk)},
  author = {Christen, J. Andr{\'e}s and Fox, Colin},
  year = {2010},
  month = jun,
  volume = {5},
  pages = {263--281},
  file = {/home/msca8h/Zotero/storage/L24GLPZT/Christen and Fox - 2010 - A general purpose sampling algorithm for continuou.pdf},
  journal = {Bayesian Anal.},
  language = {en},
  number = {2}
}

@inproceedings{christen_patus_2011,
  title = {{{PATUS}}: {{A Code Generation}} and {{Autotuning Framework}} for {{Parallel Iterative Stencil Computations}} on {{Modern Microarchitectures}}},
  shorttitle = {{{PATUS}}},
  booktitle = {2011 {{IEEE International Parallel}} \& {{Distributed Processing Symposium}}},
  author = {Christen, Matthias and Schenk, Olaf and Burkhart, Helmar},
  year = {2011},
  month = may,
  pages = {676--687},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}}
}

@inproceedings{chung_case_2006,
  title = {A {{Case Study Using Automatic Performance Tuning}} for {{Large}}-{{Scale Scientific Programs}}},
  booktitle = {2006 15th {{IEEE International Conference}} on {{High Performance Distributed Computing}}},
  author = {Chung, I.-H. and Hollingsworth, J.K.},
  year = {2006},
  pages = {45--56},
  publisher = {{IEEE}},
  address = {{Paris, France}}
}

@inproceedings{ciorba_openmp_2018,
  title = {{{OpenMP}} Loop Scheduling Revisited: Making a Case for More Schedules},
  booktitle = {Evolving {{OpenMP}} for {{Evolving Architectures}}},
  author = {Ciorba, F. M. and Iwainsky, C. and Buder, P.},
  year = {2018},
  pages = {21--36},
  publisher = {{Springer}},
  abstract = {In light of continued advances in loop scheduling, this work revisits the OpenMP loop scheduling by outlining the current state of the art in loop scheduling and presenting evidence that the existing OpenMP schedules are insufficient for all combinations of applications, systems, and their characteristics. A review of the state of the art shows that due to the specifics of the parallel applications, the variety of computing platforms, and the numerous performance degradation factors, no single loop scheduling technique can be a 'one-fits-all' solution to effectively optimize the performance of all parallel applications in all situations. The impact of irregularity in computational workloads and hardware systems, including operating system noise, on the performance of parallel applications, results in performance loss and has often been neglected in loop scheduling research, in particular, the context of OpenMP schedules. Existing dynamic loop self-scheduling techniques, such as trapezoid self-scheduling, factoring, and weighted factoring, offer an unexplored potential to alleviate this degradation in OpenMP due to the fact that they explicitly target the minimization of load imbalance and scheduling overhead. Through theoretical and experimental evaluation, this work shows that these loop self-scheduling methods provide a benefit in the context of OpenMP. In conclusion, OpenMP must include more schedules to offer a broader performance coverage of applications executing on an increasing variety of heterogeneous shared memory computing platforms.},
  file = {/home/msca8h/Zotero/storage/G4QJBAKP/OMP_scheduling_2018.pdf},
  series = {{{IWOMP}}'18}
}

@article{clintwhaley_automated_2001,
  title = {Automated Empirical Optimizations of Software and the {{ATLAS}} Project},
  author = {Clint Whaley, R. and Petitet, Antoine and Dongarra, Jack J.},
  year = {2001},
  month = jan,
  volume = {27},
  pages = {3--35},
  journal = {Parallel Computing},
  language = {en},
  number = {1-2}
}

@article{cobb_scaling_2020,
  title = {Scaling {{Hamiltonian Monte Carlo}} Inference for {{Bayesian}} Neural Networks with Symmetric Splitting},
  author = {Cobb, Adam D. and Jalaian, Brian},
  year = {2020},
  month = oct,
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) approach that exhibits favourable exploration properties in high-dimensional models such as neural networks. Unfortunately, HMC has limited use in large-data regimes and little work has explored suitable approaches that aim to preserve the entire Hamiltonian. In our work, we introduce a new symmetric integration scheme for split HMC that does not rely on stochastic gradients. We show that our new formulation is more efficient than previous approaches and is easy to implement with a single GPU. As a result, we are able to perform full HMC over common deep learning architectures using entire data sets. In addition, when we compare with stochastic gradient MCMC, we show that our method achieves better performance in both accuracy and uncertainty quantification. Our approach demonstrates HMC as a feasible option when considering inference schemes for large-scale machine learning problems.},
  archiveprefix = {arXiv},
  eprint = {2010.06772},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/QLSNMQ5R/Cobb and Jalaian - 2020 - Scaling Hamiltonian Monte Carlo Inference for Baye.pdf;/home/msca8h/Zotero/storage/NNW94ZQ6/2010.html},
  journal = {ArXiv201006772 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{cochran_sampling_1977,
  title = {Sampling Techniques},
  author = {Cochran, William Gemmell},
  year = {1977},
  edition = {3d ed},
  publisher = {{Wiley}},
  address = {{New York}},
  keywords = {Sampling (Statistics)},
  lccn = {QA276.6 .C6 1977},
  series = {Wiley Series in Probability and Mathematical Statistics}
}

@article{cohen_randomout_2017,
  title = {{{RandomOut}}: {{Using}} a Convolutional Gradient Norm to Rescue Convolutional Filters},
  shorttitle = {{{RandomOut}}},
  author = {Cohen, Joseph Paul and Lo, Henry Z. and Ding, Wei},
  year = {2017},
  month = may,
  abstract = {Filters in convolutional neural networks are sensitive to their initialization. The random numbers used to initialize filters are a bias and determine if you will "win" and converge to a satisfactory local minimum so we call this The Filter Lottery. We observe that the 28x28 Inception-V3 model without Batch Normalization fails to train 26\% of the time when varying the random seed alone. This is a problem that affects the trial and error process of designing a network. Because random seeds have a large impact it makes it hard to evaluate a network design without trying many different random starting weights. This work aims to reduce the bias imposed by the initial weights so a network converges more consistently. We propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold. This consistently improves accuracy on the 28x28 Inception-V3 with a median increase of +3.3\%. In effect our method RandomOut increases the number of filters explored without increasing the size of the network. We observe that the RandomOut method has more consistent generalization performance, having a standard deviation of 1.3\% instead of 2\% when varying random seeds, and does so faster and with fewer parameters.},
  archiveprefix = {arXiv},
  eprint = {1602.05931},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/IBJNGDRC/Cohen et al. - 2017 - RandomOut Using a convolutional gradient norm to .pdf;/home/msca8h/Zotero/storage/FRJ2TCAR/1602.html},
  journal = {ArXiv160205931 Cs},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@inproceedings{coll-font_ecgbased_2017,
  title = {{{ECG}}-Based Reconstruction of Heart Position and Orientation with {{Bayesian}} Optimization},
  booktitle = {2017 {{Computing}} in {{Cardiology Conference}}},
  author = {{Coll-Font}, Jaume and Brooks, Dana},
  year = {2017},
  month = sep,
  file = {/home/msca8h/Zotero/storage/BGBDC9EL/Coll-Font and Brooks - 2017 - ECG-Based Reconstruction of Heart Position and Ori.pdf}
}

@article{contrerasortiz_ultrasound_2012,
  title = {Ultrasound Image Enhancement: A Review},
  shorttitle = {Ultrasound Image Enhancement},
  author = {Contreras Ortiz, Sonia H. and Chiu, Tsuicheng and Fox, Martin D.},
  year = {2012},
  month = sep,
  volume = {7},
  pages = {419--428},
  journal = {Biomed. Signal Process. Control},
  language = {en},
  number = {5}
}

@article{cornuet_adaptive_2012,
  title = {Adaptive Multiple Importance Sampling},
  shorttitle = {Adaptive {{Multiple Importance Sampling}}},
  author = {Cornuet, Jean-Marie and Marin, Jean-Michel and Mira, Antonietta and Robert, Christian P.},
  year = {2012},
  month = dec,
  volume = {39},
  pages = {798--812},
  file = {/home/msca8h/Zotero/storage/9TMNYH7N/Cornuet et al. - 2012 - Adaptive Multiple Importance Sampling Adaptive.pdf},
  journal = {Scand. J. Stat.},
  language = {en},
  number = {4}
}

@article{coupe_nonlocal_2009,
  title = {Nonlocal Means-Based Speckle Filtering for Ultrasound Images},
  author = {Coupe, Pierrick and Hellier, Pierre and Kervrann, Charles and Barillot, Christian},
  year = {2009},
  month = oct,
  volume = {18},
  pages = {2221--2229},
  file = {/home/msca8h/Zotero/storage/9IFRANXP/Coupe et al. - 2009 - Nonlocal means-based speckle filtering for ultraso.pdf},
  journal = {IEEE Trans. Image Process.},
  number = {10}
}

@article{craiu_learn_2009,
  title = {Learn from Thy Neighbor: {{Parallel}}-Chain and Regional Adaptive {{MCMC}}},
  shorttitle = {Learn {{From Thy Neighbor}}},
  author = {Craiu, Radu V. and Rosenthal, Jeffrey and Yang, Chao},
  year = {2009},
  month = dec,
  volume = {104},
  pages = {1454--1466},
  file = {/home/msca8h/Zotero/storage/63PRJ2PE/Craiu et al. - 2009 - Learn From Thy Neighbor Parallel-Chain and Region.pdf},
  journal = {J. Amer. Statistical Assoc.},
  language = {en},
  number = {488}
}

@article{dagum_openmp_1998,
  title = {{{OpenMP}}: An Industry Standard {{API}} for Shared-Memory Programming},
  shorttitle = {{{OpenMP}}},
  author = {Dagum, L. and Menon, R.},
  year = {Jan.-March/1998},
  volume = {5},
  pages = {46--55},
  file = {/home/msca8h/Zotero/storage/6N6LSIYQ/Dagum and Menon - 1998 - OpenMP an industry standard API for shared-memory.pdf},
  journal = {IEEE Comput. Sci. Eng.},
  number = {1}
}

@inproceedings{dai_bayesian_2019,
  title = {Bayesian {{Optimization Meets Bayesian Optimal Stopping}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Zhongxiang and Yu, Haibin and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {1496--1506},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Bayesian optimization (BO) is a popular paradigm for optimizing the hyperparameters of machine learning (ML) models due to its sample efficiency. Many ML models require running an iterative training procedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training process (e.g., validation accuracy after each epoch) can be exploited for improving the epoch efficiency of BO algorithms by early-stopping model training under hyperparameter settings that will end up under-performing and hence eliminating unnecessary training epochs. This paper proposes to unify BO (specifically, Gaussian process-upper confidence bound (GP-UCB)) with Bayesian optimal stopping (BO-BOS) to boost the epoch efficiency of BO. To achieve this, while GP-UCB is sample-efficient in the number of function evaluations, BOS complements it with epoch efficiency for each function evaluation by providing a principled optimal stopping mechanism for early stopping. BO-BOS preserves the (asymptotic) no-regret performance of GP-UCB using our specified choice of BOS parameters that is amenable to an elegant interpretation in terms of the exploration-exploitation trade-off. We empirically evaluate the performance of BO-BOS and demonstrate its generality in hyperparameter optimization of ML models and two other interesting applications.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@inproceedings{dai_bayesian_2019a,
  title = {Bayesian {{Optimization Meets Bayesian Optimal Stopping}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Zhongxiang and Yu, Haibin and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {1496--1506},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Bayesian optimization (BO) is a popular paradigm for optimizing the hyperparameters of machine learning (ML) models due to its sample efficiency. Many ML models require running an iterative training procedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training process (e.g., validation accuracy after each epoch) can be exploited for improving the epoch efficiency of BO algorithms by early-stopping model training under hyperparameter settings that will end up under-performing and hence eliminating unnecessary training epochs. This paper proposes to unify BO (specifically, Gaussian process-upper confidence bound (GP-UCB)) with Bayesian optimal stopping (BO-BOS) to boost the epoch efficiency of BO. To achieve this, while GP-UCB is sample-efficient in the number of function evaluations, BOS complements it with epoch efficiency for each function evaluation by providing a principled optimal stopping mechanism for early stopping. BO-BOS preserves the (asymptotic) no-regret performance of GP-UCB using our specified choice of BOS parameters that is amenable to an elegant interpretation in terms of the exploration-exploitation trade-off. We empirically evaluate the performance of BO-BOS and demonstrate its generality in hyperparameter optimization of ML models and two other interesting applications.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{dai_invitation_2020,
  title = {An Invitation to Sequential {{Monte Carlo}} Samplers},
  author = {Dai, Chenguang and Heng, Jeremy and Jacob, Pierre E. and Whiteley, Nick},
  year = {2020},
  month = jul,
  abstract = {Sequential Monte Carlo samplers provide consistent approximations of sequences of probability distributions and of their normalizing constants, via particles obtained with a combination of importance weights and Markov transitions. This article presents this class of methods and a number of recent advances, with the goal of helping statisticians assess the applicability and usefulness of these methods for their purposes. Our presentation emphasizes the role of bridging distributions for computational and statistical purposes. Numerical experiments are provided on simple settings such as multivariate Normals, logistic regression and a basic susceptible-infected-recovered model, illustrating the impact of the dimension, the ability to perform inference sequentially and the estimation of normalizing constants.},
  archiveprefix = {arXiv},
  eprint = {2007.11936},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/NTFKYZUH/Dai et al. - 2020 - An invitation to sequential Monte Carlo samplers.pdf;/home/msca8h/Zotero/storage/RIJC7RCL/2007.html},
  journal = {ArXiv200711936 Stat},
  keywords = {Statistics - Computation,Statistics - Methodology},
  primaryclass = {stat}
}

@article{dai_monte_2019,
  title = {Monte {{Carlo}} Fusion},
  author = {Dai, Hongsheng and Pollock, Murray and Roberts, Gareth},
  year = {2019},
  month = mar,
  volume = {56},
  pages = {174--191},
  abstract = {Abstract             In this paper we propose a new theory and methodology to tackle the problem of unifying Monte Carlo samples from distributed densities into a single Monte Carlo draw from the target density. This surprisingly challenging problem arises in many settings (for instance, expert elicitation, multiview learning, distributed `big data' problems, etc.), but to date the framework and methodology proposed in this paper (Monte Carlo fusion) is the first general approach which avoids any form of approximation error in obtaining the unified inference. In this paper we focus on the key theoretical underpinnings of this new methodology, and simple (direct) Monte Carlo interpretations of the theory. There is considerable scope to tailor the theory introduced in this paper to particular application settings (such as the big data setting), construct efficient parallelised schemes, understand the approximation and computational efficiencies of other such unification paradigms, and explore new theoretical and methodological directions.},
  file = {/home/msca8h/Zotero/storage/H7DR4KCA/Dai et al. - 2019 - Monte Carlo fusion.pdf},
  journal = {J. Appl. Probab.},
  language = {en},
  number = {01}
}

@article{dalbey_gaussian_2014,
  title = {Gaussian {{Process Adaptive Importance Sampling}}},
  author = {Dalbey, Keith R and Swiler, Laura P},
  year = {2014},
  volume = {4},
  pages = {133--149},
  journal = {Int. J. Uncertain. Quantif.},
  number = {2}
}

@inproceedings{dalibard_boat_2017,
  title = {{{BOAT}}: Building Auto-Tuners with Structured {{Bayesian}} Optimization},
  shorttitle = {{{BOAT}}},
  booktitle = {Proc. 26th {{Int}}. {{Conf}}. {{World Wide Web}}},
  author = {Dalibard, Valentin and Schaarschmidt, Michael and Yoneki, Eiko},
  year = {2017},
  pages = {479--488},
  publisher = {{ACM Press}},
  address = {{Perth, Australia}},
  file = {/home/msca8h/Zotero/storage/B3Q82T24/Dalibard et al. - 2017 - BOAT Building Auto-Tuners with Structured Bayesia.pdf},
  language = {en},
  series = {{{WWW}} '17}
}

@article{dan_efficient_1996,
  title = {Efficient {{Coding}} of {{Natural Scenes}} in the {{Lateral Geniculate Nucleus}}: {{Experimental Test}} of a {{Computational Theory}}},
  shorttitle = {Efficient {{Coding}} of {{Natural Scenes}} in the {{Lateral Geniculate Nucleus}}},
  author = {Dan, Yang and Atick, Joseph J. and Reid, R. Clay},
  year = {1996},
  month = may,
  volume = {16},
  pages = {3351--3362},
  file = {/home/msca8h/Zotero/storage/H2Y44VPQ/Dan et al. - 1996 - Efficient Coding of Natural Scenes in the Lateral .pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {10}
}

@inproceedings{datta_stencil_2008,
  title = {Stencil {{Computation Optimization}} and {{Auto}}-{{Tuning}} on {{State}}-of-the-{{Art Multicore Architectures}}},
  booktitle = {Proceedings of the 2008 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  author = {Datta, Kaushik and Murphy, Mark and Volkov, Vasily and Williams, Samuel and Carter, Jonathan and Oliker, Leonid and Patterson, David and Shalf, John and Yelick, Katherine},
  year = {2008},
  publisher = {{IEEE Press}},
  series = {{{SC}} '08}
}

@article{dau_wastefree_2020,
  title = {Waste-Free {{Sequential Monte Carlo}}},
  author = {Dau, Hai-Dang and Chopin, Nicolas},
  year = {2020},
  month = nov,
  abstract = {A standard way to move particles in a SMC sampler is to apply several steps of a MCMC (Markov chain Monte Carlo) kernel. Unfortunately, it is not clear how many steps need to be performed for optimal performance. In addition, the output of the intermediate steps are discarded and thus wasted somehow. We propose a new, waste-free SMC algorithm which uses the outputs of all these intermediate MCMC steps as particles. We establish that its output is consistent and asymptotically normal. We use the expression of the asymptotic variance to develop various insights on how to implement the algorithm in practice. We develop in particular a method to estimate, from a single run of the algorithm, the asymptotic variance of any particle estimate. We show empirically, through a range of numerical examples, that waste-free SMC tends to outperform standard SMC samplers, and especially so in situations where the mixing of the considered MCMC kernels decreases across iterations (as in tempering or rare event problems).},
  archiveprefix = {arXiv},
  eprint = {2011.02328},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/AXQGMSHV/Dau and Chopin - 2020 - Waste-free Sequential Monte Carlo.pdf;/home/msca8h/Zotero/storage/Q9GHV89K/2011.html},
  journal = {ArXiv201102328 Stat},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@article{davis_university_2011,
  title = {The University of Florida Sparse Matrix Collection},
  author = {Davis, Timothy A. and Hu, Yifan},
  year = {2011},
  month = dec,
  volume = {38},
  journal = {ACM Trans Math Softw},
  keywords = {Graph drawing,multilevel algorithms,performance evaluation,sparse matrices},
  number = {1}
}

@inproceedings{DBLP:conf/icml/RainforthNLPMDW16,
  title = {Interacting Particle Markov Chain Monte Carlo},
  booktitle = {{{ICML}}},
  author = {Rainforth, Tom and Naesseth, Christian A. and Lindsten, Fredrik and Paige, Brooks and {van de Meent}, Jan-Willem and Doucet, Arnaud and Wood, Frank D.},
  year = {2016},
  pages = {2616--2625},
  cdate = {1451606400000},
  crossref = {conf/icml/2016}
}

@inproceedings{DBLP:journals/corr/BornscheinB14,
  title = {Reweighted Wake-Sleep},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Learning Representation}} ({{ICLR}})},
  author = {Bornschein, J{\"o}rg and Bengio, Yoshua},
  year = {2015},
  month = may,
  address = {{San Diego, California, USA}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/BornscheinB14.bib},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200}
}

@inproceedings{dean_large_2012,
  title = {Large {{Scale Distributed Deep Networks}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc'Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.},
  year = {2012},
  pages = {1223--1231},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  series = {{{NIPS}}'12}
}

@incollection{defazio_ineffectiveness_2019,
  title = {On the {{Ineffectiveness}} of {{Variance Reduced Optimization}} for {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Defazio, Aaron and Bottou, Leon},
  year = {2019},
  pages = {1755--1765},
  publisher = {{Curran Associates, Inc.}}
}

@article{definelicht_transformations_2021,
  title = {Transformations of {{High}}-{{Level Synthesis Codes}} for {{High}}-{{Performance Computing}}},
  author = {{de Fine Licht}, Johannes and Besta, Maciej and Meierhans, Simon and Hoefler, Torsten},
  year = {2021},
  month = may,
  volume = {32},
  pages = {1014--1029},
  file = {/home/msca8h/Zotero/storage/5KS289A2/de Fine Licht et al. - 2021 - Transformations of High-Level Synthesis Codes for .pdf},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {5}
}

@article{degooijer_conditional_2003,
  title = {On Conditional Density Estimation},
  author = {De Gooijer, Jan G. and Zerom, Dawit},
  year = {2003},
  month = may,
  volume = {57},
  pages = {159--176},
  file = {/home/msca8h/Zotero/storage/LYHMFZMD/De Gooijer and Zerom - 2003 - On Conditional Density Estimation.pdf},
  journal = {Statistica Neerland},
  language = {en},
  number = {2}
}

@book{delmoral_feynmankac_2004,
  title = {Feynman-{{Kac Formulae}}},
  author = {Del Moral, Pierre},
  year = {2004},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  series = {Probability and Its {{Applications}}}
}

@article{delmoral_sequential_2006,
  title = {Sequential {{Monte Carlo}} Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year = {2006},
  month = jun,
  volume = {68},
  pages = {411--436},
  file = {/home/msca8h/Zotero/storage/NNQC3GJ2/Del Moral et al. - 2006 - Sequential Monte Carlo samplers.pdf},
  journal = {J. Roy. Statist. Soc.:  B},
  language = {en},
  number = {3}
}

@misc{demetrescu_9th_2006,
  title = {9th {{DIMACS}} Implementation Challenge - Shortest Paths},
  author = {Demetrescu, Camil and Goldberg, Andrew and Johnson, David},
  year = {2006}
}

@article{deng_speckle_2011,
  title = {Speckle Reduction of Ultrasound Images Based on {{Rayleigh}}-Trimmed Anisotropic Diffusion Filter},
  author = {Deng, Yinhui and Wang, Yuanyuan and Shen, Yuzhong},
  year = {2011},
  month = oct,
  volume = {32},
  pages = {1516--1525},
  journal = {Pattern Recognition Letters},
  language = {en},
  number = {13}
}

@inproceedings{desa_understanding_2017,
  title = {Understanding and {{Optimizing Asynchronous Low}}-{{Precision Stochastic Gradient Descent}}},
  booktitle = {Proceedings of the 44th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {De Sa, Christopher and Feldman, Matthew and R{\'e}, Christopher and Olukotun, Kunle},
  year = {2017},
  pages = {561--574},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  keywords = {asynchrony,FPGA,low precision,multicore,Stochastic gradient descent},
  series = {{{ISCA}} '17}
}

@incollection{detorakis_inherent_2019,
  title = {Inherent {{Weight Normalization}} in {{Stochastic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Detorakis, Georgios and Dutta, Sourav and Khanna, Abhishek and Jerry, Matthew and Datta, Suman and Neftci, Emre},
  year = {2019},
  pages = {3291--3302},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{detorakis_inherent_2019a,
  title = {Inherent {{Weight Normalization}} in {{Stochastic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Detorakis, Georgios and Dutta, Sourav and Khanna, Abhishek and Jerry, Matthew and Datta, Suman and Neftci, Emre},
  year = {2019},
  pages = {3291--3302},
  publisher = {{Curran Associates, Inc.}}
}

@article{detrano_international_1989,
  title = {International Application of a New Probability Algorithm for the Diagnosis of Coronary Artery Disease},
  author = {Detrano, Robert and Janosi, Andras and Steinbrunn, Walter and Pfisterer, Matthias and Schmid, Johann-Jakob and Sandhu, Sarbjit and Guppy, Kern H. and Lee, Stella and Froelicher, Victor},
  year = {1989},
  month = aug,
  volume = {64},
  pages = {304--310},
  journal = {The American Journal of Cardiology},
  language = {en},
  number = {5}
}

@article{dhaka_challenges_2021,
  title = {Challenges and Opportunities in High-Dimensional Variational Inference},
  author = {Dhaka, Akash Kumar and Catalina, Alejandro and Welandawe, Manushi and Andersen, Michael Riis and Huggins, Jonathan and Vehtari, Aki},
  year = {2021},
  month = mar,
  abstract = {We explore the limitations of and best practices for using black-box variational inference to estimate posterior summaries of the model parameters. By taking an importance sampling perspective, we are able to explain and empirically demonstrate: 1) why the intuitions about the behavior of approximate families and divergences for low-dimensional posteriors fail for higher-dimensional posteriors, 2) how we can diagnose the pre-asymptotic reliability of variational inference in practice by examining the behavior of the density ratios (i.e., importance weights), 3) why the choice of variational objective is not as relevant for higher-dimensional posteriors, and 4) why, although flexible variational families can provide some benefits in higher dimensions, they also introduce additional optimization challenges. Based on these findings, for high-dimensional posteriors we recommend using the exclusive KL divergence that is most stable and easiest to optimize, and then focusing on improving the variational family or using model parameter transformations to make the posterior more similar to the approximating family. Our results also show that in low to moderate dimensions, heavy-tailed variational families and mass-covering divergences can increase the chances that the approximation can be improved by importance sampling.},
  archiveprefix = {arXiv},
  eprint = {2103.01085},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/NJET7QNK/Dhaka et al. - 2021 - Challenges and Opportunities in High-dimensional V.pdf;/home/msca8h/Zotero/storage/IVMHP8XC/2103.html},
  journal = {ArXiv210301085 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {cs, stat}
}

@article{dhamala_embedding_2020,
  title = {Embedding High-Dimensional {{Bayesian}} Optimization via Generative Modeling: {{Parameter}} Personalization of Cardiac Electrophysiological Models},
  shorttitle = {Embedding High-Dimensional {{Bayesian}} Optimization via Generative Modeling},
  author = {Dhamala, Jwala and Bajracharya, Pradeep and Arevalo, Hermenegild J. and Sapp, John L. and Hor{\'a}cek, B. Milan and Wu, Katherine C. and Trayanova, Natalia A. and Wang, Linwei},
  year = {2020},
  month = may,
  volume = {62},
  pages = {101670},
  journal = {Medical Image Analysis},
  language = {en}
}

@incollection{dhamala_highdimensional_2018,
  title = {High-Dimensional {{Bayesian}} Optimization of Personalized Cardiac Model Parameters via an Embedded Generative Model},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2018},
  author = {Dhamala, Jwala and Ghimire, Sandesh and Sapp, John L. and Hor{\'a}{\v c}ek, B. Milan and Wang, Linwei},
  year = {2018},
  volume = {11071},
  pages = {499--507},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  file = {/home/msca8h/Zotero/storage/A4UNBB42/Dhamala et al. - 2018 - High-Dimensional Bayesian Optimization of Personal.pdf}
}

@article{diaconis_analysis_2000,
  title = {Analysis of a {{Nonreversible Markov Chain Sampler}}},
  author = {Diaconis, Persi and Holmes, Susan and Neal, Radford M.},
  year = {2000},
  volume = {10},
  pages = {726--752},
  abstract = {We analyze the convergence to stationarity of a simple nonreversible Markov chain that serves as a model for several nonreversible Markov chain sampling methods that are used in practice. Our theoretical and numerical results show that nonreversibility can indeed lead to improvements over the diffusive behavior of simple Markov chain sampling schemes. The analysis uses both probabilistic techniques and an explicit diagonalization.},
  journal = {Ann. Appl. Probab.},
  number = {3}
}

@article{diaconis_analysis_2000a,
  title = {Analysis of a {{Nonreversible Markov Chain Sampler}}},
  author = {Diaconis, Persi and Holmes, Susan and Neal, Radford M.},
  year = {2000},
  volume = {10},
  pages = {726--752},
  abstract = {We analyze the convergence to stationarity of a simple nonreversible Markov chain that serves as a model for several nonreversible Markov chain sampling methods that are used in practice. Our theoretical and numerical results show that nonreversibility can indeed lead to improvements over the diffusive behavior of simple Markov chain sampling schemes. The analysis uses both probabilistic techniques and an explicit diagonalization.},
  journal = {Ann. Appl. Probab.},
  number = {3}
}

@article{diaz_derivation_2009,
  title = {Derivation of Self-Scheduling Algorithms for Heterogeneous Distributed Computer Systems: {{Application}} to Internet-Based Grids of Computers},
  shorttitle = {Derivation of Self-Scheduling Algorithms for Heterogeneous Distributed Computer Systems},
  author = {D{\'i}az, Javier and Reyes, Sebasti{\'a}n and Ni{\~n}o, Alfonso and {Mu{\~n}oz-Caro}, Camelia},
  year = {2009},
  month = jun,
  volume = {25},
  pages = {617--626},
  journal = {Future Gener. Comput. Syst.},
  language = {en},
  number = {6}
}

@inproceedings{diaz_quadratic_2006,
  title = {A {{Quadratic Self}}-{{Scheduling Algorithm}} for {{Heterogeneous Distributed Computing Systems}}},
  booktitle = {2006 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Diaz, J. and Reyes, S. and Nino, A. and {Munoz-Caro}, C.},
  year = {2006},
  month = sep,
  pages = {1--8},
  file = {/home/msca8h/Documents/parallel_scheduling/Diaz et al. - 2006 - A Quadratic Self-Scheduling Algorithm for Heteroge.pdf},
  keywords = {Algorithm design and analysis,Application software,Clustering algorithms,Distributed computing,grid computing,Grid computing,heterogeneous computing systems,heterogeneous distributed computing systems,Heuristic algorithms,Internet,Processor scheduling,quadratic self-scheduling algorithm,resource allocation,scheduling,Scheduling algorithm,scheduling strategy,Testing,transatlantic connection}
}

@article{dinan_scalable_2009,
  title = {Scalable Work Stealing},
  author = {Dinan, James and Larkins, D. Brian and Sadayappan, P. and Krishnamoorthy, Sriram and Nieplocha, Jarek},
  year = {2009},
  pages = {1--11},
  file = {/home/msca8h/Documents/parallel_scheduling/Dinan et al. - 2009 - Scalable work stealing.pdf},
  journal = {Proc. Conf. High Perform. Comput. Netw. Storage Anal.},
  keywords = {dynamic load balancing,pgas,task pools,work stealing}
}

@inproceedings{dinari_distributed_2019,
  title = {Distributed {{MCMC Inference}} in {{Dirichlet Process Mixture Models Using Julia}}},
  booktitle = {2019 19th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGRID}})},
  author = {Dinari, Or and Yu, Angel and Freifeld, Oren and Fisher, John},
  year = {2019},
  month = may,
  pages = {518--525},
  publisher = {{IEEE}},
  address = {{Larnaca, Cyprus}}
}

@article{dippel_multiscale_2002,
  title = {Multiscale Contrast Enhancement for Radiographies: {{Laplacian}} Pyramid versus Fast Wavelet Transform},
  shorttitle = {Multiscale Contrast Enhancement for Radiographies},
  author = {Dippel, S. and Stahl, M. and Wiemker, R. and Blaffert, T.},
  year = {2002},
  month = apr,
  volume = {21},
  pages = {343--353},
  journal = {IEEE Trans. Med. Imaging},
  language = {en},
  number = {4}
}

@article{doi:10.1080/01621459.1951.10500768,
  title = {The Theory of Statistical Decision},
  author = {{L. J. Savage}},
  year = {1951},
  volume = {46},
  pages = {55--67},
  publisher = {{Taylor \& Francis}},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1951.10500768},
  journal = {J. Am. Stat. Assoc.},
  number = {253}
}

@article{doi:10.1080/01621459.2000.10473908,
  title = {The Multiple-Try Method and Local Optimization in Metropolis Sampling},
  author = {Liu, Jun S. and Liang, Faming and {Wing Hung Wong}},
  year = {2000},
  volume = {95},
  pages = {121--134},
  publisher = {{Taylor \& Francis}},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.2000.10473908},
  journal = {J. Am. Stat. Assoc.},
  number = {449}
}

@inproceedings{dorta_openmp_2005,
  title = {The {{OpenMP Source Code Repository}}},
  booktitle = {13th {{Euromicro Conference}} on {{Parallel}}, {{Distributed}} and {{Network}}-{{Based Processing}}},
  author = {Dorta, A.J. and Rodriguez, C. and {de Sande}, F. and {Gonzalez-Escribano}, A.},
  year = {2005},
  pages = {244--250},
  publisher = {{IEEE}},
  address = {{Lugano, Switzerland}}
}

@inproceedings{dotsenko_autotuning_2011,
  title = {Auto-Tuning of Fast Fourier Transform on Graphics Processors},
  booktitle = {Proceedings of the 16th {{ACM}} Symposium on {{Principles}} and Practice of Parallel Programming - {{PPoPP}} '11},
  author = {Dotsenko, Yuri and Baghsorkhi, Sara S. and Lloyd, Brandon and Govindaraju, Naga K.},
  year = {2011},
  pages = {257},
  publisher = {{ACM Press}},
  address = {{San Antonio, TX, USA}},
  language = {en}
}

@inproceedings{douc_comparison_2005,
  title = {Comparison of Resampling Schemes for Particle Filtering},
  booktitle = {{{ISPA}} 2005. {{Proceedings}} of the 4th {{International Symposium}} on {{Image}} and {{Signal Processing}} and {{Analysis}}, 2005.},
  author = {Douc, R. and Cappe, O.},
  year = {2005},
  pages = {64--69},
  publisher = {{IEEE}},
  address = {{Zagreb, Croatia}},
  file = {/home/msca8h/Zotero/storage/KEZ55L2N/Douc and Cappe - 2005 - Comparison of resampling schemes for particle filt.pdf}
}

@incollection{doucet_introduction_2001,
  title = {An {{Introduction}} to {{Sequential Monte Carlo Methods}}},
  booktitle = {Sequential {{Monte Carlo Methods}} in {{Practice}}},
  author = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  pages = {3--14},
  publisher = {{Springer New York}},
  address = {{New York, NY}}
}

@article{Dua:2019,
  title = {{{UCI}} Machine Learning Repository},
  author = {Dua, Dheeru and Graff, Casey},
  year = {2017},
  publisher = {{University of California, Irvine, School of Information and Computer Sciences}}
}

@article{duan_tuning_2009,
  title = {Tuning Database Configuration Parameters with {{iTuned}}},
  author = {Duan, Songyun and Thummala, Vamsidhar and Babu, Shivnath},
  year = {2009},
  month = aug,
  volume = {2},
  pages = {1246--1257},
  file = {/home/msca8h/Zotero/storage/KYLPG32R/Duan et al. - 2009 - Tuning database configuration parameters with iTun.pdf},
  journal = {Proc. VLDB Endow.},
  language = {en},
  number = {1}
}

@article{duane_hybrid_1987,
  title = {Hybrid {{Monte Carlo}}},
  author = {Duane, Simon and Kennedy, Anthony D. and Pendleton, Brian J. and Roweth, Duncan},
  year = {1987},
  volume = {195},
  pages = {216--222},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
  journal = {Phys. Lett. B},
  number = {2}
}

@article{duarte-salazar_speckle_2020,
  title = {Speckle Noise Reduction in Ultrasound Images for Improving the Metrological Evaluation of Biomedical Applications: {{An}} Overview},
  shorttitle = {Speckle {{Noise Reduction}} in {{Ultrasound Images}} for {{Improving}} the {{Metrological Evaluation}} of {{Biomedical Applications}}},
  author = {{Duarte-Salazar}, Carlos A. and {Castro-Ospina}, Andres Eduardo and Becerra, Miguel A. and {Delgado-Trejos}, Edilson},
  year = {2020},
  volume = {8},
  pages = {15983--15999},
  file = {/home/msca8h/Zotero/storage/ZQVA8DQI/Duarte-Salazar et al. - 2020 - Speckle Noise Reduction in Ultrasound Images for I.pdf},
  journal = {IEEE Access}
}

@article{duchi_adaptive_2011,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  volume = {12},
  pages = {2121--2159},
  journal = {J. Mach. Learn. Res.},
  number = {Jul}
}

@inproceedings{duplyakin_active_2016,
  title = {Active {{Learning}} in {{Performance Analysis}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Duplyakin, Dmitry and Brown, Jed and Ricci, Robert},
  year = {2016},
  month = sep,
  pages = {182--191},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}}
}

@article{durand_impact_1996,
  title = {Impact of Memory Contention on Dynamic Scheduling on {{NUMA}} Multiprocessors},
  author = {Durand, D. and Montaut, T. and Kervella, L. and Jalby, W.},
  year = {1996},
  month = nov,
  volume = {7},
  pages = {1201--1214},
  file = {/home/msca8h/Documents/parallel_scheduling/Durand et al. - 1996 - Impact of memory contention on dynamic scheduling .pdf},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  keywords = {Analytical models,analytical performance models,BBN TC2000,dynamic scheduling,Dynamic scheduling,load balancing,load imbalance,Load management,memory contention,multiprocessing systems,Multiprocessor interconnection networks,nonuniform memory access machine,NUMA multiprocessors,parallel programming,parallel programs,Performance analysis,performance evaluation,Predictive models,processor scheduling,Processor scheduling,Programming profession,resource allocation,Scalability,scheduling overhead,self-scheduling,Shape,task duration distributions,task scheduling},
  number = {11}
}

@article{durillo_multiobjective_2019,
  title = {Multi-{{Objective}} Region-{{Aware}} Optimization of Parallel Programs},
  author = {Durillo, Juan J. and Gschwandtner, Philipp and Kofler, Klaus and Fahringer, Thomas},
  year = {2019},
  month = apr,
  volume = {83},
  pages = {3--21},
  file = {/home/msca8h/Zotero/storage/YS2TNN8I/Durillo et al. - 2019 - Multi-Objective region-Aware optimization of paral.pdf},
  journal = {Parallel Computing},
  language = {en}
}

@inproceedings{duvenaud_structure_2013,
  title = {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
  booktitle = {Proc. 30th {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Duvenaud, David and Lloyd, James and Grosse, Roger and Tenenbaum, Joshua and Zoubin, Ghahramani},
  year = {2013},
  month = jun,
  volume = {28},
  pages = {1166--1174},
  publisher = {{PMLR}},
  address = {{Atlanta, Georgia, USA}},
  abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
  series = {{{ICML}}'13}
}

@techreport{eagerd.l_adaptive_1992,
  title = {Adaptive {{Guided Self}}-{{Scheduling}}},
  author = {{Eager, D.L} and {Zahorjan, J.}},
  year = {1992},
  institution = {{Department of Computer Science and Engineering, University of Washington}},
  number = {TR92-01-01,}
}

@inproceedings{eggensperger_efficient_2015,
  title = {Efficient {{Benchmarking}} of {{Hyperparameter Optimizers}} via {{Surrogates}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Eggensperger, Katharina and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2015},
  pages = {1114--1120},
  publisher = {{AAAI Press}},
  series = {{{AAAI}}'15}
}

@inproceedings{eggensperger_efficient_2015a,
  title = {Efficient {{Benchmarking}} of {{Hyperparameter Optimizers}} via {{Surrogates}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Eggensperger, Katharina and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2015},
  pages = {1114--1120},
  publisher = {{AAAI Press}},
  series = {{{AAAI}}'15}
}

@inproceedings{eggensperger_empirical_2013,
  title = {Towards an {{Empirical Foundation}} for {{Assessing Bayesian Optimization}} of {{Hyperparameters}}},
  booktitle = {{{NIPS}} Workshop on {{Bayesian Optimization}} in {{Theory}} and {{Practice}}},
  author = {Eggensperger, K. and Feurer, M. and Hutter, F. and Bergstra, J. and Snoek, J. and Hoos, H. and {Leyton-Brown}, K.},
  year = {2013}
}

@inproceedings{egidi_are_2018,
  title = {Are {{Shots Predictive Of Soccer Results}}?},
  booktitle = {{{StanCon}} 2018},
  author = {Egidi, Leonardo and Pauli, Francesco and Torelli, Nicola},
  year = {2018},
  month = aug,
  publisher = {{Zenodo}},
  abstract = {A Stan model for soccer shooting ability.},
  copyright = {Creative Commons Attribution 4.0, Open Access},
  keywords = {Bayesian Data Analysis,Stan,StanCon}
}

@article{elvira_rethinking_2018,
  title = {Rethinking the {{Effective Sample Size}}},
  author = {Elvira, V{\'i}ctor and Martino, Luca and Robert, Christian P.},
  year = {2018},
  month = sep,
  abstract = {The effective sample size (ESS) is widely used in sample-based simulation methods for assessing the quality of a Monte Carlo approximation of a given distribution and of related integrals. In this paper, we revisit and complete the approximation of the ESS in the specific context of importance sampling (IS). The derivation of this approximation, that we will denote as \$\textbackslash widehat\{\textbackslash text\{ESS\}\}\$, is only partially available in Kong [1992]. This approximation has been widely used in the last 25 years due to its simplicity as a practical rule of thumb in a wide variety of importance sampling methods. However, we show that the multiple assumptions and approximations in the derivation of \$\textbackslash widehat\{\textbackslash text\{ESS\}\}\$, makes it difficult to be considered even as a reasonable approximation of the ESS. We extend the discussion of the ESS in the multiple importance sampling (MIS) setting, and we display numerical examples. This paper does not cover the use of ESS for MCMC algorithms.},
  archiveprefix = {arXiv},
  eprint = {1809.04129},
  eprinttype = {arxiv},
  journal = {ArXiv180904129 Stat},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@inproceedings{emmawang_demystifying_2019,
  title = {Demystifying {{Bayesian Inference Workloads}}},
  booktitle = {{{IEEE Int}}. {{Symp}}. {{Perform}}. {{Anal}}. {{Syst}}. {{Softw}}.},
  author = {Emma Wang, Yu and Zhu, Yuhao and Ko, Glenn G. and Reagen, Brandon and Wei, Gu-Yeon and Brooks, David},
  year = {2019},
  month = mar,
  pages = {177--189},
  publisher = {{IEEE}},
  address = {{Madison, WI, USA}},
  series = {{{ISPASS}}'19}
}

@article{fairbrother_gaussianprocesses_2018,
  title = {{{GaussianProcesses}}. Jl: {{A Nonparametric Bayes}} Package for the {{Julia Language}}},
  author = {Fairbrother, Jamie and Nemeth, Christopher and Rischard, Maxime and Brea, Johanni and Pinder, Thomas},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1812.09064},
  eprinttype = {arxiv},
  journal = {ArXiv Prepr. ArXiv181209064}
}

@book{fang_symmetric_2018,
  title = {Symmetric {{Multivariate}} and {{Related Distributions}}},
  author = {Fang, Kaitai and Kotz, Samuel and Ng, Kai Wang},
  year = {2018},
  abstract = {"Since the publication of the by now classical Johnson and Kotz Continuous Multivariate Distributions (Wiley, 1972) there have been substantial developments in multivariate distribution theory especially in the area of non-normal symmetric multivariate distributions. The book by Fang, Kotz and Ng summarizes these developments in a manner which is accessible to a reader with only limited background (advanced real-analysis calculus, linear algebra and elementary matrix calculus). Many of the results in this field are due to Kai-Tai Fang and his associates and appeared in Chinese publications only.A thorough literature search was conducted and the book represents the latest work - as of 1988 - in this rapidly developing field of multivariate distributions. The authors are experts in statistical distribution theory."--Provided by publisher.},
  annotation = {OCLC: 1020790331},
  language = {English}
}

@article{fann_intelligent_2000,
  title = {An {{Intelligent Parallel Loop Scheduling}} for {{Parallelizing Compilers}}},
  author = {Fann, Yun-Woei and Yang, Chao-Tung and Tseng, Shian-Shyong and Tsai, Chang-Jiun},
  year = {2000},
  volume = {16},
  pages = {169--200},
  file = {/home/msca8h/Documents/parallel_scheduling/Fann et al. - 2000 - An Intelligent Parallel Loop Scheduling for Parall.pdf},
  journal = {J Inf Sci Eng}
}

@inproceedings{fann_ipls_1998,
  title = {{{IPLS}}: {{An}} Intelligent Parallel Loop Scheduling for Multiprocessor Systems},
  booktitle = {Proceedings 1998 {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{Cat}}. {{No}}. {{98TB100250}})},
  author = {Fann, Yun-Woei and Yang, Chao-Tung and Tsai, Chang-Jiun and Tseng, Shian-Shyong},
  year = {1998},
  pages = {775--782},
  publisher = {{IEEE}}
}

@article{fearnhead_adaptive_2013,
  title = {An Adaptive Sequential {{Monte Carlo}} Sampler},
  author = {Fearnhead, Paul and Taylor, Benjamin M.},
  year = {2013},
  month = jun,
  volume = {8},
  pages = {411--438},
  file = {/home/msca8h/Zotero/storage/QTCLZVNB/Fearnhead and Taylor - 2013 - An Adaptive Sequential Monte Carlo Sampler.pdf},
  journal = {Bayesian Anal.},
  language = {en},
  number = {2}
}

@inproceedings{feng_building_2007,
  title = {Building the Tree of Life on Terascale Systems},
  booktitle = {Proceedings of the 2007 {{IEEE International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Feng, Xizhou and Cameron, Kirk W. and Sosa, Carlos P. and Smith, Brian},
  year = {2007},
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  file = {/home/msca8h/Zotero/storage/N44I829Q/Feng et al. - 2007 - Building the Tree of Life on Terascale Systems.pdf},
  series = {{{IPDPS}}'07}
}

@article{feng_parallel_2003,
  title = {Parallel Algorithms for {{Bayesian}} Phylogenetic Inference},
  author = {Feng, Xizhou and Buell, Duncan A. and Rose, John R. and Waddell, Peter J.},
  year = {2003},
  month = jul,
  volume = {63},
  pages = {707--718},
  journal = {J. Parallel Distrib. Comput.},
  language = {en},
  number = {7-8}
}

@inproceedings{feng_uniform_2018,
  title = {Uniform Convergence of Sample Average Approximation with Adaptive Multiple Importance Sampling},
  booktitle = {2018 {{Winter Simulation Conference}} ({{WSC}})},
  author = {Feng, M. Ben and Maggiar, Alvaro and Staum, Jeremy and Wachter, Andreas},
  year = {2018},
  month = dec,
  pages = {1646--1657},
  publisher = {{IEEE}},
  address = {{Gothenburg, Sweden}}
}

@article{ferrari_beta_2004,
  title = {Beta {{Regression}} for {{Modelling Rates}} and {{Proportions}}},
  author = {Ferrari, Silvia and {Cribari-Neto}, Francisco},
  year = {2004},
  month = aug,
  volume = {31},
  pages = {799--815},
  journal = {Journal of Applied Statistics},
  language = {en},
  number = {7}
}

@article{filippone_pseudomarginal_2014,
  title = {Pseudo-Marginal {{Bayesian}} Inference for {{Gaussian}} Processes},
  author = {Filippone, Maurizio and Girolami, Mark},
  year = {2014},
  month = nov,
  volume = {36},
  pages = {2214--2226},
  file = {/home/msca8h/Zotero/storage/QQ967QMI/Filippone and Girolami - 2014 - Pseudo-Marginal Bayesian Inference for Gaussian Pr.pdf},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  number = {11}
}

@phdthesis{finke_extended_2015,
  title = {On Extended State-Space Constructions for Monte Carlo Methods},
  author = {Finke, Axel},
  year = {2015},
  month = jul,
  abstract = {This thesis develops computationally efficient methodology in two areas. Firstly, we consider a particularly challenging class of discretely observed continuous-time point-process models. For these, we analyse and improve an existing filtering algorithm based on sequential Monte Carlo (smc) methods. To estimate the static parameters in such models, we devise novel particle Gibbs samplers. One of these exploits a sophisticated non-entred parametrisation whose benefits in a Markov chain Monte Carlo (mcmc) context have previously been limited by the lack of blockwise updates for the latent point process. We apply this algorithm to a L\'evy-driven stochastic volatility model. Secondly, we devise novel Monte Carlo methods \textendash{} based around pseudo-marginal and conditional smc approaches \textendash{} for performing optimisation in latent-variable models and more generally. To ease the explanation of the wide range of techniques employed in this work, we describe a generic importance-sampling framework which admits virtually all Monte Carlo methods, including smc and mcmc methods, as special cases. Indeed, hierarchical combinations of different Monte Carlo schemes such as smc within mcmc or smc within smc can be justified as repeated applications of this framework.},
  school = {University of Warwick},
  type = {Ph.{{D}}. {{Thesis}}}
}

@article{finn_echocardiographic_2011,
  title = {Echocardiographic Speckle Reduction Comparison},
  author = {Finn, S and Glavin, M and Jones, E},
  year = {2011},
  month = jan,
  volume = {58},
  pages = {82--101},
  journal = {IEEE Trans. Ultrason., Ferroelect., Freq. Contr.},
  number = {1}
}

@article{finzi_generalizing_2020,
  title = {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},
  author = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  year = {2020},
  month = feb,
  abstract = {The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.},
  archiveprefix = {arXiv},
  eprint = {2002.12880},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/5WPY7SJX/Finzi et al. - 2020 - Generalizing Convolutional Neural Networks for Equ.pdf;/home/msca8h/Zotero/storage/5PDTUENJ/2002.html},
  journal = {ArXiv200212880 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{fishman_discreteevent_2001,
  title = {Discrete-{{Event Simulation}}},
  author = {Fishman, George S.},
  year = {2001},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  file = {/home/msca8h/Zotero/storage/W8M8QNTR/Fishman - 2001 - Discrete-Event Simulation.pdf}
}

@techreport{flynn_scheduling_1990,
  title = {Scheduling Variable-Length Parallel Subtasks},
  author = {Flynn, L. E. and Hummel, S. F.},
  year = {1990},
  month = feb,
  institution = {{IBM Research T. J. Watson Research Center}},
  file = {/home/msca8h/Documents/parallel_scheduling/Flynn and Hummel - 1990 - Scheduling variable-length parallel subtasks.pdf}
}

@book{fort_deep_2020,
  title = {Deep Ensembles: {{A}} Loss Landscape Perspective},
  author = {Fort, Stanislav and Hu, Clara Huiyi and Lakshminarayanan, Balaji},
  year = {2020}
}

@article{fox_adapting_2003,
  title = {Adapting the {{Sample Size}} in {{Particle Filters Through KLD}}-{{Sampling}}},
  author = {Fox, Dieter},
  year = {2003},
  month = dec,
  volume = {22},
  pages = {985--1003},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {12}
}

@article{frankle_training_2020,
  title = {Training {{BatchNorm}} and {{Only BatchNorm}}: {{On}} the {{Expressive Power}} of {{Random Features}} in {{CNNs}}},
  shorttitle = {Training {{BatchNorm}} and {{Only BatchNorm}}},
  author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
  year = {2020},
  month = feb,
  abstract = {Batch normalization (BatchNorm) has become an indispensable tool for training deep neural networks, yet it is still poorly understood. Although previous work has typically focused on its normalization component, BatchNorm also adds two per-feature trainable parameters: a coefficient and a bias. However, the role and expressive power of these parameters remains unclear. To study this question, we investigate the performance achieved when training only these parameters and freezing all others at their random initializations. We find that doing so leads to surprisingly high performance. For example, a sufficiently deep ResNet reaches 83\% accuracy on CIFAR-10 in this configuration. Interestingly, BatchNorm achieves this performance in part by naturally learning to disable around a third of the random features without any changes to the training objective. Not only do these results highlight the under-appreciated role of the affine parameters in BatchNorm, but - in a broader sense - they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.},
  archiveprefix = {arXiv},
  eprint = {2003.00152},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/IX8W55US/Frankle et al. - 2020 - Training BatchNorm and Only BatchNorm On the Expr.pdf;/home/msca8h/Zotero/storage/LIYI3EGL/2003.html},
  journal = {ArXiv200300152 Cs Stat},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{frigo_design_2005,
  title = {The {{Design}} and {{Implementation}} of {{FFTW3}}},
  author = {Frigo, M. and Johnson, S.G.},
  year = {2005},
  month = feb,
  volume = {93},
  pages = {216--231},
  file = {/home/msca8h/Zotero/storage/8Q8SPIXB/Frigo and Johnson - 2005 - The Design and Implementation of FFTW3.pdf},
  journal = {Proc. IEEE},
  number = {2}
}

@incollection{frigola_bayesian_2013,
  title = {Bayesian {{Inference}} and {{Learning}} in {{Gaussian Process State}}-{{Space Models}} with {{Particle MCMC}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Frigola, Roger and Lindsten, Fredrik and Sch{\"o}n, Thomas B and Rasmussen, Carl Edward},
  year = {2013},
  pages = {3156--3164},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{frigola_variational_2014,
  title = {Variational {{Gaussian Process State}}-{{Space Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl Edward},
  year = {2014},
  pages = {3680--3688},
  publisher = {{Curran Associates, Inc.}}
}

@article{fuentes_autonomous_2020,
  title = {Autonomous Ultrasonic Inspection Using {{Bayesian}} Optimisation and Robust Outlier Analysis},
  author = {Fuentes, R. and Gardner, P. and Mineo, C. and Rogers, T.J. and Pierce, S.G. and Worden, K. and Dervilis, N. and Cross, E.J.},
  year = {2020},
  month = nov,
  volume = {145},
  pages = {106897},
  file = {/home/msca8h/Zotero/storage/WQB92MII/Fuentes et al. - 2020 - Autonomous ultrasonic inspection using Bayesian op.pdf},
  journal = {Mechanical Systems and Signal Processing},
  language = {en}
}

@article{gadioli_margot_2019,
  title = {{{mARGOt}}: {{A Dynamic Autotuning Framework}} for {{Self}}-{{Aware Approximate Computing}}},
  shorttitle = {{{mARGOt}}},
  author = {Gadioli, Davide and Vitali, Emanuele and Palermo, Gianluca and Silvano, Cristina},
  year = {2019},
  month = may,
  volume = {68},
  pages = {713--728},
  journal = {IEEE Trans. Comput.},
  number = {5}
}

@article{garbuno-inigo_gaussian_2016,
  title = {Gaussian Process Hyper-Parameter Estimation Using {{Parallel Asymptotically Independent Markov Sampling}}},
  author = {{Garbuno-Inigo}, A. and DiazDelaO, F.A. and Zuev, K.M.},
  year = {2016},
  month = nov,
  volume = {103},
  pages = {367--383},
  file = {/home/msca8h/Zotero/storage/GRU9QYV8/Garbuno-Inigo et al. - 2016 - Gaussian process hyper-parameter estimation using .pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}

@article{garthwaite_adaptive_2016,
  title = {Adaptive Optimal Scaling of {{Metropolis}}\textendash{{Hastings}} Algorithms Using the {{Robbins}}\textendash{{Monro}} Process},
  author = {Garthwaite, P. H. and Fan, Y. and Sisson, S. A.},
  year = {2016},
  month = sep,
  volume = {45},
  pages = {5098--5111},
  file = {/home/msca8h/Zotero/storage/H5AFFK39/Garthwaite et al. - 2016 - Adaptive optimal scaling of Metropolis–Hastings al.pdf},
  journal = {Communications in Statistics - Theory and Methods},
  language = {en},
  number = {17}
}

@article{gast_new_2018,
  title = {A New Analysis of {{Work Stealing}} with Latency},
  author = {Gast, N. and Khatiri, M. and Trystram, D. and Wagner, F.},
  year = {2018},
  abstract = {We study in this paper the impact of communication latency on the classical Work Stealing load balancing algorithm. Our paper extends the reference model in which we introduce a latency parameter. By using a theoretical analysis and simulation, we study the overall impact of this latency on the Makespan (maximum completion time). We derive a new expression of the expected running time of a bag of tasks scheduled by Work Stealing. This expression enables us to predict under which conditions a given run will yield acceptable performance. For instance, we can easily calibrate the maximal number of processors to use for a given work/platform combination. All our results are validated through simulation on a wide range of parameters.},
  archiveprefix = {arXiv},
  eprint = {1805.00857},
  eprinttype = {arxiv},
  journal = {arXiv:1805.00857}
}

@inproceedings{gates_autotuning_2017,
  title = {Autotuning Batch {{Cholesky}} Factorization in {{CUDA}} with Interleaved Layout of Matrices},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Gates, Mark and Kurzak, Jakub and Luszczek, Piotr and {Yu Pei} and Dongarra, Jack},
  year = {2017},
  month = may,
  pages = {1408--1417},
  publisher = {{IEEE}},
  address = {{Lake Buena Vista, FL}}
}

@misc{gcc_gcc_2018,
  title = {{{GCC}}, the {{GNU}} Compiler Collection},
  author = {GCC},
  year = {2018},
  month = jul,
  abstract = {The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, Go, and D, as well as libraries for these languages (libstdc++,...)}
}

@inproceedings{ge2018t,
  title = {Turing: A Language for Flexible Probabilistic Inference},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
  year = {2018},
  volume = {84},
  pages = {1682--1690},
  publisher = {{ML Research Press}},
  biburl = {https://dblp.org/rec/bib/conf/aistats/GeXG18},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{geirhos_shortcut_2020,
  title = {Shortcut {{Learning}} in {{Deep Neural Networks}}},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020},
  month = apr,
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  archiveprefix = {arXiv},
  eprint = {2004.07780},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/F59FZCPR/Geirhos et al. - 2020 - Shortcut Learning in Deep Neural Networks.pdf;/home/msca8h/Zotero/storage/FESPD5QZ/2004.html},
  journal = {ArXiv200407780 Cs Q-Bio},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  primaryclass = {cs, q-bio}
}

@book{gelman_bayesian_2014,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John and Stern, Hal and Dunson, David and Vehtari, Aki and Rubin, Donald},
  year = {2014},
  edition = {Third},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
  keywords = {Bayesian statistical decision theory,MATHEMATICS / Probability \& Statistics / General},
  lccn = {QA279.5 .G45 2014},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science}
}

@book{gelman_data_2007,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge; New York}},
  annotation = {OCLC: ocm67375137},
  keywords = {Multilevel models (Statistics),Regression analysis},
  lccn = {HA31.3 .G45 2007},
  series = {Analytical Methods for Social Research}
}

@article{gelman_inference_1992,
  title = {Inference from Iterative Simulation Using Multiple Sequences},
  author = {Gelman, Andrew and Rubin, Donald B.},
  year = {1992},
  month = nov,
  volume = {7},
  pages = {457--472},
  file = {/home/msca8h/Zotero/storage/ALGTLQQX/Gelman and Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf},
  journal = {Statist. Sci.},
  language = {en},
  number = {4}
}

@incollection{gelman_inference_2011,
  title = {Inference from Simulations and Monitoring Convergence},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Gelman, Andrew and Shirley, Kenneth},
  year = {2011},
  pages = {163--174},
  publisher = {{CRC Press}}
}

@article{gelman_simulating_1998,
  title = {Simulating Normalizing Constants: From Importance Sampling to Bridge Sampling to Path Sampling},
  shorttitle = {Simulating Normalizing Constants},
  author = {Gelman, Andrew and Meng, Xiao-Li},
  year = {1998},
  month = may,
  volume = {13},
  file = {/home/msca8h/Zotero/storage/A6GUD9J4/Gelman and Meng - 1998 - Simulating normalizing constants from importance .pdf},
  journal = {Statist. Sci.},
  number = {2}
}

@inproceedings{geyer_markov_1991,
  title = {Markov {{Chain Monte Carlo Maximum Likelihood}}},
  booktitle = {Proceedings of the 23rd {{Symposium}} on the {{Interface}}},
  author = {Geyer, Charles J.},
  year = {1991},
  pages = {pp. 156-163},
  publisher = {{Interface Foundation of North America}},
  abstract = {Markov chain Monte Carlo (e. g., the Metropolis algorithm and Gibbs sampler) is a general tool for simulation of complex stochastic processes useful in many types of statistical inference. The basics of Markov chain Monte Carlo are reviewed, including choice of algorithms and variance estimation, and some new methods are introduced. The use of Markov chain Monte Carlo for maximum likelihood estimation is explained, and its performance is compared with maximum pseudo likelihood estimation.}
}

@article{geyer_practical_1992,
  title = {Practical {{Markov}} Chain {{Monte Carlo}}},
  author = {Geyer, Charles J.},
  year = {1992},
  month = nov,
  volume = {7},
  pages = {473--483},
  file = {/home/msca8h/Zotero/storage/FNT6APTW/Geyer - 1992 - Practical Markov Chain Monte Carlo.pdf},
  journal = {Statist. Sci.},
  language = {en},
  number = {4}
}

@incollection{geyer2011introduction,
  title = {Introduction to Markov Chain Monte Carlo},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Geyer, Charles J},
  year = {2011},
  pages = {3--48},
  publisher = {{CRC Press}}
}

@article{gilboa_image_2004,
  title = {Image Enhancement and Denoising by Complex Diffusion Processes},
  author = {Gilboa, G. and Sochen, N. and Zeevi, Y.Y.},
  year = {2004},
  month = aug,
  volume = {26},
  pages = {1020--1036},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  language = {en},
  number = {8}
}

@article{giordani_adaptive_2010,
  title = {Adaptive Independent {{Metropolis}}\textendash{{Hastings}} by Fast Estimation of Mixtures of Normals},
  author = {Giordani, Paolo and Kohn, Robert},
  year = {2010},
  month = jan,
  volume = {19},
  pages = {243--259},
  file = {/home/msca8h/Zotero/storage/DG63WRM7/Giordani and Kohn - 2010 - Adaptive Independent Metropolis–Hastings by Fast E.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {2}
}

@article{girolami_riemann_2011,
  title = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods: {{Riemann Manifold Langevin}} and {{Hamiltonian Monte Carlo Methods}}},
  shorttitle = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods},
  author = {Girolami, Mark and Calderhead, Ben},
  year = {2011},
  month = mar,
  volume = {73},
  pages = {123--214},
  journal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  language = {en},
  number = {2}
}

@misc{gleich_wikipedia20070206_2007,
  title = {Wikipedia-20070206},
  author = {Gleich, D},
  year = {2007}
}

@article{glynn_analysis_1992,
  title = {Analysis of {{Initial Transient Deletion}} for {{Parallel Steady}}-{{State Simulations}}},
  author = {Glynn, Peter W. and Heidelberger, Philip},
  year = {1992},
  month = jul,
  volume = {13},
  pages = {904--922},
  journal = {SIAM J. Sci. and Stat. Comput.},
  language = {en},
  number = {4}
}

@article{Gopal2011RunningRM,
  title = {Running Regenerative Markov Chains in Parallel},
  author = {Gopal, Vikneswaran and Casella, George},
  year = {2011},
  journal = {unpublished}
}

@article{gordon_novel_1993,
  title = {Novel Approach to Nonlinear/Non-{{Gaussian Bayesian}} State Estimation},
  author = {Gordon, N.J. and Salmond, D.J. and Smith, A.F.M.},
  year = {1993},
  volume = {140},
  pages = {107},
  file = {/home/msca8h/Zotero/storage/458KJ7FI/Gordon et al. - 1993 - Novel approach to nonlinearnon-Gaussian Bayesian .pdf},
  journal = {IEE Proc. F Radar Signal Process. UK},
  language = {en},
  number = {2}
}

@article{gorman_analysis_1988,
  title = {Analysis of Hidden Units in a Layered Network Trained to Classify Sonar Targets},
  author = {Gorman, R.Paul and Sejnowski, Terrence J.},
  year = {1988},
  month = jan,
  volume = {1},
  pages = {75--89},
  journal = {Neural Networks},
  language = {en},
  number = {1}
}

@article{goyal_accurate_2017,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  year = {2017},
  month = jun,
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archiveprefix = {arXiv},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/TUKW27U7/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet i.pdf;/home/msca8h/Zotero/storage/2XSTJVTS/1706.html},
  journal = {ArXiv170602677 Cs},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{gramacy_bayesian_2008,
  title = {Bayesian {{Treed Gaussian Process Models With}} an {{Application}} to {{Computer Modeling}}},
  author = {Gramacy, Robert B and Lee, Herbert K. H},
  year = {2008},
  month = sep,
  volume = {103},
  pages = {1119--1130},
  file = {/home/msca8h/Zotero/storage/S58VXEAP/Gramacy and Lee - 2008 - Bayesian Treed Gaussian Process Models With an App.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {483}
}

@article{gramacy_importance_2010,
  title = {Importance Tempering},
  author = {Gramacy, Robert and Samworth, Richard and King, Ruth},
  year = {2010},
  month = jan,
  volume = {20},
  pages = {1--7},
  journal = {Stat Comput},
  language = {en},
  number = {1}
}

@article{gramacy_particle_2011,
  title = {Particle {{Learning}} of {{Gaussian Process Models}} for {{Sequential Design}} and {{Optimization}}},
  author = {Gramacy, Robert B. and Polson, Nicholas G.},
  year = {2011},
  month = jan,
  volume = {20},
  pages = {102--118},
  file = {/home/msca8h/Documents/bayesian_optimization/gramacy2011.pdf;/home/msca8h/Zotero/storage/6YUF6AMF/Gramacy and Polson - 2011 - Particle Learning of Gaussian Process Models for S.pdf},
  journal = {J. Comput. Graph. Stat.},
  language = {en},
  number = {1}
}

@article{green_increasing_2020,
  title = {Increasing the Efficiency of {{Sequential Monte Carlo}} Samplers through the Use of Approximately Optimal {{L}}-Kernels},
  author = {Green, Peter L. and Moore, Robert E. and Jackson, Ryan J. and Li, Jinglai and Maskell, Simon},
  year = {2020},
  month = apr,
  abstract = {By facilitating the generation of samples from arbitrary probability distributions, Markov Chain Monte Carlo (MCMC) is, arguably, \textbackslash emph\{the\} tool for the evaluation of Bayesian inference problems that yield non-standard posterior distributions. In recent years, however, it has become apparent that Sequential Monte Carlo (SMC) samplers have the potential to outperform MCMC in a number of ways. SMC samplers are better suited to highly parallel computing architectures and also feature various tuning parameters that are not available to MCMC. One such parameter - the `L-kernel' - is a user-defined probability distribution that can be used to influence the efficiency of the sampler. In the current paper, the authors explain how to derive an expression for the L-kernel that minimises the variance of the estimates realised by an SMC sampler. Various approximation methods are then proposed to aid implementation of the proposed L-kernel. The improved performance of the resulting algorithm is demonstrated in multiple scenarios. For the examples shown in the current paper, the use of an approximately optimum L-kernel has reduced the variance of the SMC estimates by up to 99 \% while also reducing the number of times that resampling was required by between 65 \% and 70 \%. Python code and code tests accompanying this manuscript are available through the Github repository \textbackslash url\{https://github.com/plgreenLIRU/SMC\_approx\_optL\}.},
  archiveprefix = {arXiv},
  eprint = {2004.12838},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/QME9EYBA/Green et al. - 2020 - Increasing the efficiency of Sequential Monte Carl.pdf;/home/msca8h/Zotero/storage/V5GX63NC/2004.html},
  journal = {ArXiv200412838 Stat},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@article{green_increasing_2022,
  title = {Increasing the Efficiency of {{Sequential Monte Carlo}} Samplers through the Use of Approximately Optimal {{L}}-Kernels},
  author = {Green, P.L. and Devlin, L.J. and Moore, R.E. and Jackson, R.J. and Li, J. and Maskell, S.},
  year = {2022},
  month = jan,
  volume = {162},
  pages = {108028},
  file = {/home/msca8h/Zotero/storage/NNF9LQDW/Green et al. - 2022 - Increasing the efficiency of Sequential Monte Carl.pdf},
  journal = {Mechanical Systems and Signal Processing},
  language = {en}
}

@article{grenander_representations_1994,
  title = {Representations of {{Knowledge}} in {{Complex Systems}}},
  author = {Grenander, Ulf and Miller, Michael I.},
  year = {1994},
  volume = {56},
  pages = {549--603},
  abstract = {Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject-matter knowledge that can be used as a basis for algorithmic `understanding' of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15 000 \texttimes ) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump-diffusion equation of generalized Langevin form. The jumps occur for the creation-annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. To justify this it is shown that the resulting jump-diffusion process is ergodic so that the solution converges to the desired probability measure. To speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.},
  journal = {J. R. Stat. Soc. Ser. B Methodol.},
  number = {4}
}

@inproceedings{griffiths_constrained_2017,
  title = {Constrained {{Bayesian}} Optimization for Automatic Chemical Design},
  booktitle = {Proceedings of the {{NIPS Workshop}} on {{Bayesian Optimization}} ({{BayesOpt}}'17)},
  author = {Griffiths, Ryan-Rhys and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2017},
  abstract = {Automatic Chemical Design provides a framework for generating novel molecules with optimized molecular properties. The current model suffers from the pathology that it tends to produce invalid molecular structures. By reformulating the search procedure as a constrained Bayesian optimization problem, we showcase improvements in both the validity and quality of the generated molecules. We demonstrate that the model consistently produces novel molecules ranking above the 90th percentile of the distribution over training set scores across a range of objective functions. Importantly, our method suffers no degradation in the complexity or the diversity of the generated molecules.},
  file = {/home/msca8h/Zotero/storage/S4WAADMA/Griffiths and Hernández-Lobato - 2017 - Constrained Bayesian Optimization for Automatic Ch.pdf;/home/msca8h/Zotero/storage/ZB93RBZS/1709.html},
  keywords = {Statistics - Machine Learning}
}

@inproceedings{grubel_performance_2015,
  title = {The {{Performance Implication}} of {{Task Size}} for {{Applications}} on the {{HPX Runtime System}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Grubel, Patricia and Kaiser, Hartmut and Cook, Jeanine and Serio, Adrian},
  year = {2015},
  month = sep,
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Grubel et al. - 2015 - The Performance Implication of Task Size for Appli.pdf}
}

@article{gu_recent_2018,
  title = {Recent Advances in Convolutional Neural Networks},
  author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
  year = {2018},
  month = may,
  volume = {77},
  pages = {354--377},
  file = {/home/msca8h/Zotero/storage/6HSV9JLM/Gu et al. - 2018 - Recent advances in convolutional neural networks.pdf;/home/msca8h/Zotero/storage/CTXXIA74/gu2017.pdf},
  journal = {Pattern Recognition},
  language = {en}
}

@inproceedings{gupta_exploiting_2018,
  title = {Exploiting {{Strategy}}-{{Space Diversity}} for {{Batch Bayesian Optimization}}},
  booktitle = {Proceedings of the {{Twenty}}-{{First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gupta, Sunil and Shilton, Alistair and Rana, Santu and Venkatesh, Svetha},
  year = {2018},
  month = apr,
  volume = {84},
  pages = {538--547},
  publisher = {{PMLR}},
  address = {{Playa Blanca, Lanzarote, Canary Islands}},
  abstract = {This paper proposes a novel approach to batch Bayesian optimisation using a multi-objective optimisation framework with exploitation and exploration forming two objectives. The key advantage of this approach is that it uses a suite of strategies to balance exploration and exploitation and thus can efficiently handle the optimisation of a variety of functions with small to large number of local extrema. Another advantage is that it automatically determines the batch size within a specified budget avoiding unnecessary function evaluations. Theoretical analysis shows that the regret not only reduces sub-linearly but also by an additional reduction factor determined by the batch size. We demonstrate the efficiency of our algorithm by optimising a variety of benchmark functions, performing hyperparameter tuning of support vector regression and classification, and finally heat treatment process of an Al-Sc alloy. Comparisons with recent baseline algorithms confirm the usefulness of our algorithm.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{gustafson_reevaluating_1988,
  title = {Reevaluating {{Amdahl}}'s Law},
  author = {Gustafson, John L.},
  year = {1988},
  month = may,
  volume = {31},
  pages = {532--533},
  file = {/home/msca8h/Zotero/storage/4IL35GWU/Gustafson - 1988 - Reevaluating Amdahl's law.pdf},
  journal = {Commun. ACM},
  language = {en},
  number = {5}
}

@article{gustafson_value_2004,
  title = {On the {{Value}} of Derivative Evaluations and Random Walk Suppression in {{Markov Chain Monte Carlo}} Algorithms},
  author = {Gustafson, Paul and MacNab, Ying C. and Wen, Sijin},
  year = {2004},
  month = jan,
  volume = {14},
  pages = {23--38},
  journal = {Statistics and Computing},
  language = {en},
  number = {1}
}

@inproceedings{gustafsson_evaluating_2020,
  title = {Evaluating Scalable {{Bayesian}} Deep Learning Methods for Robust Computer Vision},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Gustafsson, Fredrik K. and Danelljan, Martin and Schon, Thomas B.},
  year = {2020},
  month = jun,
  pages = {1289--1298},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  file = {/home/msca8h/Zotero/storage/N63YCCXS/Gustafsson et al. - 2020 - Evaluating Scalable Bayesian Deep Learning Methods.pdf}
}

@article{gyori_nonasymptotic_2015,
  title = {Non-Asymptotic Confidence Intervals for {{MCMC}} in Practice},
  author = {Gyori, Benjamin M. and Paulin, Daniel},
  year = {2015},
  month = sep,
  abstract = {Using concentration inequalities, we give non-asymptotic confidence intervals for estimates obtained by Markov chain Monte Carlo (MCMC) simulations, when using the approximation \$\textbackslash mathbb\{E\}\_\{\textbackslash pi\} f\textbackslash approx (1/(N-t\_0))\textbackslash cdot \textbackslash sum\_\{i=t\_0+1\}\^N f(X\_i)\$. To allow the application of non-asymptotic error bounds in practice, here we state bounds formulated in terms of the spectral properties of the chain and the properties of \$f\$ and propose estimators of the parameters appearing in the bounds, including the spectral gap, mixing time, and asymptotic variance. We introduce a method for setting the burn-in time and the initial distribution that is theoretically well-founded and yet is relatively simple to apply. We also investigate the estimation of \$\textbackslash mathbb\{E\}\_\{\textbackslash pi\}f\$ via subsampling and by using parallel runs instead of a single run. Our results are applicable to both reversible and non-reversible Markov chains on discrete as well as general state spaces. We illustrate our methods by simulations for three examples of Bayesian inference in the context of risk models and clinical trials.},
  archiveprefix = {arXiv},
  eprint = {1212.2016},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/2CNZD5IU/Gyori and Paulin - 2015 - Non-asymptotic confidence intervals for MCMC in pr.pdf;/home/msca8h/Zotero/storage/BI28PNCY/1212.html},
  journal = {arXiv:1212.2016 [math]},
  keywords = {65C05; 60J10; 62M05; 82B20; 68Q87; 68W20,Mathematics - Probability},
  primaryclass = {math}
}

@article{haario2001,
  title = {An Adaptive Metropolis Algorithm},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  year = {2001},
  month = apr,
  volume = {7},
  pages = {223--242},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  fjournal = {Bernoulli},
  journal = {Bernoulli},
  number = {2}
}

@inproceedings{habib2018auxiliary,
  title = {Auxiliary Variational {{MCMC}}},
  booktitle = {International Conference on Learning Representations},
  author = {Habib, Raza and Barber, David},
  year = {2019}
}

@article{hagerup_allocating_1997,
  title = {Allocating Independent Tasks to Parallel Processors: An Experimental Study},
  shorttitle = {Allocating {{Independent Tasks}} to {{Parallel Processors}}},
  author = {Hagerup, Torben},
  year = {1997},
  month = dec,
  volume = {47},
  pages = {185--197},
  file = {/home/msca8h/Documents/parallel_scheduling/Hagerup - 1997 - Allocating Independent Tasks to Parallel Processor.pdf},
  journal = {J. Parallel Distrib. Comput.},
  language = {en},
  number = {2}
}

@book{hairer_geometric_2006,
  title = {Geometric {{Numerical Integration}}},
  author = {Hairer, Ernst},
  year = {2006},
  volume = {31},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  file = {/home/msca8h/Zotero/storage/3ATR88KQ/2006 - Geometric Numerical Integration.pdf},
  language = {en},
  series = {Springer {{Series}} in {{Computational Mathematics}}}
}

@incollection{hairer_symplectic_2006,
  title = {Symplectic {{Integration}} of {{Hamiltonian Systems}}},
  booktitle = {Geometric {{Numerical Integration}}},
  author = {Hairer, Ernst and Wanner, Gerhard and Lubich, Christian},
  year = {2006},
  volume = {31},
  pages = {179--236},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  language = {en}
}

@inproceedings{han_stein_2017,
  title = {Stein {{Variational Adaptive Importance Sampling}}},
  booktitle = {Proceedings of the {{Thirty}}-{{Third Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}, {{UAI}} 2017, {{Sydney}}, {{Australia}}, {{August}} 11-15, 2017},
  author = {Han, Jun and Liu, Qiang},
  year = {2017}
}

@inproceedings{han_stein_2018,
  title = {Stein {{Variational Gradient Descent Without Gradient}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Han, Jun and Liu, Qiang},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {1900--1908},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm Sweden}},
  abstract = {Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the introduced bias by re-weighting the gradients in a proper form. We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits all the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and further, propose an annealed GF-SVGD that consistently outperforms a number of recent advanced gradient-free MCMC methods in our empirical studies.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{hannun_cardiologistlevel_2019,
  title = {Cardiologist-Level Arrhythmia Detection and Classification in Ambulatory Electrocardiograms Using a Deep Neural Network},
  author = {Hannun, Awni Y. and Rajpurkar, Pranav and Haghpanahi, Masoumeh and Tison, Geoffrey H. and Bourn, Codie and Turakhia, Mintu P. and Ng, Andrew Y.},
  year = {07/jan/2019},
  volume = {25},
  pages = {65--69},
  journal = {Nat. Med.},
  number = {1}
}

@incollection{hansen_cma_2006,
  title = {The {{CMA}} Evolution Strategy: A Comparing Review},
  shorttitle = {The {{CMA Evolution Strategy}}},
  booktitle = {Towards a {{New Evolutionary Computation}}},
  author = {Hansen, Nikolaus},
  year = {2006},
  volume = {192},
  pages = {75--102},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Zotero/storage/3QQYYDLZ/Hansen - 2006 - The CMA Evolution Strategy A Comparing Review.pdf},
  language = {en}
}

@article{hansen_cma_2016,
  title = {The {{CMA}} Evolution Strategy: {{A}} Tutorial},
  author = {Hansen, Nikolaus},
  year = {2016},
  archiveprefix = {arXiv},
  eprint = {1604.00772},
  eprinttype = {arxiv},
  journal = {ArXiv Prepr. ArXiv160400772}
}

@misc{hansen_cmaes_2019,
  title = {{{CMA}}-{{ES}}/Pycma: R2.7.0},
  author = {Hansen, Nikolaus and Akimoto, Youhei and Baudis, Petr},
  year = {2019},
  month = apr,
  howpublished = {Zenodo}
}

@inproceedings{hartono_annotationbased_2009,
  title = {Annotation-Based Empirical Performance Tuning Using {{Orio}}},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}},
  author = {Hartono, Albert and Norris, Boyana and Sadayappan, P.},
  year = {2009},
  month = may,
  pages = {1--11},
  publisher = {{IEEE}},
  address = {{Rome, Italy}},
  file = {/home/msca8h/Zotero/storage/C5Z5CP4D/Hartono et al. - 2009 - Annotation-based empirical performance tuning usin.pdf}
}

@article{hastings_monte_1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {Hastings, W. K.},
  year = {1970},
  month = apr,
  volume = {57},
  pages = {97--109},
  file = {/home/msca8h/Zotero/storage/YF7HGJJC/Hastings - 1970 - Monte Carlo sampling methods using Markov chains a.pdf},
  journal = {Biometrika},
  language = {en},
  number = {1}
}

@inproceedings{he_powerful_2016,
  title = {A {{Powerful Generative Model Using Random Weights}} for the {{Deep Image Representation}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {He, Kun and Wang, Yan and Hopcroft, John},
  year = {2016},
  pages = {631--639},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  series = {{{NIPS}}'16}
}

@inproceedings{hemmsen_ultrasound_2010,
  title = {Ultrasound Image Quality Assessment: A Framework for Evaluation of Clinical Image Quality},
  shorttitle = {Ultrasound Image Quality Assessment},
  booktitle = {Proc. {{SPIE Med}}. {{Imag}}.},
  author = {Hemmsen, Martin Christian and Petersen, Mads M{\o}ller and Nikolov, Svetoslav Ivanov and Nielsen, Michael Backmann and Jensen, J{\o}rgen Arendt},
  year = {2010},
  month = mar,
  pages = {76290C},
  address = {{San Diego, California, USA}},
  file = {/home/msca8h/Zotero/storage/WXP9D6RE/Hemmsen et al. - 2010 - Ultrasound image quality assessment a framework f.pdf}
}

@article{hemmsen_vivo_2012,
  title = {In {{Vivo Evaluation}} of {{Synthetic Aperture Sequential Beamforming}}},
  author = {Hemmsen, Martin Christian and Hansen, Peter M{\o}ller and Lange, Theis and Hansen, Jens Munk and Hansen, Kristoffer Lindskov and Nielsen, Michael Bachmann and Jensen, J{\o}rgen Arendt},
  year = {2012},
  month = apr,
  volume = {38},
  pages = {708--716},
  journal = {Ultrasound in Medicine \& Biology},
  language = {en},
  number = {4}
}

@article{hennig_entropy_2012,
  title = {Entropy {{Search}} for {{Information}}-Efficient {{Global Optimization}}},
  author = {Hennig, Philipp and Schuler, Christian J.},
  year = {2012},
  month = jun,
  volume = {13},
  pages = {1809--1837},
  journal = {J Mach Learn Res},
  keywords = {expectation propagation,Gaussian processes,information,optimization,probability},
  number = {1}
}

@inproceedings{henrandez-lobato_predictive_2014,
  title = {Predictive Entropy Search for Efficient Global Optimization of Black-Box Functions},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {{Henr{\'a}ndez-Lobato}, Jos{\'e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
  year = {2014},
  volume = {27},
  pages = {918--926},
  series = {{{NIPS}}'14}
}

@article{hensman_gaussian_2013,
  title = {Gaussian Processes for Big Data},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},
  year = {2013},
  archiveprefix = {arXiv},
  eprint = {1309.6835},
  eprinttype = {arxiv},
  journal = {ArXiv Prepr. ArXiv13096835}
}

@inproceedings{hensman_mcmc_2015,
  title = {{{MCMC}} for {{Variationally Sparse Gaussian Processes}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Hensman, James and Matthews, Alexander G. de G. and Filippone, Maurizio and Ghahramani, Zoubin},
  year = {2015},
  pages = {1648--1656},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  file = {/home/msca8h/Zotero/storage/63C6CTGI/Hensman et al. - 2015 - MCMC for Variationally Sparse Gaussian Processes.pdf},
  series = {{{NIPS}}'15}
}

@article{hesterberg_weighted_1995,
  title = {Weighted Average Importance Sampling and Defensive Mixture Distributions},
  author = {Hesterberg, Tim},
  year = {1995},
  month = may,
  volume = {37},
  pages = {185--194},
  journal = {Technometrics},
  language = {en},
  number = {2}
}

@article{hettinger_forward_2017,
  title = {Forward {{Thinking}}: {{Building}} and {{Training Neural Networks One Layer}} at a {{Time}}},
  shorttitle = {Forward {{Thinking}}},
  author = {Hettinger, Chris and Christensen, Tanner and Ehlert, Ben and Humpherys, Jeffrey and Jarvis, Tyler and Wade, Sean},
  year = {2017},
  month = jun,
  abstract = {We present a general framework for training deep neural networks without backpropagation. This substantially decreases training time and also allows for construction of deep networks with many sorts of learners, including networks whose layers are defined by functions that are not easily differentiated, like decision trees. The main idea is that layers can be trained one at a time, and once they are trained, the input data are mapped forward through the layer to create a new learning problem. The process is repeated, transforming the data through multiple layers, one at a time, rendering a new data set, which is expected to be better behaved, and on which a final output layer can achieve good performance. We call this forward thinking and demonstrate a proof of concept by achieving state-of-the-art accuracy on the MNIST dataset for convolutional neural networks. We also provide a general mathematical formulation of forward thinking that allows for other types of deep learning problems to be considered.},
  archiveprefix = {arXiv},
  eprint = {1706.02480},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/SR52EAE6/Hettinger et al. - 2017 - Forward Thinking Building and Training Neural Net.pdf;/home/msca8h/Zotero/storage/N36RZQSN/1706.html},
  journal = {ArXiv170602480 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{hoefler_performance_2011,
  title = {Performance Modeling for Systematic Performance Tuning},
  booktitle = {State of the {{Practice Reports}} on - {{SC}} '11},
  author = {Hoefler, Torsten and Gropp, William and Kramer, William and Snir, Marc},
  year = {2011},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Seattle, Washington}},
  file = {/home/msca8h/Zotero/storage/CXPPFFCW/Hoefler et al. - 2011 - Performance modeling for systematic performance tu.pdf},
  language = {en}
}

@inproceedings{hoffeins_examining_2017,
  title = {Examining the {{Reproducibility}} of {{Using Dynamic Loop Scheduling Techniques}} in {{Scientific Applications}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Hoffeins, Franziska and Ciorba, Florina M. and Banicescu, Ioana},
  year = {2017},
  month = may,
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Hoffeins et al. - 2017 - Examining the Reproducibility of Using Dynamic Loo.pdf}
}

@incollection{hoffer_norm_2018,
  title = {Norm Matters: Efficient and Accurate Normalization Schemes in Deep Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  year = {2018},
  pages = {2160--2170},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{hoffman_outputspace_2015,
  title = {Output-Space Predictive Entropy Search for Flexible Global Optimization},
  booktitle = {{{NIPS}} Workshop on {{Bayesian Optimization}}},
  author = {Hoffman, Matthew W and Ghahramani, Zoubin},
  year = {2015}
}

@book{hol_resampling_2004,
  title = {Resampling in Particle Filters},
  author = {Hol, Jeroen D},
  year = {2004},
  publisher = {{Institutionen f\"or systemteknik}}
}

@article{holden_adaptive_2009,
  title = {Adaptive Independent {{Metropolis}}\textendash{{Hastings}}},
  author = {Holden, Lars and Hauge, Ragnar and Holden, Marit},
  year = {2009},
  month = feb,
  volume = {19},
  file = {/home/msca8h/Zotero/storage/XQC5UB3T/Holden et al. - 2009 - Adaptive independent Metropolis–Hastings.pdf},
  journal = {Ann. Appl. Probab.},
  number = {1}
}

@article{honeine_online_2012,
  title = {Online {{Kernel Principal Component Analysis}}: {{A Reduced}}-{{Order Model}}},
  shorttitle = {Online {{Kernel Principal Component Analysis}}},
  author = {Honeine, P.},
  year = {2012},
  month = sep,
  volume = {34},
  pages = {1814--1826},
  file = {/home/msca8h/Zotero/storage/D7XI2LWP/Honeine - 2012 - Online Kernel Principal Component Analysis A Redu.pdf},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  number = {9}
}

@article{honeine_preimage_2011,
  title = {Preimage {{Problem}} in {{Kernel}}-{{Based Machine Learning}}},
  author = {Honeine, Paul and Richard, Cedric},
  year = {2011},
  month = mar,
  volume = {28},
  pages = {77--88},
  file = {/home/msca8h/Zotero/storage/U7SHY2UD/Honeine and Richard - 2011 - Preimage Problem in Kernel-Based Machine Learning.pdf},
  journal = {IEEE Signal Process. Mag.},
  number = {2}
}

@inproceedings{hong_efficient_2018,
  title = {Efficient Sparse-Matrix Multi-Vector Product on {{GPUs}}},
  booktitle = {Proceedings of the 27th {{International Symposium}} on {{High}}-{{Performance Parallel}} and {{Distributed Computing}} - {{HPDC}} '18},
  author = {Hong, Changwan and Sadayappan, P. and {Sukumaran-Rajam}, Aravind and Bandyopadhyay, Bortik and Kim, Jinsung and Kurt, S{\"u}reyya Emre and Nisa, Israt and Sabhlok, Shivani and {\c C}ataly{\"u}rek, {\"U}mit V. and Parthasarathy, Srinivasan},
  year = {2018},
  pages = {66--79},
  publisher = {{ACM Press}},
  address = {{Tempe, Arizona}},
  language = {en}
}

@article{horowitz_generalized_1991,
  title = {A Generalized Guided {{Monte Carlo}} Algorithm},
  author = {Horowitz, Alan M.},
  year = {1991},
  month = oct,
  volume = {268},
  pages = {247--252},
  file = {/home/msca8h/Zotero/storage/82JGXGWZ/Horowitz - 1991 - A generalized guided Monte Carlo algorithm.pdf},
  journal = {Physics Letters B},
  language = {en},
  number = {2}
}

@inproceedings{hou_autotuning_2017,
  title = {Auto-{{Tuning Strategies}} for {{Parallelizing Sparse Matrix}}-{{Vector}} ({{SpMV}}) {{Multiplication}} on {{Multi}}- and {{Many}}-{{Core Processors}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Hou, Kaixi and Feng, Wu-chun and Che, Shuai},
  year = {2017},
  month = may,
  pages = {713--722},
  publisher = {{IEEE}},
  address = {{Orlando / Buena Vista, FL, USA}}
}

@inproceedings{hu_cluster_2016,
  title = {Cluster Driven Anisotropic Diffusion for Speckle Reduction in Ultrasound Images},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Hu, Zilong and Tang, Jinshan},
  year = {2016},
  month = sep,
  pages = {2325--2329},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}}
}

@article{hummel_factoring_1992,
  title = {Factoring: A Method for Scheduling Parallel Loops},
  author = {Hummel, Susan Flynn and Schonberg, Edith and Flynn, Lawrence E.},
  year = {1992},
  month = aug,
  volume = {35},
  pages = {90--101},
  file = {/home/msca8h/Documents/parallel_scheduling/Hummel et al. - 1992 - Factoring A Method for Scheduling Parallel Loops.pdf},
  journal = {Commun ACM},
  keywords = {chunking,dynamic scheduling,parallel loop scheduling,partitioning},
  number = {8}
}

@inproceedings{hummel_loadsharing_1996,
  title = {Load-Sharing in Heterogeneous Systems via Weighted Factoring},
  booktitle = {Proceedings of the {{Eighth Annual ACM Symposium}} on {{Parallel Algorithms}} and {{Architectures}}},
  author = {Hummel, Susan Flynn and Schmidt, Jeanette and Uma, R. N. and Wein, Joel},
  year = {1996},
  pages = {318--328},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  file = {/home/msca8h/Documents/parallel_scheduling/Hummel et al. - 1996 - Load-sharing in Heterogeneous Systems via Weighted.pdf},
  series = {{{SPAA}} '96}
}

@article{iba_population_2001,
  title = {Population {{Monte Carlo}} Algorithms.},
  author = {Iba, Yukito},
  year = {2001},
  volume = {16},
  pages = {279--286},
  file = {/home/msca8h/Zotero/storage/H7935D92/Iba - 2001 - Population Monte Carlo algorithms..pdf},
  journal = {Transactions of the Japanese Society for Artificial Intelligence}
}

@article{iglberger_expression_2012,
  title = {Expression {{Templates Revisited}}: {{A Performance Analysis}} of {{Current Methodologies}}},
  shorttitle = {Expression {{Templates Revisited}}},
  author = {Iglberger, Klaus and Hager, Georg and Treibig, Jan and R{\"u}de, Ulrich},
  year = {2012},
  month = jan,
  volume = {34},
  pages = {C42-C69},
  journal = {SIAM J. Sci. Comput.},
  language = {en},
  number = {2}
}

@article{imperialcollegecovid-19responseteam_estimating_2020,
  title = {Estimating the Effects of Non-Pharmaceutical Interventions on {{COVID}}-19 in {{Europe}}},
  author = {{Imperial College COVID-19 Response Team} and Flaxman, Seth and Mishra, Swapnil and Gandy, Axel and Unwin, H. Juliette T. and Mellan, Thomas A. and Coupland, Helen and Whittaker, Charles and Zhu, Harrison and Berah, Tresnia and Eaton, Jeffrey W. and Monod, M{\'e}lodie and Ghani, Azra C. and Donnelly, Christl A. and Riley, Steven and Vollmer, Michaela A. C. and Ferguson, Neil M. and Okell, Lucy C. and Bhatt, Samir},
  year = {2020},
  month = jun,
  file = {/home/msca8h/Zotero/storage/CFWA2QPN/Imperial College COVID-19 Response Team et al. - 2020 - Estimating the effects of non-pharmaceutical inter.pdf},
  journal = {Nature},
  language = {en}
}

@inproceedings{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  pages = {448--456},
  publisher = {{JMLR.org}},
  series = {{{ICML}}'15}
}

@article{izmailov_subspace_2019,
  title = {Subspace {{Inference}} for {{Bayesian Deep Learning}}},
  author = {Izmailov, Pavel and Maddox, Wesley J. and Kirichenko, Polina and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = jul,
  abstract = {Bayesian inference was once a gold standard for learning with neural networks, providing accurate full predictive distributions and well calibrated uncertainty. However, scaling Bayesian inference techniques to deep neural networks is challenging due to the high dimensionality of the parameter space. In this paper, we construct low-dimensional subspaces of parameter space, such as the first principal components of the stochastic gradient descent (SGD) trajectory, which contain diverse sets of high performing models. In these subspaces, we are able to apply elliptical slice sampling and variational inference, which struggle in the full parameter space. We show that Bayesian model averaging over the induced posterior in these subspaces produces accurate predictions and well calibrated predictive uncertainty for both regression and image classification.},
  archiveprefix = {arXiv},
  eprint = {1907.07504},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/F2WJBSZJ/Izmailov et al. - 2019 - Subspace Inference for Bayesian Deep Learning.pdf;/home/msca8h/Zotero/storage/MHZI3QHT/1907.html},
  journal = {ArXiv190707504 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{jackson_electrostrictive_2019,
  title = {Electrostrictive {{Cavitation}} in {{Water Induced}} by a {{SnO2 Nanoparticle}}},
  author = {Jackson, Shane and Nakano, Aiichiro and Vashishta, Priya and Kalia, Rajiv K.},
  year = {2019},
  month = dec,
  volume = {4},
  pages = {22274--22279},
  file = {/home/msca8h/Zotero/storage/HCCPA52N/Jackson et al. - 2019 - Electrostrictive Cavitation in Water Induced by a .pdf},
  journal = {ACS Omega},
  language = {en},
  number = {27}
}

@article{jacob_unbiased_2020,
  title = {Unbiased {{Markov}} Chain {{Monte Carlo}} Methods with Couplings},
  author = {Jacob, Pierre E. and O'Leary, John and Atchad{\'e}, Yves F.},
  year = {2020},
  month = jul,
  volume = {82},
  pages = {543--600},
  file = {/home/msca8h/Zotero/storage/SBUQB749/Jacob et al. - 2020 - Unbiased Markov chain Monte Carlo methods with cou.pdf},
  journal = {J. Roy. Stat. Soc. B},
  language = {en},
  number = {3}
}

@article{jacob_using_2011,
  title = {Using Parallel Computation to Improve Independent {{Metropolis}}\textendash{{Hastings}} Based Estimation},
  author = {Jacob, P. and Robert, C. P. and Smith, M. H.},
  year = {2011},
  month = jan,
  volume = {20},
  pages = {616--635},
  file = {/home/msca8h/Zotero/storage/5S5JSC42/Jacob et al. - 2011 - Using Parallel Computation to Improve Independent .pdf},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {3}
}

@incollection{jamil_test_2013,
  title = {Test {{Functions}} for {{Global Optimization}}},
  booktitle = {Swarm {{Intelligence}} and {{Bio}}-{{Inspired Computation}}},
  author = {Jamil, Momin and Yang, Xin-She and Zepernick, Hans-J{\"u}rgen},
  year = {2013},
  pages = {193--222},
  publisher = {{Elsevier}},
  language = {en}
}

@article{jasra_inference_2011,
  title = {Inference for {{L\'evy}}-{{Driven Stochastic Volatility Models}} via {{Adaptive Sequential Monte Carlo}}: {{L\'evy}}-Driven Stochastic Volatility},
  shorttitle = {Inference for {{L\'evy}}-{{Driven Stochastic Volatility Models}} via {{Adaptive Sequential Monte Carlo}}},
  author = {Jasra, Ajay and Stephens, David A. and Doucet, Arnaud and Tsagaris, Theodoros},
  year = {2011},
  month = mar,
  volume = {38},
  pages = {1--22},
  journal = {Scand. J. Stat.},
  language = {en},
  number = {1}
}

@article{jasra_populationbased_2007,
  title = {On Population-Based Simulation for Static Inference},
  author = {Jasra, Ajay and Stephens, David A. and Holmes, Christopher C.},
  year = {2007},
  month = aug,
  volume = {17},
  pages = {263--279},
  journal = {Stat Comput},
  language = {en},
  number = {3}
}

@inproceedings{jerfel2021variational,
  title = {Variational Refinement for Importance {{SamplingUsing}} the Forward Kullback-Leibler Divergence},
  booktitle = {Third Symposium on Advances in Approximate Bayesian Inference},
  author = {Jerfel, Ghassen and Wang, Serena Lutong and Fannjiang, Clara and Heller, Katherine A and Ma, Yian and Jordan, Michael},
  year = {2021}
}

@article{ji_parallelizing_2019,
  title = {Parallelizing {{Word2Vec}} in {{Shared}} and {{Distributed Memory}}},
  author = {Ji, Shihao and Satish, Nadathur and Li, Sheng and Dubey, Pradeep},
  year = {2019},
  pages = {1--1},
  file = {/home/msca8h/Zotero/storage/J6XQ383P/Ji et al. - 2019 - Parallelizing Word2Vec in Shared and Distributed M.pdf},
  journal = {IEEE Trans. Parallel Distrib. Syst.}
}

@inproceedings{jia_autotuning_2016,
  title = {Auto-Tuning {{Spark Big Data Workloads}} on {{POWER8}}: {{Prediction}}-{{Based Dynamic SMT Threading}}},
  shorttitle = {Auto-Tuning {{Spark Big Data Workloads}} on {{POWER8}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Parallel Architectures}} and {{Compilation}} - {{PACT}} '16},
  author = {Jia, Zhen and Xue, Chao and Chen, Guancheng and Zhan, Jianfeng and Zhang, Lixin and Lin, Yonghua and Hofstee, Peter},
  year = {2016},
  pages = {387--400},
  publisher = {{ACM Press}},
  address = {{Haifa, Israel}},
  language = {en}
}

@incollection{jiang_linear_2018,
  title = {A {{Linear Speedup Analysis}} of {{Distributed Deep Learning}} with {{Sparse}} and {{Quantized Communication}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Jiang, Peng and Agrawal, Gagan},
  year = {2018},
  pages = {2525--2536},
  publisher = {{Curran Associates, Inc.}}
}

@article{jiang_mcmc_2021,
  title = {{{MCMC}} Confidence Intervals and Biases},
  author = {Jiang, Yu Hang and Liu, Tong and Lou, Zhiya and Rosenthal, Jeffrey S. and Shangguan, Shanshan and Wang, Fei and Wu, Zixuan},
  year = {2021},
  month = jun,
  abstract = {The recent paper "Simple confidence intervals for MCMC without CLTs" by J.S. Rosenthal, showed the derivation of a simple MCMC confidence interval using only Chebyshev's inequality, not CLT. That result required certain assumptions about how the estimator bias and variance grow with the number of iterations \$n\$. In particular, the bias is \$o(1/\textbackslash sqrt\{n\})\$. This assumption seemed mild. It is generally believed that the estimator bias will be \$O(1/n)\$ and hence \$o(1/\textbackslash sqrt\{n\})\$. However, questions were raised by researchers about how to verify this assumption. Indeed, we show that this assumption might not always hold. In this paper, we seek to simplify and weaken the assumptions in the previously mentioned paper, to make MCMC confidence intervals without CLTs more widely applicable.},
  archiveprefix = {arXiv},
  eprint = {2012.02816},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/6HAE5WUU/Jiang et al. - 2021 - MCMC Confidence Intervals and Biases.pdf;/home/msca8h/Zotero/storage/X7FK8STD/2012.html},
  journal = {ArXiv201202816 Math Stat},
  keywords = {60J10; 62E20,Mathematics - Statistics Theory},
  primaryclass = {math, stat}
}

@inproceedings{jiang*_fantastic_2020,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Jiang*, Yiding and Neyshabur*, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2020}
}

@article{jin_openmp_1999,
  title = {The {{OpenMP}} Implementation of {{NAS}} Parallel Benchmarks and Its Performance},
  author = {Jin, Hao-Qiang and Frumkin, Michael and Yan, Jerry},
  year = {1999}
}

@inproceedings{jinbumkang_new_2014,
  title = {A New Feature-Enhanced Speckle Reduction Method Based on Multiscale Analysis and Synthesis for Ultrasound {{B}}-Mode Imaging},
  booktitle = {2014 {{IEEE International Ultrasonics Symposium}}},
  author = {{Jinbum Kang} and Yoo, Yangmo},
  year = {2014},
  month = sep,
  pages = {1320--1323},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}}
}

@article{JMLR:v15:hoffman14a,
  title = {The No-u-Turn Sampler: Adaptively Setting Path Lengths in {{Hamiltonian Monte Carlo}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2014},
  volume = {15},
  pages = {1593--1623},
  journal = {J. Mach. Learn. Res.},
  number = {47}
}

@article{JMLR:v15:nishihara14a,
  title = {Parallel {{MCMC}} with Generalized Elliptical Slice Sampling},
  author = {Nishihara, Robert and Murray, Iain and Adams, Ryan P.},
  year = {2014},
  volume = {15},
  pages = {2087--2112},
  journal = {J. Mach. Learn. Res.},
  number = {61}
}

@article{JMLR:v18:15-205,
  title = {On {{Markov}} Chain {{Monte Carlo}} Methods for Tall Data},
  author = {Bardenet, R{\'e}mi and Doucet, Arnaud and Holmes, Chris},
  year = {2017},
  volume = {18},
  pages = {1--43},
  journal = {J. Mach. Learn. Res.},
  number = {47}
}

@article{JMLR:v18:16-107,
  title = {Automatic Differentiation Variational Inference},
  author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
  year = {2017},
  volume = {18},
  pages = {1--45},
  journal = {J. Mach. Learn. Res.},
  number = {14}
}

@article{JMLR:v19:17-084,
  title = {Scalable Bayes via Barycenter in Wasserstein Space},
  author = {Srivastava, Sanvesh and Li, Cheng and Dunson, David B.},
  year = {2018},
  volume = {19},
  pages = {1--35},
  journal = {J. Mach. Learn. Res.},
  number = {8}
}

@article{JMLR:v20:17-757,
  title = {Dependent Relevance Determination for Smooth and Structured Sparse Regression},
  author = {Wu, Anqi and Koyejo, Oluwasanmi and Pillow, Jonathan},
  year = {2019},
  volume = {20},
  pages = {1--43},
  journal = {J. Mach. Learn. Res.},
  number = {89}
}

@article{JMLR:v20:18-403,
  title = {Pyro: {{Deep}} Universal Probabilistic Programming},
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
  year = {2019},
  volume = {20},
  pages = {1--6},
  journal = {J. Mach. Learn. Res.},
  number = {28}
}

@article{JMLR:v20:19-306,
  title = {Log-Concave Sampling: {{Metropolis}}-{{Hastings}} Algorithms Are Fast},
  author = {Dwivedi, Raaz and Chen, Yuansi and Wainwright, Martin J. and Yu, Bin},
  year = {2019},
  volume = {20},
  pages = {1--42},
  journal = {J. Mach. Learn. Res.},
  number = {183}
}

@book{johnson_nlopt_2011,
  title = {The {{NLopt}} Nonlinear-Optimization Package},
  author = {Johnson, Steven G.},
  year = {2011},
  keywords = {imported}
}

@article{jones_efficient_1998,
  title = {Efficient Global Optimization of Expensive Black-Box Functions},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  year = {1998},
  volume = {13},
  pages = {455--492},
  journal = {J. Glob. Optim.},
  number = {4}
}

@article{jones_lipschitzian_1993,
  title = {Lipschitzian Optimization without the {{Lipschitz}} Constant},
  author = {Jones, D. R. and Perttunen, C. D. and Stuckman, B. E.},
  year = {1993},
  month = oct,
  volume = {79},
  pages = {157--181},
  journal = {J Optim Theory Appl},
  language = {en},
  number = {1}
}

@article{jones_remark_1984,
  title = {Remark {{AS R50}}: {{A Remark}} on {{Algorithm AS}} 176. {{Kernal Density Estimation Using}} the {{Fast Fourier Transform}}},
  shorttitle = {Remark {{AS R50}}},
  author = {Jones, M. C. and Lotwick, H. W.},
  year = {1984},
  volume = {33},
  pages = {120},
  journal = {Applied Statistics},
  number = {1}
}

@article{jordan_introduction_1999,
  title = {An Introduction to Variational Methods for Graphical Models},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  year = {1999},
  volume = {37},
  pages = {183--233},
  file = {/home/msca8h/Zotero/storage/IG37GFMG/Jordan et al. - 1999 - [No title found].pdf},
  journal = {Mach. Learn.},
  number = {2}
}

@inproceedings{jordan_multiobjective_2012,
  title = {A Multi-Objective Auto-Tuning Framework for Parallel Codes},
  booktitle = {2012 {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Jordan, Herbert and Thoman, Peter and Durillo, Juan J. and Pellegrini, Simone and Gschwandtner, Philipp and Fahringer, Thomas and Moritsch, Hans},
  year = {2012},
  month = nov,
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}}
}

@inproceedings{jordan_multiobjective_2012a,
  title = {A Multi-Objective Auto-Tuning Framework for Parallel Codes},
  booktitle = {2012 {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Jordan, Herbert and Thoman, Peter and Durillo, Juan J. and Pellegrini, Simone and Gschwandtner, Philipp and Fahringer, Thomas and Moritsch, Hans},
  year = {2012},
  month = nov,
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}}
}

@article{jospin_handson_2020,
  title = {Hands-on {{Bayesian Neural Networks}} -- a {{Tutorial}} for {{Deep Learning Users}}},
  author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
  year = {2020},
  month = jul,
  abstract = {Modern deep learning methods have equipped researchers and engineers with incredibly powerful tools to tackle problems that previously seemed impossible. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural networks predictions. This paper provides a tutorial for researchers and scientists who are using machine learning, especially deep learning, with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks.},
  archiveprefix = {arXiv},
  eprint = {2007.06823},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/MMVZ4S4W/Jospin et al. - 2020 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf;/home/msca8h/Zotero/storage/QWAVED58/2007.html},
  journal = {ArXiv200706823 Cs Stat},
  keywords = {62-02 (Primary),Computer Science - Machine Learning,G.3,I.2.6,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{kadav_asap_2016,
  title = {{{ASAP}}: {{Asynchronous Approximate Data}}-{{Parallel Computation}}},
  shorttitle = {{{ASAP}}},
  author = {Kadav, Asim and Kruus, Erik},
  year = {2016},
  month = dec,
  abstract = {Emerging workloads, such as graph processing and machine learning are approximate because of the scale of data involved and the stochastic nature of the underlying algorithms. These algorithms are often distributed over multiple machines using bulk-synchronous processing (BSP) or other synchronous processing paradigms such as map-reduce. However, data parallel processing primitives such as repeated barrier and reduce operations introduce high synchronization overheads. Hence, many existing data-processing platforms use asynchrony and staleness to improve data-parallel job performance. Often, these systems simply change the synchronous communication to asynchronous between the worker nodes in the cluster. This improves the throughput of data processing but results in poor accuracy of the final output since different workers may progress at different speeds and process inconsistent intermediate outputs. In this paper, we present ASAP, a model that provides asynchronous and approximate processing semantics for data-parallel computation. ASAP provides fine-grained worker synchronization using NOTIFY-ACK semantics that allows independent workers to run asynchronously. ASAP also provides stochastic reduce that provides approximate but guaranteed convergence to the same result as an aggregated all-reduce. In our results, we show that ASAP can reduce synchronization costs and provides 2-10X speedups in convergence and up to 10X savings in network costs for distributed machine learning applications and provides strong convergence guarantees.},
  archiveprefix = {arXiv},
  eprint = {1612.08608},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/5L8QTER2/Kadav and Kruus - 2016 - ASAP Asynchronous Approximate Data-Parallel Compu.pdf;/home/msca8h/Zotero/storage/YNYDDIFR/1612.html},
  journal = {ArXiv161208608 Cs},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{kadupitiya_machine_2020,
  title = {Machine Learning for Parameter Auto-Tuning in Molecular Dynamics Simulations: {{Efficient}} Dynamics of Ions near Polarizable Nanoparticles},
  shorttitle = {Machine Learning for Parameter Auto-Tuning in Molecular Dynamics Simulations},
  author = {Kadupitiya, Jcs and Fox, Geoffrey C and Jadhao, Vikram},
  year = {2020},
  month = jan,
  pages = {109434201989945},
  abstract = {Simulating the dynamics of ions near polarizable nanoparticles (NPs) using coarse-grained models is extremely challenging due to the need to solve the Poisson equation at every simulation timestep. Recently, a molecular dynamics (MD) method based on a dynamical optimization framework bypassed this obstacle by representing the polarization charge density as virtual dynamic variables and evolving them in parallel with the physical dynamics of ions. We highlight the computational gains accessible with the integration of machine learning (ML) methods for parameter prediction in MD simulations by demonstrating how they were realized in MD simulations of ions near polarizable NPs. An artificial neural network\textendash based regression model was integrated with MD simulation and predicted the optimal simulation timestep and optimization parameters characterizing the virtual system with 94.3\% success. The ML-enabled auto-tuning of parameters generated accurate dynamics of ions for {$\approx$} 10 million steps while improving the stability of the simulation by over an order of magnitude. The integration of ML-enhanced framework with hybrid Open Multi-Processing / Message Passing Interface (OpenMP/MPI) parallelization techniques reduced the computational time of simulating systems with thousands of ions and induced charges from thousands of hours to tens of hours, yielding a maximum speedup of {$\approx$} 3 from ML-only acceleration and a maximum speedup of {$\approx$} 600 from the combination of ML and parallel computing methods. Extraction of ionic structure in concentrated electrolytes near oil\textendash water emulsions demonstrates the success of the method. The approach can be generalized to select optimal parameters in other MD applications and energy minimization problems.},
  file = {/home/msca8h/Zotero/storage/Z8NXCT6I/Kadupitiya et al. - 2020 - Machine learning for parameter auto-tuning in mole.pdf},
  journal = {The International Journal of High Performance Computing Applications},
  language = {en}
}

@inproceedings{kaleem_adaptive_2014,
  title = {Adaptive Heterogeneous Scheduling for Integrated {{GPUs}}},
  booktitle = {2014 23rd {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Kaleem, R. and Barik, R. and Shpeisman, T. and Hu, C. and Lewis, B. T. and Pingali, K.},
  year = {2014},
  month = aug,
  pages = {151--162},
  keywords = {adaptive heterogeneous scheduling,asymmetric scheduling algorithm,C++ languages,CPU-GPU communication,graphics processing units,Graphics processing units,Heterogeneous computing,integrated CPU-GPU processors,integrated GPUs,Intel 4th generation core processor,Irregular applications,Kernel,load balancing,microprocessor chips,NVIDIA discrete GPU,online profiling-based scheduling algorithms,processor scheduling,Programming,scheduling,Scheduling algorithms}
}

@article{kalinnik_online_2014,
  title = {Online Auto-Tuning for the Time-Step-Based Parallel Solution of {{ODEs}} on Shared-Memory Systems},
  author = {Kalinnik, Natalia and Korch, Matthias and Rauber, Thomas},
  year = {2014},
  month = aug,
  volume = {74},
  pages = {2722--2744},
  journal = {Journal of Parallel and Distributed Computing},
  language = {en},
  number = {8}
}

@inproceedings{kamil_autotuning_2010,
  title = {An Auto-Tuning Framework for Parallel Multicore Stencil Computations},
  booktitle = {2010 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}} ({{IPDPS}})},
  author = {Kamil, Shoaib and Chan, Cy and Oliker, Leonid and Shalf, John and Williams, Samuel},
  year = {2010},
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  file = {/home/msca8h/Zotero/storage/HNYEDUZ2/Kamil et al. - 2010 - An auto-tuning framework for parallel multicore st.pdf}
}

@incollection{kamp_efficient_2019,
  title = {Efficient {{Decentralized Deep Learning}} by {{Dynamic Model Averaging}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kamp, Michael and Adilova, Linara and Sicking, Joachim and H{\"u}ger, Fabian and Schlicht, Peter and Wirtz, Tim and Wrobel, Stefan},
  year = {2019},
  volume = {11051},
  pages = {393--409},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  file = {/home/msca8h/Zotero/storage/PGYVH8C2/Kamp et al. - 2019 - Efficient Decentralized Deep Learning by Dynamic M.pdf},
  language = {en}
}

@inproceedings{kandasamy_high_2015,
  title = {High {{Dimensional Bayesian Optimisation}} and {{Bandits}} via {{Additive Models}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Kandasamy, Kirthevasan and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  year = {2015},
  pages = {295--304},
  publisher = {{JMLR.org}},
  series = {{{ICML}}'15}
}

@article{kang_new_2016,
  title = {A New Feature-Enhanced Speckle Reduction Method Based on Multiscale Analysis for Ultrasound {{B}}-Mode Imaging},
  author = {Kang, Jinbum and Lee, Jae Young and Yoo, Yangmo},
  year = {2016},
  month = jun,
  volume = {63},
  pages = {1178--1191},
  journal = {IEEE Trans. Biomed. Eng.},
  number = {6}
}

@article{kaplanberkaya_survey_2018,
  title = {A Survey on {{ECG}} Analysis},
  author = {Kaplan Berkaya, Selcan and Uysal, Alper Kursat and Sora Gunal, Efnan and Ergin, Semih and Gunal, Serkan and Gulmezoglu, M. Bilginer},
  year = {2018},
  month = may,
  volume = {43},
  pages = {216--235},
  journal = {Biomedical Signal Processing and Control},
  language = {en}
}

@article{karagiannis_annealed_2013,
  title = {Annealed {{Importance Sampling Reversible Jump MCMC Algorithms}}},
  author = {Karagiannis, Georgios and Andrieu, Christophe},
  year = {2013},
  month = jul,
  volume = {22},
  pages = {623--648},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {3}
}

@article{karnopp_random_1963,
  title = {Random Search Techniques for Optimization Problems},
  author = {Karnopp, Dean C.},
  year = {1963},
  month = aug,
  volume = {1},
  pages = {111--121},
  journal = {Automatica},
  language = {en},
  number = {2-3}
}

@article{keith_adaptive_2008,
  title = {Adaptive Independence Samplers},
  author = {Keith, Jonathan M. and Kroese, Dirk P. and Sofronov, George Y.},
  year = {2008},
  month = dec,
  volume = {18},
  pages = {409--420},
  file = {/home/msca8h/Zotero/storage/HS8SJGPP/Keith et al. - 2008 - Adaptive independence samplers.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {4}
}

@incollection{kejariwal_efficient_2006,
  title = {An Efficient Approach for Self-Scheduling Parallel Loops on Multiprogrammed Parallel Computers},
  booktitle = {Languages and {{Compilers}} for {{Parallel Computing}}},
  author = {Kejariwal, Arun and Nicolau, Alexandru and Polychronopoulos, Constantine D.},
  year = {2006},
  volume = {4339},
  pages = {441--449},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Zotero/storage/96DDIPPN/Kejariwal et al. - 2006 - An Efficient Approach for Self-scheduling Parallel.pdf}
}

@inproceedings{kejariwal_historyaware_2006,
  title = {History-Aware Self-Scheduling},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Parallel Process}}.},
  author = {Kejariwal, A. and Nicolau, A. and Polychronopoulos, C. D.},
  year = {2006},
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Kejariwal et al. - 2006 - History-aware Self-Scheduling.pdf},
  series = {{{ICPP}}'06}
}

@article{kennedy_acceptances_1991,
  title = {Acceptances and Autocorrelations in Hybrid {{Monte Carlo}}},
  author = {Kennedy, A.D. and Pendleton, Brian},
  year = {1991},
  month = may,
  volume = {20},
  pages = {118--121},
  journal = {Nuclear Physics B - Proceedings Supplements},
  language = {en}
}

@article{kennedy_acceptances_1991a,
  title = {Acceptances and Autocorrelations in Hybrid {{Monte Carlo}}},
  author = {Kennedy, A.D. and Pendleton, Brian},
  year = {1991},
  month = may,
  volume = {20},
  pages = {118--121},
  journal = {Nuclear Physics B - Proceedings Supplements},
  language = {en}
}

@incollection{kermarrec_evaluating_2008,
  title = {Evaluating the {{Quality}} of a {{Network Topology}} through {{Random Walks}}},
  booktitle = {Distributed {{Computing}}},
  author = {Kermarrec, Anne-Marie and Le Merrer, Erwan and Sericola, Bruno and Tr{\'e}dan, Gilles},
  year = {2008},
  volume = {5218},
  pages = {509--511},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}}
}

@article{khan_switchable_2020,
  title = {Switchable {{Deep Beamformer}}},
  author = {Khan, Shujaat and Huh, Jaeyoung and Ye, Jong Chul},
  year = {2020},
  month = sep,
  abstract = {Recent proposals of deep beamformers using deep neural networks have attracted significant attention as computational efficient alternatives to adaptive and compressive beamformers. Moreover, deep beamformers are versatile in that image post-processing algorithms can be combined with the beamforming. Unfortunately, in the current technology, a separate beamformer should be trained and stored for each application, demanding significant scanner resources. To address this problem, here we propose a \{\textbackslash em switchable\} deep beamformer that can produce various types of output such as DAS, speckle removal, deconvolution, etc., using a single network with a simple switch. In particular, the switch is implemented through Adaptive Instance Normalization (AdaIN) layers, so that various output can be generated by merely changing the AdaIN code. Experimental results using B-mode focused ultrasound confirm the flexibility and efficacy of the proposed methods for various applications.},
  archiveprefix = {arXiv},
  eprint = {2008.13646},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/YIB7XIR8/Khan et al. - 2020 - Switchable Deep Beamformer.pdf;/home/msca8h/Zotero/storage/DKP47Y39/2008.html},
  journal = {ArXiv200813646 Cs Eess Stat},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  primaryclass = {cs, eess, stat}
}

@inproceedings{khatami_hpx_2017,
  title = {{{HPX}} Smart Executors},
  booktitle = {Proc. 3rd {{Int}}. {{Workshop Extreme Scale Program}}. {{Models Middleware}}},
  author = {Khatami, Z. and Troska, L. and Kaiser, H. and Ramanujam, J. and Serio, A.},
  year = {2017},
  abstract = {The performance of many parallel applications depends on loop-level parallelism. However, manually parallelizing all loops may result in degrading parallel performance, as some of them cannot scale desirably to a large number of threads. In addition, the overheads of manually tuning loop parameters might prevent an application from reaching its maximum parallel performance. We illustrate how machine learning techniques can be applied to address these challenges. In this research, we develop a framework that is able to automatically capture the static and dynamic information of a loop. Moreover, we advocate a novel method by introducing HPX smart executors for determining the execution policy, chunk size, and prefetching distance of an HPX loop to achieve higher possible performance by feeding static information captured during compilation and runtime-based dynamic information to our learning model. Our evaluated execution results show that using these smart executors can speed up the HPX execution process by around 12\%-35\% for the Matrix Multiplication, Stream and \$2D\$ Stencil benchmarks compared to setting their HPX loop's execution policy/parameters manually or using HPX auto-parallelization techniques.},
  file = {/home/msca8h/Documents/parallel_scheduling/HPX_smart_executors.pdf;/home/msca8h/Zotero/storage/3S6Z2YCI/HPX_smart_executors.pdf},
  series = {{{ESPM2}}'17}
}

@article{khu-raikim_evaluating_2020,
  title = {Evaluating the {{Strong Scalability}} of {{Parallel Markov}}-{{Chain Monte Carlo Algorithms}}},
  author = {{Khu-Rai Kim} and Maskell, Simon and Sungyong Park},
  year = {2020},
  publisher = {{Unpublished}},
  language = {en}
}

@inproceedings{kim_robust_2019,
  title = {Towards {{Robust Data}}-{{Driven Parallel Loop Scheduling Using Bayesian Optimization}}},
  booktitle = {2019 {{IEEE}} 27th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  author = {Kim, Khu-rai and Kim, Youngjae and Park, Sungyong},
  year = {2019},
  month = oct,
  pages = {241--248},
  publisher = {{IEEE}},
  address = {{Rennes, FR}}
}

@inproceedings{kim_robust_2019a,
  title = {Towards Robust Data-Driven Parallel Loop Scheduling Using {{Bayesian}} Optimization},
  booktitle = {{{IEEE}} 27th {{Int}}. {{Symp}}. {{Model}}., {{Anal}}. {{Simul}}. {{Comput}}. {{Telecommun}}. {{Syst}}.},
  author = {Kim, Khu-rai and Kim, Youngjae and Park, Sungyong},
  year = {2019},
  pages = {241--248},
  publisher = {{IEEE}},
  address = {{Rennes, FR}}
}

@article{kim_stochastic_1998,
  title = {Stochastic {{Volatility}}: {{Likelihood Inference}} and {{Comparison}} with {{ARCH Models}}},
  shorttitle = {Stochastic {{Volatility}}},
  author = {Kim, Sangjoon and Shepherd, Neil and Chib, Siddhartha},
  year = {1998},
  month = jul,
  volume = {65},
  pages = {361--393},
  journal = {Rev. Econ. Stud.},
  language = {en},
  number = {3}
}

@inproceedings{kim2021adaptive,
  title = {Adaptive Strategy for Resetting a Non-Stationary Markov Chain during Learning via Joint Stochastic Approximation},
  booktitle = {Proceedings of the 3rd Symposium on Advances in Approximate Bayesian, to {{Appear}}},
  author = {Kim, Hyunsu and Lee, Juho and Yang, Hongseok},
  year = {2021}
}

@inproceedings{kingma_adam_2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {Proceedings of the 3rd {{International Conference}} for {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2015},
  address = {{San Diego, California, USA}},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/ICB6B4IV/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/msca8h/Zotero/storage/REWW72SC/1412.html},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{klambauer_selfnormalizing_2017,
  title = {Self-{{Normalizing Neural Networks}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  pages = {972--981},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  series = {{{NIPS}}'17}
}

@article{klein_fast_2017,
  title = {Fast {{Bayesian}} Hyperparameter Optimization on Large Datasets},
  author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  year = {2017},
  volume = {11},
  pages = {4945--4968},
  file = {/home/msca8h/Zotero/storage/DAEJNAYW/Klein et al. - 2017 - Fast Bayesian hyperparameter optimization on large.pdf},
  journal = {Electron. J. Statist.},
  language = {en},
  number = {2}
}

@inproceedings{klein_robo_2017,
  title = {{{RoBO}}: {{A}} Flexible and Robust {{Bayesian}} Optimization Framework in {{Python}}},
  booktitle = {{{NIPS}} 2017 {{Bayesian Optimization Workshop}}},
  author = {Klein, Aaron and Falkner, Stefan and Mansur, Numair and Hutter, Frank},
  year = {2017}
}

@incollection{klug_autopin_2011,
  title = {Autopin \textendash{} {{Automated Optimization}} of {{Thread}}-to-{{Core Pinning}} on {{Multicore Systems}}},
  booktitle = {Transactions on {{High}}-{{Performance Embedded Architectures}} and {{Compilers III}}},
  author = {Klug, Tobias and Ott, Michael and Weidendorfer, Josef and Trinitis, Carsten},
  year = {2011},
  volume = {6590},
  pages = {219--235},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}}
}

@article{koblents_population_2015,
  title = {A Population {{Monte Carlo}} Scheme with Transformed Weights and Its Application to Stochastic Kinetic Models},
  author = {Koblents, Eugenia and M{\'i}guez, Joaqu{\'i}n},
  year = {2015},
  month = mar,
  volume = {25},
  pages = {407--425},
  file = {/home/msca8h/Zotero/storage/SI34WAQ6/Koblents and Míguez - 2015 - A population Monte Carlo scheme with transformed w.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@inproceedings{kochanski_bayesian_2017,
  title = {Bayesian Optimization for a Better Dessert},
  booktitle = {Proceedings of the {{NIPS Workshop}} on {{Bayesian Optimization}} ({{BayesOpt}}'17)},
  author = {Kochanski, Greg and Golovin, Daniel and Karro, John and Solnik, Benjamin and Moitra, Subhodeep and Sculley, D},
  year = {2017},
  file = {/home/msca8h/Documents/bayesian_optimization/Kochanski et al. - 2017 - Bayesian Optimization for a Better Dessert.pdf}
}

@techreport{kong_note_1992,
  title = {A Note on Importance Sampling Using Standardized Weights},
  author = {Kong, Augustine},
  year = {1992},
  month = jul,
  institution = {{Department of Statistics, University of Chicago}},
  number = {348},
  type = {Technical {{Report}}}
}

@article{kong_sequential_1994,
  title = {Sequential {{Imputations}} and {{Bayesian Missing Data Problems}}},
  author = {Kong, Augustine and Liu, Jun S. and Wong, Wing Hung},
  year = {1994},
  month = mar,
  volume = {89},
  pages = {278--288},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {425}
}

@article{kovesi_image_1999,
  title = {Image Features from Phase Congruency},
  author = {Kovesi, Peter},
  year = {1999},
  volume = {1},
  pages = {1--26},
  journal = {J. Comput. Vis. Res.},
  number = {3}
}

@book{koyama_computational_2018,
  title = {Computational {{Design}} with {{Crowds}}},
  author = {Koyama, Yuki and Igarashi, Takeo},
  year = {2018},
  month = mar,
  volume = {1},
  publisher = {{Oxford University Press}},
  abstract = {Computational design is aimed at supporting automating design processes using computational techniques. However, some classes of design tasks involve criteria that are difficult to handle only with computers. For example, visual design tasks seeking to fulfil aesthetic goals are difficult to handle purely with computers. One promising approach is to leverage human computation; that is, to incorporate human input into the computation process. Crowdsourcing platforms provide a convenient way to integrate such human computation into a working system. In this chapter, we discuss such computational design with crowds in the domain of parameter tweaking tasks in visual design. Parameter tweaking is often performed to maximize the aesthetic quality of designed objects. Computational design powered by crowds can solve this maximization problem by leveraging human computation. We discuss the opportunities and challenges of computational design with crowds with two illustrative examples: (1) estimating the objective function to facilitate the design exploration by a designer and (2) directly searching for the optimal parameter setting that maximizes the objective function.},
  file = {/home/msca8h/Zotero/storage/EUB5G7JJ/Koyama and Igarashi - 2018 - Computational Design with Crowds.pdf},
  language = {en}
}

@article{koyama_sequential_2020,
  title = {Sequential Gallery for Interactive Visual Design Optimization},
  author = {Koyama, Yuki and Sato, Issei and Goto, Masataka},
  year = {2020},
  month = jul,
  volume = {39},
  pages = {88:1-88:12},
  file = {/home/msca8h/Zotero/storage/ZV72YBV3/Koyama et al. - 2020 - Sequential gallery for interactive visual design o.pdf},
  journal = {ACM Trans. Graph.},
  language = {en},
  number = {4},
  series = {{{SIGGRAPH}}'20}
}

@article{krissian_oriented_2007,
  title = {Oriented Speckle Reducing Anisotropic Diffusion},
  author = {Krissian, Karl and Westin, Carl-Fredrik and Kikinis, Ron and Vosburgh, Kirby G.},
  year = {2007},
  month = may,
  volume = {16},
  pages = {1412--1424},
  journal = {IEEE Trans. Image Process.},
  number = {5}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  series = {{{NIPS}}'12}
}

@article{kruskal_allocating_1985,
  title = {Allocating Independent Subtasks on Parallel Processors},
  author = {Kruskal, C. P. and Weiss, A.},
  year = {1985},
  month = oct,
  volume = {SE-11},
  pages = {1001--1016},
  file = {/home/msca8h/Documents/parallel_scheduling/Kruskal and Weiss - 1985 - Allocating Independent Subtasks on Parallel Proces.pdf},
  journal = {IEEE Trans. Softw. Eng.},
  number = {10}
}

@inproceedings{kumar_overview_2016,
  title = {An Overview of Modern Cache Memory and Performance Analysis of Replacement Policies},
  booktitle = {2016 {{IEEE International Conference}} on {{Engineering}} and {{Technology}} ({{ICETECH}})},
  author = {Kumar, S. and Singh, P. K.},
  year = {2016},
  month = mar,
  pages = {210--214},
  keywords = {associative cache,benchmark testing,cache memory,Cache memory,Cache Performance,cache replacement policy,cache storage,Clocks,Conferences,current generation processors,design parameter,energy consumption,FIFO,hard disk,Hardware,hit rate,Hit rate,LFU,LRU,memory architecture,memory hierarchy,microprocessor chips,Miss rate,Multicore processing,Organizations,performance analysis,performance evaluation,performance improvement,processor performance,RANDOM,registers,Registers,Replacement Policy,virtual memory,virtual storage}
}

@inproceedings{kurth_exascale_2018,
  title = {Exascale Deep Learning for Climate Analytics},
  booktitle = {Proc.  {{Int}}. {{Conf}}. {{High Perform}}. {{Comput}}. {{Networking}}, {{Storage}}, {{Anal}}.},
  author = {Kurth, Thorsten and Treichler, Sean and Romero, Joshua and Mudigonda, Mayur and Luehr, Nathan and Phillips, Everett and Mahesh, Ankur and Matheson, Michael and Deslippe, Jack and Fatica, Massimiliano and {al}, et},
  year = {2018},
  series = {{{SC}} '18}
}

@article{kurzak_autotuning_2012,
  title = {Autotuning {{GEMM Kernels}} for the {{Fermi GPU}}},
  author = {Kurzak, Jakub and Tomov, Stanimire and Dongarra, Jack},
  year = {2012},
  month = nov,
  volume = {23},
  pages = {2045--2057},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {11}
}

@inproceedings{kurzak_massively_2019,
  title = {Massively {{Parallel Automated Software Tuning}}},
  booktitle = {Proceedings of the 48th {{International Conference}} on {{Parallel Processing}}  - {{ICPP}} 2019},
  author = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
  year = {2019},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  language = {en}
}

@inproceedings{kurzak_massively_2019a,
  title = {Massively {{Parallel Automated Software Tuning}}},
  booktitle = {Proceedings of the 48th {{International Conference}} on {{Parallel Processing}}  - {{ICPP}} 2019},
  author = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
  year = {2019},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  language = {en}
}

@inproceedings{laberge_scheduling_2019,
  title = {Scheduling {{Optimization}} of {{Parallel Linear Algebra Algorithms Using Supervised Learning}}},
  booktitle = {2019 {{IEEE}}/{{ACM Workshop}} on {{Machine Learning}} in {{High Performance Computing Environments}} ({{MLHPC}})},
  author = {{laberge}, gabriel and Shirzad, Shahrzad and Diehl, Patrick and Kaiser, Hartmut and Prudhomme, Serge and Lemoine, Adrian S.},
  year = {2019},
  month = nov,
  pages = {31--43},
  publisher = {{IEEE}},
  address = {{Denver, CO, USA}},
  file = {/home/msca8h/Zotero/storage/28R5IL4D/laberge et al. - 2019 - Scheduling Optimization of Parallel Linear Algebra.pdf}
}

@inproceedings{laberge_scheduling_2019a,
  title = {Scheduling Optimization of Parallel Linear Algebra Algorithms Using Supervised Learning},
  booktitle = {{{IEEE}}/{{ACM Workshop Mach}}. {{Learn}}. {{High Perform}}. {{Comput}}. {{Environ}}.},
  author = {Laberge, Gabriel and Shirzad, Shahrzad and Diehl, Patrick and Kaiser, Hartmut and Prudhomme, Serge and Lemoine, Adrian S.},
  year = {2019},
  month = nov,
  pages = {31--43},
  publisher = {{IEEE}},
  address = {{Denver, CO, USA}},
  file = {/home/msca8h/Zotero/storage/7IEGGCK8/laberge et al. - 2019 - Scheduling Optimization of Parallel Linear Algebra.pdf},
  series = {{{MLHPC}}'19}
}

@inproceedings{lafond_dfw_2016,
  title = {D-{{FW}}: {{Communication}} Efficient Distributed Algorithms for High-Dimensional Sparse Optimization},
  shorttitle = {D-{{FW}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lafond, Jean and Wai, Hoi-To and Moulines, Eric},
  year = {2016},
  month = mar,
  pages = {4144--4148},
  publisher = {{IEEE}},
  address = {{Shanghai}},
  file = {/home/msca8h/Zotero/storage/5TZAVVFE/Lafond et al. - 2016 - D-FW Communication efficient distributed algorith.pdf}
}

@article{lamberti_independent_2017,
  title = {Independent {{Resampling Sequential Monte Carlo Algorithms}}},
  author = {Lamberti, Roland and Petetin, Yohan and Desbouvries, Francois and Septier, Francois},
  year = {2017},
  month = oct,
  volume = {65},
  pages = {5318--5333},
  file = {/home/msca8h/Zotero/storage/79VIR5YR/lamberti2017.pdf;/home/msca8h/Zotero/storage/ICAIAK26/Lamberti et al. - 2017 - Independent Resampling Sequential Monte Carlo Algo.pdf},
  journal = {IEEE Trans. Signal Process.},
  number = {20}
}

@article{lamberti_semiindependent_2018,
  title = {Semi-{{Independent Resampling}} for {{Particle Filtering}}},
  author = {Lamberti, Roland and Petetin, Yohan and Desbouvries, Francois and Septier, Francois},
  year = {2018},
  month = jan,
  volume = {25},
  pages = {130--134},
  file = {/home/msca8h/Zotero/storage/3U63MQ2P/Lamberti et al. - 2018 - Semi-Independent Resampling for Particle Filtering.pdf;/home/msca8h/Zotero/storage/5UCMWPG8/lamberti2018.pdf},
  journal = {IEEE Signal Process. Lett.},
  number = {1}
}

@article{lartillot_computing_2006,
  title = {Computing {{Bayes}} Factors Using Thermodynamic Integration},
  author = {Lartillot, Nicolas and Philippe, Herv{\'e}},
  year = {2006},
  month = apr,
  volume = {55},
  pages = {195--207},
  file = {/home/msca8h/Zotero/storage/HS3B5PUZ/Lartillot and Philippe - 2006 - Computing Bayes Factors Using Thermodynamic Integr.pdf},
  journal = {Syst. Biol.},
  language = {en},
  number = {2}
}

@article{leclerc_deep_2019,
  title = {Deep Learning for Segmentation Using an Open Large-Scale Dataset in {{2D}} Echocardiography},
  author = {Leclerc, Sarah and Smistad, Erik and Pedrosa, Joao and Ostvik, Andreas and Cervenansky, Frederic and Espinosa, Florian and Espeland, Torvald and Berg, Erik Andreas Rye and Jodoin, Pierre-Marc and Grenier, Thomas and Lartizien, Carole and Dhooge, Jan and Lovstakken, Lasse and Bernard, Olivier},
  year = {2019},
  month = sep,
  volume = {38},
  pages = {2198--2210},
  file = {/home/msca8h/Zotero/storage/A43RDL35/Leclerc et al. - 2019 - Deep Learning for Segmentation Using an Open Large.pdf},
  journal = {IEEE Trans. Med. Imaging},
  number = {9}
}

@incollection{lecun_efficient_2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  volume = {7700},
  pages = {9--48},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  language = {en}
}

@article{ledoit_wellconditioned_2004,
  title = {A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices},
  author = {Ledoit, Olivier and Wolf, Michael},
  year = {2004},
  month = feb,
  volume = {88},
  pages = {365--411},
  file = {/home/msca8h/Zotero/storage/CLJVPUNC/Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf},
  journal = {Journal of Multivariate Analysis},
  language = {en},
  number = {2}
}

@article{lee_distributed_2017,
  title = {Distributed {{Stochastic Variance Reduced Gradient Methods}} by {{Sampling Extra Data}} with {{Replacement}}},
  author = {Lee, Jason D. and Lin, Qihang and Ma, Tengyu and Yang, Tianbao},
  year = {2017},
  volume = {18},
  pages = {1--43},
  file = {/home/msca8h/Zotero/storage/DTSH3QKY/Lee et al. - 2017 - Distributed Stochastic Variance Reduced Gradient M.pdf},
  journal = {J. Mach. Learn. Res.},
  number = {122}
}

@inproceedings{lee_petascale_2013,
  title = {Petascale Direct Numerical Simulation of Turbulent Channel Flow on up to {{786K}} Cores},
  booktitle = {{{SC}} '13: {{Proceedings}} of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Lee, M. and Malaya, N. and Moser, R. D.},
  year = {2013},
  month = nov,
  pages = {1--11},
  keywords = {Abstracts,Benchmark testing,boundary layer turbulence,channel flow,computational fluid dynamics,Data transpose,DNS,Equations,external flows,flow simulation,fluid flow equations,grid modeling,high Reynolds number,Mathematical model,Mechanical engineering,MPI alltoall,numerical analysis,Numerical simulation,Parallel FFT,performance optimization,Petascale,petascale direct numerical simulation,Turbulence,turbulent flow over walls,Vectors,wall bounded turbulent flow}
}

@article{lee_utility_2010,
  title = {On the Utility of Graphics Cards to Perform Massively Parallel Simulation of Advanced {{Monte Carlo}} Methods},
  author = {Lee, Anthony and Yau, Christopher and Giles, Michael B. and Doucet, Arnaud and Holmes, Christopher C.},
  year = {2010},
  month = jan,
  volume = {19},
  pages = {769--789},
  file = {/home/msca8h/Zotero/storage/GEUTUAWS/Lee et al. - 2010 - On the Utility of Graphics Cards to Perform Massiv.pdf},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {4}
}

@article{leimkuhler_ensemble_2018,
  title = {Ensemble Preconditioning for {{Markov}} Chain {{Monte Carlo}} Simulation},
  author = {Leimkuhler, Benedict and Matthews, Charles and Weare, Jonathan},
  year = {2018},
  month = mar,
  volume = {28},
  pages = {277--290},
  file = {/home/msca8h/Zotero/storage/JEH6GIMV/Leimkuhler et al. - 2018 - Ensemble preconditioning for Markov chain Monte Ca.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@article{leimkuhler_ensemble_2018a,
  title = {Ensemble Preconditioning for {{Markov}} Chain {{Monte Carlo}} Simulation},
  author = {Leimkuhler, Benedict and Matthews, Charles and Weare, Jonathan},
  year = {2018},
  month = mar,
  volume = {28},
  pages = {277--290},
  file = {/home/msca8h/Zotero/storage/QVH4ZTA5/Leimkuhler et al. - 2018 - Ensemble preconditioning for Markov chain Monte Ca.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@inproceedings{leskovec_graphs_2005,
  title = {Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations},
  booktitle = {Proc. 11th {{ACM SIGKDD Int}}. {{Conf}}. {{Knowl}}. {{Discovery Data Mining}}},
  author = {Leskovec, Jure and Kleinberg, Jon and Faloutsos, Christos},
  year = {2005},
  pages = {177--187},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {densification power laws,graph generators,graph mining,heavy-tailed distributions,small-world phenomena},
  series = {{{KDD}} '05}
}

@article{letham_constrained_2018,
  title = {Constrained {{Bayesian}} Optimization with Noisy Experiments},
  author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
  year = {2018},
  month = aug,
  volume = {14},
  pages = {495--519},
  file = {/home/msca8h/Documents/bayesian_optimization/Letham et al. - 2017 - Constrained Bayesian Optimization with Noisy Exper.pdf},
  journal = {Bayesian Anal.},
  number = {2}
}

@article{letham_reexamining_2020,
  title = {Re-{{Examining Linear Embeddings}} for {{High}}-{{Dimensional Bayesian Optimization}}},
  author = {Letham, Benjamin and Calandra, Roberto and Rai, Akshara and Bakshy, Eytan},
  year = {2020},
  month = oct,
  abstract = {Bayesian optimization (BO) is a popular approach to optimize expensive-to-evaluate black-box functions. A significant challenge in BO is to scale to high-dimensional parameter spaces while retaining sample efficiency. A solution considered in existing literature is to embed the high-dimensional space in a lower-dimensional manifold, often via a random linear embedding. In this paper, we identify several crucial issues and misconceptions about the use of linear embeddings for BO. We study the properties of linear embeddings from the literature and show that some of the design choices in current approaches adversely impact their performance. We show empirically that properly addressing these issues significantly improves the efficacy of linear embeddings for BO on a range of problems, including learning a gait policy for robot locomotion.},
  archiveprefix = {arXiv},
  eprint = {2001.11659},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/3UPVUJK8/Letham et al. - 2020 - Re-Examining Linear Embeddings for High-Dimensiona.pdf;/home/msca8h/Zotero/storage/Q7JVMEYJ/2001.html},
  journal = {Adv. NIPS},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{lethunguyen_improving_2014,
  title = {Improving {{SMC}} Sampler Estimate by Recycling All Past Simulated Particles},
  booktitle = {2014 {{IEEE Workshop}} on {{Statistical Signal Processing}} ({{SSP}})},
  author = {Le Thu Nguyen, Thi and Septier, Francois and Peters, Gareth W. and Delignon, Yves},
  year = {2014},
  month = jun,
  pages = {117--120},
  publisher = {{IEEE}},
  address = {{Gold Coast, Australia}}
}

@article{li_approximate_2017,
  title = {Approximate Inference with Amortised {{MCMC}}},
  author = {Li, Yingzhen and Turner, Richard E. and Liu, Qiang},
  year = {2017},
  month = may,
  abstract = {We propose a novel approximate inference algorithm that approximates a target distribution by amortising the dynamics of a user-selected MCMC sampler. The idea is to initialise MCMC using samples from an approximation network, apply the MCMC operator to improve these samples, and finally use the samples to update the approximation network thereby improving its quality. This provides a new generic framework for approximate inference, allowing us to deploy highly complex, or implicitly defined approximation families with intractable densities, including approximations produced by warping a source of randomness through a deep neural network. Experiments consider image modelling with deep generative models as a challenging test for the method. Deep models trained using amortised MCMC are shown to generate realistic looking samples as well as producing diverse imputations for images with regions of missing pixels.},
  archiveprefix = {arXiv},
  eprint = {1702.08343},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/YMZ2XQ9Q/Li et al. - 2017 - Approximate Inference with Amortised MCMC.pdf;/home/msca8h/Zotero/storage/YBE7ZLFP/1702.html},
  journal = {ArXiv170208343 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@incollection{li_communication_2014,
  title = {Communication {{Efficient Distributed Machine Learning}} with the {{Parameter Server}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Li, Mu and Andersen, David G and Smola, Alexander J and Yu, Kai},
  year = {2014},
  pages = {19--27},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/msca8h/Zotero/storage/BI58R6BL/Li et al. - 2014 - Communication Efficient Distributed Machine Learni.pdf}
}

@inproceedings{li_locality_1993,
  title = {Locality and Loop Scheduling on {{NUMA}} Multiprocessors},
  booktitle = {1993 {{International Conference}} on {{Parallel Processing}} - {{ICPP}}\textbackslash textquotesingle93 {{Vol2}}},
  author = {Li, Hui and Tandri, Sudarsan and Stumm, Michael and Sevcik, Kenneth C.},
  year = {1993},
  month = aug,
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Li et al. - 1993 - Locality and Loop Scheduling on NUMA Multiprocesso.pdf}
}

@inproceedings{li_machine_2009,
  title = {Machine Learning Based Online Performance Prediction for Runtime Parallelization and Task Scheduling},
  booktitle = {2009 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}}},
  author = {Li, J. and Ma, X. and Singh, K. and Schulz, M. and de Supinski, B. R. and McKee, S. A.},
  year = {2009},
  month = apr,
  pages = {89--100},
  file = {/home/msca8h/Documents/parallel_scheduling/Li et al. - 2009 - Machine learning based online performance predicti.pdf},
  keywords = {Application software,Artificial Neural Networks,authoring languages,automatic performance prediction,Automatic Task Scheduling,Costs,Hardware,learning (artificial intelligence),Load management,machine learning,Machine learning,next-generation software,online performance prediction,online task partitioning,parallel programming,Parallel programming,Performance Prediction,pR framework,Predictive models,program execution,R language,Runtime,runtime parallelization,scheduling,Scheduling algorithm,scripting language,Scripting Languages,task analysis,task cost estimates,task scheduling,Testing}
}

@inproceedings{li_metis_2018,
  title = {Metis: {{Robustly Tuning Tail Latencies}} of {{Cloud Systems}}},
  booktitle = {2018 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 18)},
  author = {Li, Zhao Lucis and Liang, Chieh-Jan Mike and He, Wenjia and Zhu, Lianjie and Dai, Wenjun and Jiang, Jin and Sun, Guangzhong},
  year = {2018},
  month = jul,
  pages = {981--992},
  publisher = {{USENIX Association}},
  address = {{Boston, MA}}
}

@article{li_performance_2015,
  title = {Performance {{Analysis}} and {{Optimization}} for {{SpMV}} on {{GPU Using Probabilistic Modeling}}},
  author = {Li, Kenli and Yang, Wangdong and Li, Keqin},
  year = {2015},
  month = jan,
  volume = {26},
  pages = {196--205},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {1}
}

@inproceedings{li_pipesgd_2018,
  title = {Pipe-{{SGD}}: {{A Decentralized Pipelined SGD Framework}} for {{Distributed Deep Net Training}}},
  booktitle = {Proceedings of the {{32Nd International Conference}} on {{Neural Information Processing Systems}}},
  author = {Li, Youjie and Yu, Mingchao and Li, Songze and Avestimehr, Salman and Kim, Nam Sung and Schwing, Alexander},
  year = {2018},
  pages = {8056--8067},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  file = {/home/msca8h/Zotero/storage/C5KDQVMS/Li et al. - 2018 - Pipe-SGD A Decentralized Pipelined SGD Framework .pdf},
  series = {{{NIPS}}'18}
}

@article{li2018feature,
  title = {Feature Selection: {{A}} Data Perspective},
  author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P and Tang, Jiliang and Liu, Huan},
  year = {2018},
  volume = {50},
  pages = {94},
  publisher = {{ACM}},
  journal = {ACM Comput. Surveys},
  number = {6}
}

@incollection{lian_asynchronous_2015,
  title = {Asynchronous {{Parallel Stochastic Gradient}} for {{Nonconvex Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  year = {2015},
  pages = {2737--2745},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{lian_can_2017,
  title = {Can {{Decentralized Algorithms Outperform Centralized Algorithms}}? {{A Case Study}} for {{Decentralized Parallel Stochastic Gradient Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  year = {2017},
  pages = {5330--5340},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{liao_fast_2006,
  title = {Fast and {{Adaptive Low}}-{{Pass Whitening Filters}} for {{Natural Images}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Neural Information Processing}} - {{Volume Part II}}},
  author = {Liao, Ling-Zhi and Luo, Si-Wei and Tian, Mei and Zhao, Lian-Wei},
  year = {2006},
  pages = {343--352},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  series = {{{ICONIP}}'06}
}

@article{liao_sharpening_2019,
  title = {Sharpening {{Jensen}}'s Inequality},
  author = {Liao, J. G. and Berg, Arthur},
  year = {2019},
  month = jul,
  volume = {73},
  pages = {278--281},
  file = {/home/msca8h/Zotero/storage/42BDW3KX/Liao and Berg - 2019 - Sharpening Jensen's Inequality.pdf},
  journal = {The American Statistician},
  language = {en},
  number = {3}
}

@article{lift-tensorforce,
  title = {{{LIFT}}: {{Reinforcement}} Learning in Computer Systems by Learning from Demonstrations},
  author = {Schaarschmidt, Michael and Kuhnle, Alexander and Ellis, Ben and Fricke, Kai and Gessert, Felix and Yoneki, Eiko},
  year = {2018},
  volume = {abs/1808.07903},
  archiveprefix = {arXiv},
  eprint = {1808.07903},
  eprinttype = {arxiv},
  journal = {CoRR}
}

@article{lilja_dynamic_2000,
  title = {Dynamic Task Scheduling Using Online Optimization},
  author = {Lilja, D.J. and {Lau Ying Kit} and Hamidzadeh, B.},
  year = {Nov./2000},
  volume = {11},
  pages = {1151--1163},
  file = {/home/msca8h/Documents/parallel_scheduling/Lilja et al. - 2000 - Dynamic task scheduling using online optimization.pdf},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {11}
}

@inproceedings{lim_autotuning_2017,
  title = {Autotuning {{GPU Kernels}} via {{Static}} and {{Predictive Analysis}}},
  booktitle = {2017 46th {{International Conference}} on {{Parallel Processing}} ({{ICPP}})},
  author = {Lim, Robert and Norris, Boyana and Malony, Allen},
  year = {2017},
  month = aug,
  pages = {523--532},
  publisher = {{IEEE}},
  address = {{Bristol, United Kingdom}},
  file = {/home/msca8h/Zotero/storage/6MMUNZHQ/Lim et al. - 2017 - Autotuning GPU Kernels via Static and Predictive A.pdf}
}

@inproceedings{lin_deep_2018,
  title = {Deep {{Gradient Compression}}: {{Reducing}} the {{Communication Bandwidth}} for {{Distributed Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, Bill},
  year = {2018}
}

@article{lin_don_2018,
  title = {Don't {{Use Large Mini}}-{{Batches}}, {{Use Local SGD}}},
  author = {Lin, Tao and Stich, Sebastian U. and Patel, Kumar Kshitij and Jaggi, Martin},
  year = {2018},
  month = aug,
  abstract = {Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. Drastic increases in the mini-batch sizes have lead to key efficiency and scalability gains in recent years. However, progress faces a major roadblock, as models trained with large batches often do not generalize well. Local SGD can offer the same communication vs. computation pattern as mini-batch SGD---thus is as efficient as mini-batch SGD from a systems perspective---but instead of performing a single large-batch update in each round, it performs several local parameter updates sequentially. We extensively study the communication efficiency vs. performance trade-offs associated with local SGD and provide a new variant, called \textbackslash emph\{post-local SGD\}. We show that it significantly improves the generalization performance compared to large-batch training and converges to flatter minima.},
  archiveprefix = {arXiv},
  eprint = {1808.07217},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/2FMPE9SP/Lin et al. - 2018 - Don't Use Large Mini-Batches, Use Local SGD.pdf;/home/msca8h/Zotero/storage/M8RMGBTP/1808.html},
  journal = {ArXiv180807217 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{liu_autotuning_2019,
  title = {An {{Autotuning Protocol}} to {{Rapidly Build Autotuners}}},
  author = {Liu, Junhong and Tan, Guangming and Luo, Yulong and Li, Jiajia and Mo, Zeyao and Sun, Ninghui},
  year = {2019},
  month = jan,
  volume = {5},
  pages = {1--25},
  journal = {ACM Trans. Parallel Comput.},
  language = {en},
  number = {2}
}

@inproceedings{liu_blackbox_2016,
  title = {Black-Box {{Importance Sampling}}},
  booktitle = {The 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}, {{AISTATS}}},
  author = {Liu, Qiang and Lee, Jason D.},
  year = {2016},
  month = oct,
  abstract = {Importance sampling is widely used in machine learning and statistics, but its power is limited by the restriction of using simple proposals for which the importance weights can be tractably calculated. We address this problem by studying black-box importance sampling methods that calculate importance weights for samples generated from any unknown proposal or black-box mechanism. Our method allows us to use better and richer proposals to solve difficult problems, and (somewhat counter-intuitively) also has the additional benefit of improving the estimation accuracy beyond typical importance sampling. Both theoretical and empirical analyses are provided.},
  archiveprefix = {arXiv},
  eprint = {1610.05247},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/326KUNX3/Liu and Lee - 2016 - Black-box Importance Sampling.pdf;/home/msca8h/Zotero/storage/FSWKHSFB/1610.html},
  keywords = {Statistics - Machine Learning}
}

@inproceedings{liu_differentiable_2019,
  title = {Differentiable {{Kernel Evolution}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Yu and Liu, Jihao and Wang, Xiaogang and Zeng, Ailing},
  year = {2019},
  month = oct,
  pages = {1834--1843},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}}
}

@article{liu_evolving_2020,
  title = {Evolving {{Normalization}}-{{Activation Layers}}},
  author = {Liu, Hanxiao and Brock, Andrew and Simonyan, Karen and Le, Quoc V.},
  year = {2020},
  month = apr,
  abstract = {Normalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing them separately, we unify them into a single computation graph, and evolve its structure starting from low-level primitives. Our layer search algorithm leads to the discovery of EvoNorms, a set of new normalization-activation layers that go beyond existing design patterns. Several of these layers enjoy the property of being independent from the batch statistics. Our experiments show that EvoNorms not only excel on a variety of image classification models including ResNets, MobileNets and EfficientNets, but also transfer well to Mask R-CNN for instance segmentation and BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers by a significant margin in many cases.},
  archiveprefix = {arXiv},
  eprint = {2004.02967},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/EVDUUA7Q/Liu et al. - 2020 - Evolving Normalization-Activation Layers.pdf;/home/msca8h/Zotero/storage/EWGJGHGH/2004.html},
  journal = {ArXiv200402967 Cs Stat},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{liu_limited_1989,
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  author = {Liu, Dong C. and Nocedal, Jorge},
  year = {1989},
  month = aug,
  volume = {45},
  pages = {503--528},
  file = {/home/msca8h/Zotero/storage/26UTS3KN/Liu and Nocedal - 1989 - On the limited memory BFGS method for large scale .pdf},
  journal = {Math. Program.},
  language = {en},
  number = {1-3}
}

@article{liu_metropolized_1996,
  title = {Metropolized Independent Sampling with Comparisons to Rejection Sampling and Importance Sampling},
  author = {Liu, Jun S.},
  year = {1996},
  month = jun,
  volume = {6},
  pages = {113--119},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@article{liu_safe_1994,
  title = {Safe Self-Scheduling: A Parallel Loop Scheduling Scheme for Shared-Memory Multiprocessors},
  author = {Liu, Jie and Saletore, Vikram A and Lewis, Ted G},
  year = {1994},
  volume = {22},
  pages = {589--616},
  file = {/home/msca8h/Zotero/storage/VPWVZP4K/Liu et al. - 1994 - Safe self-scheduling a parallel loop scheduling s.pdf},
  journal = {Int. J. Parallel Program.},
  number = {6}
}

@article{liu_sequential_1998,
  title = {Sequential {{Monte Carlo Methods}} for {{Dynamic Systems}}},
  author = {Liu, Jun S. and Chen, Rong},
  year = {1998},
  month = sep,
  volume = {93},
  pages = {1032--1044},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {443}
}

@inproceedings{liu_stein_2016,
  title = {Stein {{Variational Gradient Descent}}: {{A General Purpose Bayesian Inference Algorithm}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Liu, Qiang and Wang, Dilin},
  year = {2016},
  pages = {2378--2386},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  series = {{{NIPS}}'16}
}

@article{liu_unbiased_2017,
  title = {An {{Unbiased MCMC FPGA}}-{{Based Accelerator}} in the {{Land}} of {{Custom Precision Arithmetic}}},
  author = {Liu, Shuanglong and Mingas, Grigorios and Bouganis, Christos-Savvas},
  year = {2017},
  month = may,
  volume = {66},
  pages = {745--758},
  file = {/home/msca8h/Zotero/storage/Y83FNACA/Liu et al. - 2017 - An Unbiased MCMC FPGA-Based Accelerator in the Lan.pdf},
  journal = {IEEE Trans. Comput.},
  number = {5}
}

@inproceedings{liu_understanding_2019,
  title = {Understanding and {{Accelerating Particle}}-{{Based Variational Inference}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Liu, Chang and Zhuo, Jingwei and Cheng, Pengyu and Zhang, Ruiyi and Zhu, Jun},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {4082--4092},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Particle-based variational inference methods (ParVIs) have gained attention in the Bayesian inference literature, for their capacity to yield flexible and accurate approximations. We explore ParVIs from the perspective of Wasserstein gradient flows, and make both theoretical and practical contributions. We unify various finite-particle approximations that existing ParVIs use, and recognize that the approximation is essentially a compulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumptions and relations of existing ParVIs, and also inspires new ParVIs. We propose an acceleration framework and a principled bandwidth-selection method for general ParVIs; these are based on the developed theory and leverage the geometry of the Wasserstein space. Experimental results show the improved convergence by the acceleration framework and enhanced sample accuracy by the bandwidth-selection method.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{liu_when_2018,
  title = {When {{Gaussian Process Meets Big Data}}: {{A Review}} of {{Scalable GPs}}},
  author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1807.01065},
  eprinttype = {arxiv},
  journal = {ArXiv Prepr. ArXiv180701065}
}

@article{loizou_comparative_2005,
  title = {Comparative Evaluation of Despeckle Filtering in Ultrasound Imaging of the Carotid Artery},
  author = {Loizou, C.P. and Pattichis, C.S. and Christodoulou, C.I. and Istepanian, R.S.H. and Pantziaris, M. and Nicolaides, A.},
  year = {2005},
  month = oct,
  volume = {52},
  pages = {1653--1669},
  file = {/home/msca8h/Zotero/storage/EHHSXC9U/Loizou et al. - 2005 - Comparative evaluation of despeckle filtering in u.pdf},
  journal = {IEEE Trans. Ultrason., Ferroelect., Freq. Contr.},
  number = {10}
}

@article{loizou_quality_2006,
  title = {Quality Evaluation of Ultrasound Imaging in the Carotid Artery Based on Normalization and Speckle Reduction Filtering},
  author = {Loizou, C. P. and Pattichis, C. S. and Pantziaris, M. and Tyllis, T. and Nicolaides, A.},
  year = {2006},
  month = may,
  volume = {44},
  pages = {414--426},
  file = {/home/msca8h/Zotero/storage/C778CPS3/Loizou et al. - 2006 - Quality evaluation of ultrasound imaging in the ca.pdf},
  journal = {Med. Biol. Eng. Comput.},
  language = {en},
  number = {5}
}

@article{lorenz_neuroadaptive_2017,
  title = {Neuroadaptive {{Bayesian Optimization}} and {{Hypothesis Testing}}.},
  author = {Lorenz, R. and Hampshire, A. and Leech, R.},
  year = {2017},
  volume = {21},
  pages = {155--167},
  abstract = {Cognitive neuroscientists are often interested in broad research questions, yet use overly narrow experimental designs by considering only a small subset of possible experimental conditions. This limits the generalizability and reproducibility of many research findings. Here, we propose an alternative approach that resolves these problems by taking advantage of recent developments in real-time data analysis and machine learning. Neuroadaptive Bayesian optimization is a powerful strategy to efficiently explore more experimental conditions than is currently possible with standard methodology. We argue that such an approach could broaden the hypotheses considered in cognitive science, improving the generalizability of findings. In addition, Bayesian optimization can be combined with preregistration to cover exploration, mitigating researcher bias more broadly and improving reproducibility.},
  file = {/home/msca8h/Zotero/storage/GYNTNE2H/Neuroadaptive Bayesian Optimization.pdf},
  journal = {Trends Cogn. Sci.},
  keywords = {bayes theorem,cognition,generalization,humans,models,psychology,reproducibility of results,statistical},
  number = {3}
}

@inproceedings{lorenz_tailoring_2016,
  title = {Towards Tailoring Non-Invasive Brain Stimulation Using Real-Time {{fMRI}} and {{Bayesian}} Optimization},
  booktitle = {2016 {{International Workshop}} on {{Pattern Recognition}} in {{Neuroimaging}} ({{PRNI}})},
  author = {Lorenz, Romy and Monti, Ricardo P and Hampshire, Adam and Koush, Yury and Anagnostopoulos, Christoforos and Faisal, Aldo A and Sharp, David and Montana, Giovanni and Leech, Robert and Violante, Ines R},
  year = {2016},
  month = jun,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Trento, Italy}},
  file = {/home/msca8h/Zotero/storage/N2VBCKZV/Lorenz et al. - 2016 - Towards tailoring non-invasive brain stimulation u.pdf}
}

@article{lu_complex_2020,
  title = {Complex {{Convolutional Neural Networks}} for {{Ultrasound Image Reconstruction}} from {{In}}-{{Phase}}/{{Quadrature Signal}}},
  author = {Lu, Jingfeng and Millioz, Fabien and Garcia, Damien and Salles, Sebastien and Ye, Dong and Friboulet, Denis},
  year = {2020},
  month = sep,
  abstract = {A wide variety of studies based on deep learning have recently been investigated to improve ultrasound (US) imaging. Most of these approaches were performed on radio frequency (RF) signals. However, inphase/quadrature (I/Q) digital beamformers (IQBF) are now widely used as low-cost strategies. In this work, we leveraged complex convolutional neural networks (CCNNs) for reconstructing ultrasound images from I/Q signals. We recently described a CNN architecture called ID-Net, which exploited an inception layer devoted to the reconstruction of RF diverging-wave (DW) ultrasound images. We derived in this work the complex equivalent of this network, i.e., the complex inception for DW network (CID-Net), operating on I/Q data. We provided experimental evidence that the CID-Net yields the same image quality as that obtained from the RF-trained CNNs; i.e., by using only three I/Q images, the CID-Net produced high-quality images competing with those obtained by coherently compounding 31 RF images. Moreover, we showed that the CID-Net outperforms the straightforward architecture consisting in processing separately the real and imaginary parts of the I/Q signal, indicating thereby the importance of consistently processing the I/Q signals using a network that exploits the complex nature of such signal.},
  archiveprefix = {arXiv},
  eprint = {2009.11536},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/8THCRWB8/Lu et al. - 2020 - Complex Convolutional Neural Networks for Ultrasou.pdf;/home/msca8h/Zotero/storage/9BT532PM/2009.html},
  journal = {ArXiv200911536 Eess},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing},
  primaryclass = {eess}
}

@inproceedings{lucco_dynamic_1992,
  title = {A Dynamic Scheduling Method for Irregular Parallel Programs},
  booktitle = {Proc. {{ACM SIGPLAN}} 1992 {{Conf}}. {{Program}}. {{Lang}}. {{Des}}. {{Implementation}}},
  author = {Lucco, Steven},
  year = {1992},
  pages = {200--211},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  file = {/home/msca8h/Documents/parallel_scheduling/Lucco - 1992 - A Dynamic Scheduling Method for Irregular Parallel.pdf},
  keywords = {Tapering},
  series = {{{PLDI}} '92}
}

@article{luke_optimal_1998,
  title = {The Optimal Effectiveness Metric for Parallel Application Analysis},
  author = {Luke, Edward A. and Banicescu, Ioana and Li, Jin},
  year = {1998},
  month = jun,
  volume = {66},
  pages = {223--229},
  file = {/home/msca8h/Zotero/storage/SW5CF2ZP/Luke et al. - 1998 - The optimal effectiveness metric for parallel appl.pdf},
  journal = {Information Processing Letters},
  language = {en},
  number = {5}
}

@inproceedings{luo_learning_2017,
  title = {Learning {{Deep Architectures}} via {{Generalized Whitened Neural Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Luo, Ping},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {2238--2246},
  publisher = {{PMLR}},
  address = {{International Convention Centre, Sydney, Australia}},
  abstract = {Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation. However, the whitening transformation increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties. First, GWNN is able to learn compact representation to reduce computations. Second, it enables whitening transformation to be performed in a short period, preserving good conditioning. Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency. Extensive experiments on various datasets demonstrate the benefits of GWNN.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@techreport{mackay_local_2001,
  title = {Local Minima, Symmetry-Breaking, and Model Pruning in Variational Free Energy Minimization},
  author = {MacKay, David J.C.},
  year = {2001},
  month = jun,
  type = {Technical {{Report}}}
}

@article{maddox_rethinking_2020,
  title = {Rethinking {{Parameter Counting}} in {{Deep Models}}: {{Effective Dimensionality Revisited}}},
  shorttitle = {Rethinking {{Parameter Counting}} in {{Deep Models}}},
  author = {Maddox, Wesley J. and Benton, Gregory and Wilson, Andrew Gordon},
  year = {2020},
  month = mar,
  abstract = {Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models.},
  archiveprefix = {arXiv},
  eprint = {2003.02139},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/KGB6NS8H/Maddox et al. - 2020 - Rethinking Parameter Counting in Deep Models Effe.pdf;/home/msca8h/Zotero/storage/K3JFRBDY/2003.html},
  journal = {ArXiv200302139 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{maddox_simple_2019,
  title = {A {{Simple Baseline}} for {{Bayesian Uncertainty}} in {{Deep Learning}}},
  author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = dec,
  abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
  archiveprefix = {arXiv},
  eprint = {1902.02476},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/Y2XGQUJ4/Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf;/home/msca8h/Zotero/storage/YNZSNCEI/1902.html},
  journal = {ArXiv190202476 Cs Stat},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{magnusson_convergence_2016,
  title = {On the {{Convergence}} of {{Alternating Direction Lagrangian Methods}} for {{Nonconvex Structured Optimization Problems}}},
  author = {Magnusson, Sindri and Weeraddana, Pradeep Chathuranga and Rabbat, Michael G. and Fischione, Carlo},
  year = {2016},
  month = sep,
  volume = {3},
  pages = {296--309},
  file = {/home/msca8h/Zotero/storage/L93CB5AZ/Magnusson et al. - 2016 - On the Convergence of Alternating Direction Lagran.pdf},
  journal = {IEEE Trans. Control Netw. Syst.},
  number = {3}
}

@inproceedings{mahendran_adaptive_2012,
  title = {Adaptive {{MCMC}} with {{Bayesian Optimization}}},
  booktitle = {Proceedings of the {{Fifteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and Freitas, Nando De},
  year = {2012},
  month = apr,
  volume = {22},
  pages = {751--760},
  publisher = {{PMLR}},
  address = {{La Palma, Canary Islands}},
  abstract = {This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@inproceedings{majo_memory_2011,
  title = {Memory {{Management}} in {{NUMA Multicore Systems}}: {{Trapped Between Cache Contention}} and {{Interconnect Overhead}}},
  booktitle = {Proceedings of the {{International Symposium}} on {{Memory Management}}},
  author = {Majo, Zoltan and Gross, Thomas R.},
  year = {2011},
  pages = {11--20},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  file = {/home/msca8h/Documents/parallel_scheduling/Majo and Gross - 2011 - Memory Management in NUMA Multicore Systems Trapp.pdf},
  keywords = {memory allocation,multicore processors,numa,shared resource contention},
  series = {{{ISMM}} '11}
}

@incollection{malkomes_automating_2018,
  title = {Automating {{Bayesian}} Optimization with {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Malkomes, Gustavo and Garnett, Roman},
  year = {2018},
  pages = {5984--5994},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{malkomes_automating_2018a,
  title = {Automating {{Bayesian}} Optimization with {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Malkomes, Gustavo and Garnett, Roman},
  year = {2018},
  pages = {5984--5994},
  publisher = {{Curran Associates, Inc.}}
}

@article{mangoubi_does_2018,
  title = {Does {{Hamiltonian Monte Carlo}} Mix Faster than a Random Walk on Multimodal Densities?},
  author = {Mangoubi, Oren and Pillai, Natesh S. and Smith, Aaron},
  year = {2018},
  month = sep,
  abstract = {Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of Markov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity of HMC algorithms is their excellent performance as the dimension \$d\$ of the target becomes large: under conditions that are satisfied for many common statistical models, optimally-tuned HMC algorithms have a running time that scales like \$d\^\{0.25\}\$. In stark contrast, the running time of the usual Random-Walk Metropolis (RWM) algorithm, optimally tuned, scales like \$d\$. This superior scaling of the HMC algorithm with dimension is attributed to the fact that it, unlike RWM, incorporates the gradient information in the proposal distribution. In this paper, we investigate a different scaling question: does HMC beat RWM for highly \$\textbackslash textit\{multimodal\}\$ targets? We find that the answer is often \$\textbackslash textit\{no\}\$. We compute the spectral gaps for both the algorithms for a specific class of multimodal target densities, and show that they are identical. The key reason is that, within one mode, the gradient is effectively ignorant about other modes, thus negating the advantage the HMC algorithm enjoys in unimodal targets. We also give heuristic arguments suggesting that the above observation may hold quite generally. Our main tool for answering this question is a novel simple formula for the conductance of HMC using Liouville's theorem. This result allows us to compute the spectral gap of HMC algorithms, for both the classical HMC with isotropic momentum and the recent Riemannian HMC, for multimodal targets.},
  archiveprefix = {arXiv},
  eprint = {1808.03230},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/A2MVFI4P/Mangoubi et al. - 2018 - Does Hamiltonian Monte Carlo mix faster than a ran.pdf;/home/msca8h/Zotero/storage/XF4ZLP2X/1808.html},
  journal = {ArXiv180803230 Cs Math Stat},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {cs, math, stat}
}

@inproceedings{marathe_performance_2017,
  title = {Performance {{Modeling}} under {{Resource Constraints Using Deep Transfer Learning}}},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Marathe, Aniruddha and Anirudh, Rushil and Jain, Nikhil and Bhatele, Abhinav and Thiagarajan, Jayaraman and Kailkhura, Bhavya and Yeom, Jae-Seung and Rountree, Barry and Gamblin, Todd},
  year = {2017},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {deep learning,parameter selection,performance prediction,transfer learning},
  series = {{{SC}} '17}
}

@article{marin_consistency_2019,
  title = {Consistency of Adaptive Importance Sampling and Recycling Schemes},
  author = {Marin, Jean-Michel and Pudlo, Pierre and Sedki, Mohammed},
  year = {2019},
  month = aug,
  volume = {25},
  journal = {Bernoulli},
  number = {3}
}

@article{markatos_using_1994,
  title = {Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors},
  author = {Markatos, E.P. and LeBlanc, T.J.},
  year = {1994},
  month = apr,
  volume = {5},
  pages = {379--400},
  file = {/home/msca8h/Documents/parallel_scheduling/Markatos and LeBlanc - 1994 - Using processor affinity in loop scheduling on sha.pdf},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {4}
}

@article{martinez-cantin_bayesopt_2014,
  title = {{{BayesOpt}}: A {{Bayesian}} Optimization Library for Nonlinear Optimization, Experimental Design and Bandits},
  author = {{Martinez-Cantin}, Ruben},
  year = {2014},
  volume = {15},
  pages = {3915--3919},
  journal = {J. Mach. Learn. Res.}
}

@article{martinez-cantin_practical_2017,
  title = {Practical {{Bayesian}} Optimization in the Presence of Outliers},
  author = {{Martinez-Cantin}, Ruben and Tee, Kevin and McCourt, Michael},
  year = {2017},
  month = dec,
  abstract = {Inference in the presence of outliers is an important field of research as outliers are ubiquitous and may arise across a variety of problems and domains. Bayesian optimization is method that heavily relies on probabilistic inference. This allows outstanding sample efficiency because the probabilistic machinery provides a memory of the whole optimization process. However, that virtue becomes a disadvantage when the memory is populated with outliers, inducing bias in the estimation. In this paper, we present an empirical evaluation of Bayesian optimization methods in the presence of outliers. The empirical evidence shows that Bayesian optimization with robust regression often produces suboptimal results. We then propose a new algorithm which combines robust regression (a Gaussian process with Student-t likelihood) with outlier diagnostics to classify data points as outliers or inliers. By using an scheduler for the classification of outliers, our method is more efficient and has better convergence over the standard robust regression. Furthermore, we show that even in controlled situations with no expected outliers, our method is able to produce better results.},
  archiveprefix = {arXiv},
  eprint = {1712.04567},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/66IZF3TX/Martinez-Cantin et al. - 2017 - Practical Bayesian optimization in the presence of.pdf;/home/msca8h/Zotero/storage/ER5VPHMX/1712.html},
  journal = {arXiv:1712.04567 [cs, stat]},
  keywords = {90C26; 62K25; 62F35,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{martinez-cantin_robust_2017,
  title = {Robust {{Bayesian}} Optimization with {{Student}}-t Likelihood},
  author = {{Martinez-Cantin}, Ruben and McCourt, Michael and Tee, Kevin},
  year = {2017},
  archiveprefix = {arXiv},
  eprint = {1707.05729},
  eprinttype = {arxiv},
  journal = {ArXiv Prepr. ArXiv170705729}
}

@article{martino_adaptive_2015,
  title = {An Adaptive Population Importance Sampler: Learning from Uncertainty},
  shorttitle = {An {{Adaptive Population Importance Sampler}}},
  author = {Martino, Luca and Elvira, Victor and Luengo, David and Corander, Jukka},
  year = {2015},
  month = aug,
  volume = {63},
  pages = {4422--4437},
  file = {/home/msca8h/Zotero/storage/VUZC8DMX/Martino et al. - 2015 - An Adaptive Population Importance Sampler Learnin.pdf},
  journal = {IEEE Trans. Signal Process.},
  number = {16}
}

@article{martino_issues_2017,
  title = {Issues in the {{Multiple Try Metropolis}} Mixing},
  author = {Martino, L. and Louzada, F.},
  year = {2017},
  month = mar,
  volume = {32},
  pages = {239--252},
  file = {/home/msca8h/Zotero/storage/8KAR68L2/Martino and Louzada - 2017 - Issues in the Multiple Try Metropolis mixing.pdf},
  journal = {Comput Stat},
  language = {en},
  number = {1}
}

@article{martino_layered_2017,
  title = {Layered Adaptive Importance Sampling},
  author = {Martino, L. and Elvira, V. and Luengo, D. and Corander, J.},
  year = {2017},
  month = may,
  volume = {27},
  pages = {599--623},
  file = {/home/msca8h/Zotero/storage/BA9U4DD3/Martino et al. - 2017 - Layered adaptive importance sampling.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {3}
}

@incollection{martino_metropolis_2017,
  title = {Metropolis {{Sampling}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Martino, Luca and Elvira, Victor},
  year = {2017},
  month = may,
  pages = {1--18},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  language = {en}
}

@article{martino_orthogonal_2016,
  title = {Orthogonal Parallel {{MCMC}} Methods for Sampling and Optimization},
  author = {Martino, L. and Elvira, V. and Luengo, D. and Corander, J. and Louzada, F.},
  year = {2016},
  month = nov,
  volume = {58},
  pages = {64--84},
  file = {/home/msca8h/Zotero/storage/ELWC3EB8/Martino et al. - 2016 - Orthogonal parallel MCMC methods for sampling and .pdf},
  journal = {Digital Signal Processing},
  language = {en}
}

@inproceedings{martino_parallel_2016,
  title = {Parallel Metropolis Chains with Cooperative Adaptation},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Martino, L. and Elvira, V and Luengo, D. and Louzada, F},
  year = {2016},
  month = mar,
  pages = {3974--3978},
  publisher = {{IEEE}},
  address = {{Shanghai}},
  file = {/home/msca8h/Zotero/storage/L88AUWPC/Martino et al. - 2016 - Parallel metropolis chains with cooperative adapta.pdf}
}

@article{martino_review_2018,
  title = {A Review of Multiple Try {{MCMC}} Algorithms for Signal Processing},
  author = {Martino, Luca},
  year = {2018},
  month = apr,
  volume = {75},
  pages = {134--152},
  file = {/home/msca8h/Zotero/storage/EZWARYVG/Martino - 2018 - A review of multiple try MCMC algorithms for signa.pdf},
  journal = {Digital Signal Processing},
  language = {en}
}

@article{martino_review_2018a,
  title = {A Review of Multiple Try {{MCMC}} Algorithms for Signal Processing},
  author = {Martino, Luca},
  year = {2018},
  month = apr,
  volume = {75},
  pages = {134--152},
  file = {/home/msca8h/Zotero/storage/4Y9E6D3P/Martino - 2018 - A review of multiple try MCMC algorithms for signa.pdf},
  journal = {Digital Signal Processing},
  language = {en}
}

@inproceedings{martino_smelly_2015,
  title = {Smelly Parallel {{MCMC}} Chains},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Martino, L. and Elvira, V. and Luengo, D. and {Artes-Rodriguez}, A. and Corander, J.},
  year = {2015},
  month = apr,
  pages = {4070--4074},
  publisher = {{IEEE}},
  address = {{South Brisbane, Queensland, Australia}}
}

@article{martis_ecg_2013,
  title = {{{ECG}} Beat Classification Using {{PCA}}, {{LDA}}, {{ICA}} and {{Discrete Wavelet Transform}}},
  author = {Martis, Roshan Joy and Acharya, U. Rajendra and Min, Lim Choo},
  year = {2013},
  month = sep,
  volume = {8},
  pages = {437--448},
  journal = {Biomedical Signal Processing and Control},
  language = {en},
  number = {5}
}

@book{mcbook,
  title = {Monte {{Carlo}} Theory, Methods and Examples},
  author = {Owen, Art B.},
  year = {2013}
}

@article{mcphail_robustness_2018,
  title = {Robustness {{Metrics}}: {{How Are They Calculated}}, {{When Should They Be Used}} and {{Why Do They Give Different Results}}?},
  shorttitle = {Robustness {{Metrics}}},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  year = {2018},
  month = feb,
  volume = {6},
  pages = {169--191},
  file = {/home/msca8h/Zotero/storage/KKKQVPMS/McPhail et al. - 2018 - Robustness Metrics How Are They Calculated, When .pdf},
  journal = {Earth's Future},
  language = {en},
  number = {2}
}

@article{mcphail_robustness_2018a,
  title = {Robustness Metrics: How Are They Calculated, When Should They Be Used and Why Do They Give Different Results?},
  shorttitle = {Robustness {{Metrics}}},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  year = {2018},
  month = feb,
  volume = {6},
  pages = {169--191},
  file = {/home/msca8h/Zotero/storage/URPWCJ8U/McPhail et al. - 2018 - Robustness Metrics How Are They Calculated, When .pdf},
  journal = {Earth's Future},
  language = {en},
  number = {2}
}

@article{mcsharry_dynamical_2003,
  title = {A Dynamical Model for Generating Synthetic Electrocardiogram Signals},
  author = {McSharry, P.E. and Clifford, G.D. and Tarassenko, L. and Smith, L.A.},
  year = {2003},
  month = mar,
  volume = {50},
  pages = {289--294},
  file = {/home/msca8h/Zotero/storage/93DN66HR/McSharry et al. - 2003 - A dynamical model for generating synthetic electro.pdf},
  journal = {IEEE Trans. Biomed. Eng.},
  language = {en},
  number = {3}
}

@article{mei_phase_2020,
  title = {Phase Asymmetry Ultrasound Despeckling with Fractional Anisotropic Diffusion and Total Variation},
  author = {Mei, Kunqiang and Hu, Bin and Fei, Baowei and Qin, Binjie},
  year = {2020},
  volume = {29},
  pages = {2845--2859},
  file = {/home/msca8h/Zotero/storage/F68MZU4A/Mei et al. - 2020 - Phase Asymmetry Ultrasound Despeckling With Fracti.pdf},
  journal = {IEEE Trans. Image Process.}
}

@inproceedings{Menon2020AutotuningPC,
  title = {Auto-Tuning Parameter Choices in {{HPC}} Applications Using Bayesian Optimization},
  author = {Menon, Harshitha and Bhatele, Abhinav and Gamblin, Todd},
  year = {2020}
}

@article{mesquita_embarrassingly_2019,
  title = {Embarrassingly Parallel {{MCMC}} Using Deep Invertible Transformations},
  author = {Mesquita, Diego and Blomstedt, Paul and Kaski, Samuel},
  year = {2019},
  month = mar,
  abstract = {While MCMC methods have become a main work-horse for Bayesian inference, scaling them to large distributed datasets is still a challenge. Embarrassingly parallel MCMC strategies take a divide-and-conquer stance to achieve this by writing the target posterior as a product of subposteriors, running MCMC for each of them in parallel and subsequently combining the results. The challenge then lies in devising efficient aggregation strategies. Current strategies trade-off between approximation quality, and costs of communication and computation. In this work, we introduce a novel method that addresses these issues simultaneously. Our key insight is to introduce a deep invertible transformation to approximate each of the subposteriors. These approximations can be made accurate even for complex distributions and serve as intermediate representations, keeping the total communication cost limited. Moreover, they enable us to sample from the product of the subposteriors using an efficient and stable importance sampling scheme. We demonstrate the approach outperforms available state-of-the-art methods in a range of challenging scenarios, including high-dimensional and heterogeneous subposteriors.},
  archiveprefix = {arXiv},
  eprint = {1903.04556},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/BHJCXPKR/Mesquita et al. - 2019 - Embarrassingly parallel MCMC using deep invertible.pdf;/home/msca8h/Zotero/storage/MHP3YGCQ/1903.html},
  journal = {ArXiv190304556 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{metropolis_equation_1953,
  title = {Equation of State Calculations by Fast Computing Machines},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = {1953},
  month = jun,
  volume = {21},
  pages = {1087--1092},
  file = {/home/msca8h/Zotero/storage/DV7X38EW/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf},
  journal = {The Journal of Chemical Physics},
  language = {en},
  number = {6}
}

@article{michel_clock_2019,
  title = {Clock {{Monte Carlo}} Methods},
  author = {Michel, Manon and Tan, Xiaojun and Deng, Youjin},
  year = {2019},
  month = jan,
  volume = {99},
  pages = {010105},
  file = {/home/msca8h/Zotero/storage/YAVUQYGQ/Michel et al. - 2019 - Clock Monte Carlo methods.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {1}
}

@article{ming-yuenchan_perceptionbased_2009,
  title = {Perception-{{Based Transparency Optimization}} for {{Direct Volume Rendering}}},
  author = {{Ming-Yuen Chan} and {Yingcai Wu} and {Wai-Ho Mak} and {Wei Chen} and {Huamin Qu}},
  year = {2009},
  month = nov,
  volume = {15},
  pages = {1283--1290},
  journal = {IEEE Trans. Visual. Comput. Graphics},
  number = {6}
}

@article{mingas_populationbased_2016,
  title = {Population-Based {{MCMC}} on Multi-Core {{CPUs}}, {{GPUs}} and {{FPGAs}}},
  author = {Mingas, Grigorios and Bouganis, Christos-Savvas},
  year = {2016},
  month = apr,
  volume = {65},
  pages = {1283--1296},
  file = {/home/msca8h/Zotero/storage/UR5NSRXA/Mingas and Bouganis - 2016 - Population-Based MCMC on Multi-Core CPUs, GPUs and.pdf},
  journal = {IEEE Trans. Comput.},
  number = {4}
}

@article{minh_understanding_2015,
  title = {Understanding the {{Hastings Algorithm}}},
  author = {Minh, David D. L. and Minh, Do Le (Paul)},
  year = {2015},
  month = feb,
  volume = {44},
  pages = {332--349},
  file = {/home/msca8h/Zotero/storage/23G7T4HA/Minh and Minh - 2015 - Understanding the Hastings Algorithm.pdf},
  journal = {Communications in Statistics - Simulation and Computation},
  language = {en},
  number = {2}
}

@techreport{minka2005divergence,
  title = {Divergence Measures and Message Passing},
  author = {Minka, Tom},
  year = {2005},
  month = jan,
  pages = {17},
  abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.},
  number = {MSR-TR-2005-173}
}

@article{mira_ordering_2001,
  title = {Ordering and Improving the Performance of {{Monte Carlo Markov}} Chains},
  author = {Mira, Antonietta},
  year = {2001},
  month = nov,
  volume = {16},
  file = {/home/msca8h/Zotero/storage/FS5LQEF4/Mira - 2001 - Ordering and Improving the Performance of Monte Ca.pdf},
  journal = {Statist. Sci.},
  number = {4}
}

@inproceedings{mishra_despeckling_2018,
  title = {Despeckling {{CNN}} with Ensembles of Classical Outputs},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Mishra, Deepak and Tyagi, Sarthak and Chaudhury, Santanu and Sarkar, Mukul and Singh Soin, Arvinder},
  year = {2018},
  month = aug,
  pages = {3802--3807},
  publisher = {{IEEE}},
  address = {{Beijing}}
}

@incollection{mishra_edge_2017,
  title = {Edge Aware Geometric Filter for Ultrasound Image Enhancement},
  booktitle = {Medical {{Image Understanding}} and {{Analysis}}},
  author = {Mishra, Deepak and Chaudhury, Santanu and Sarkar, Mukul and Soin, Arvinder Singh},
  year = {2017},
  volume = {723},
  pages = {109--120},
  publisher = {{Springer International Publishing}},
  address = {{Cham}}
}

@article{mishra_edge_2018,
  title = {Edge Probability and Pixel Relativity-Based Speckle Reducing Anisotropic Diffusion},
  author = {Mishra, Deepak and Chaudhury, Santanu and Sarkar, Mukul and Soin, Arvinder Singh and Sharma, Vivek},
  year = {2018},
  month = feb,
  volume = {27},
  pages = {649--664},
  journal = {IEEE Trans. Image Process.},
  number = {2}
}

@inproceedings{mnih_asynchronous_2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  volume = {48},
  pages = {1928--1937},
  publisher = {{PMLR}},
  address = {{New York, New York, USA}},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@techreport{moore_metropolishastings_2020,
  title = {Metropolis-{{Hastings}} Forward Proposal},
  author = {Moore, Robert E.},
  year = {2020},
  month = may,
  institution = {{University of Liverpool}},
  type = {Technical {{Report}}}
}

@inproceedings{moshavegh_advanced_2015,
  title = {Advanced Automated Gain Adjustments for In-Vivo Ultrasound Imaging},
  booktitle = {{{IEEE International Ultrasonics Symposium}}},
  author = {Moshavegh, Ramin and Hemmsen, Martin Christian and Martins, Bo and Hansen, Kristoffer Lindskov and Ewertsen, Caroline and Brandt, Andreas Hjelm and Bechsgaard, Thor and Nielsen, Michael Bachmann and Jensen, Jorgen Arendt},
  year = {2015},
  month = oct,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}},
  file = {/home/msca8h/Zotero/storage/5WDSSY5J/Moshavegh et al. - 2015 - Advanced automated gain adjustments for in-vivo ul.pdf}
}

@article{muller_latencyhiding_2016,
  title = {Latency-{{Hiding Work Stealing}}},
  author = {Muller, S. K. and Acar, U. A.},
  year = {2016},
  file = {/home/msca8h/Zotero/storage/JRBMFTWY/latency_hiding_WS.pdf},
  journal = {Proc. 28th ACM Symp. Parallelism Algorithms Archit. SPAA 16}
}

@inproceedings{muralidharan_nitro_2014,
  title = {Nitro: {{A Framework}} for {{Adaptive Code Variant Tuning}}},
  shorttitle = {Nitro},
  booktitle = {2014 {{IEEE}} 28th {{International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Muralidharan, Saurav and Shantharam, Manu and Hall, Mary and Garland, Michael and Catanzaro, Bryan},
  year = {2014},
  month = may,
  pages = {501--512},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}},
  file = {/home/msca8h/Zotero/storage/7C36F2UG/Muralidharan et al. - 2014 - Nitro A Framework for Adaptive Code Variant Tunin.pdf}
}

@book{murphy_machine_2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts London, England}},
  annotation = {OCLC: 812073227},
  file = {/home/msca8h/Zotero/storage/3XQPF7G8/Murphy - 2012 - Machine learning a probabilistic perspective.pdf},
  language = {eng},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{murray_anytime_2017,
  title = {Anytime {{Monte Carlo}}},
  author = {Murray, Lawrence M. and Singh, Sumeetpal and Jacob, Pierre E. and Lee, Anthony},
  year = {2017},
  month = jun,
  abstract = {A Monte Carlo algorithm typically simulates some prescribed number of samples, taking some random real time to complete the computations necessary. This work considers the converse: to impose a real-time budget on the computation, so that the number of samples simulated is random. To complicate matters, the real time taken for each simulation may depend on the sample produced, so that the samples themselves are not independent of their number, and a length bias with respect to compute time is apparent. This is especially problematic when a Markov chain Monte Carlo (MCMC) algorithm is used and the final state of the Markov chain---rather than an average over all states---is required. The length bias does not diminish with the compute budget in this case. It occurs, for example, in sequential Monte Carlo (SMC) algorithms. We propose an anytime framework to address the concern, using a continuous-time Markov jump process to study the progress of the computation in real time. We show that the length bias can be eliminated for any MCMC algorithm by using a multiple chain construction. The utility of this construction is demonstrated on a large-scale SMC-squared implementation, using four billion particles distributed across a cluster of 128 graphics processing units on the Amazon EC2 service. The anytime framework imposes a real-time budget on the MCMC move steps within SMC-squared, ensuring that all processors are simultaneously ready for the resampling step, demonstrably reducing wait times and providing substantial control over the total compute budget.},
  archiveprefix = {arXiv},
  eprint = {1612.03319},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/Q2YJ6GBX/Murray et al. - 2017 - Anytime Monte Carlo.pdf;/home/msca8h/Zotero/storage/Q58ID2J8/1612.html},
  journal = {ArXiv161203319 Stat},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  primaryclass = {stat}
}

@inproceedings{murray_elliptical_2010,
  title = {Elliptical Slice Sampling},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Murray, Iain and Adams, Ryan and MacKay, David},
  year = {2010},
  month = may,
  volume = {9},
  pages = {541--548},
  publisher = {{PMLR}},
  address = {{Chia Laguna Resort, Sardinia, Italy}},
  abstract = {Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@incollection{murray_slice_2010,
  title = {Slice Sampling Covariance Hyperparameters of Latent {{Gaussian}} Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Murray, Iain and Adams, Ryan P},
  year = {2010},
  pages = {1732--1740},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{murray2010distributed,
  title = {Distributed {{Markov}} Chain {{Monte Carlo}}},
  booktitle = {Neural {{Information Processing Systems Workshop}} on {{Leaning}} on {{Cores}}, {{Clusters}}, and {{Clouds}}},
  author = {Murray, Lawrence},
  year = {2010}
}

@inproceedings{mytkowicz_producing_2009,
  title = {Producing {{Wrong Data Without Doing Anything Obviously Wrong}}!},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
  year = {2009},
  pages = {265--276},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  file = {/home/msca8h/Zotero/storage/D2GNBCCN/Mytkowicz et al. - 2009 - Producing Wrong Data Without Doing Anything Obviou.pdf},
  keywords = {bias,measurement,performance},
  series = {{{ASPLOS XIV}}}
}

@article{naesseth_elements_2019,
  title = {Elements of {{Sequential Monte Carlo}}},
  author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch{\"o}n, Thomas B.},
  year = {2019},
  volume = {12},
  pages = {307--392},
  file = {/home/msca8h/Zotero/storage/IX9KJ4ZI/Naesseth et al. - 2019 - Elements of Sequential Monte Carlo.pdf},
  journal = {Found. Trends\textregistered{} Mach. Learn.},
  number = {3}
}

@article{nagare_multi_2017,
  title = {A Multi Directional Perfect Reconstruction Filter Bank Designed with 2-{{D}} Eigenfilter Approach: Application to Ultrasound Speckle Reduction},
  shorttitle = {A {{Multi Directional Perfect Reconstruction Filter Bank Designed}} with 2-{{D Eigenfilter Approach}}},
  author = {Nagare, Mukund B and Patil, Bhushan D and Holambe, Raghunath S},
  year = {2017},
  month = feb,
  volume = {41},
  pages = {31},
  journal = {J. Med. Syst.},
  language = {en},
  number = {2}
}

@article{nakarmi_kernelbased_2017,
  title = {A {{Kernel}}-{{Based Low}}-{{Rank}} ({{KLR}}) {{Model}} for {{Low}}-{{Dimensional Manifold Recovery}} in {{Highly Accelerated Dynamic MRI}}},
  author = {Nakarmi, Ukash and Wang, Yanhua and Lyu, Jingyuan and Liang, Dong and Ying, Leslie},
  year = {2017},
  month = nov,
  volume = {36},
  pages = {2297--2307},
  file = {/home/msca8h/Zotero/storage/2T4DFE2H/Nakarmi et al. - 2017 - A Kernel-Based Low-Rank (KLR) Model for Low-Dimens.pdf},
  journal = {IEEE Trans. Med. Imaging},
  number = {11}
}

@article{neal_annealed_2001,
  title = {Annealed Importance Sampling},
  author = {Neal, Radford M.},
  year = {2001},
  volume = {11},
  pages = {125--139},
  journal = {Stat. Comput.},
  number = {2}
}

@book{neal_bayesian_1996,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  year = {1996},
  volume = {118},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  series = {Lecture {{Notes}} in {{Statistics}}}
}

@article{neal_improved_1992,
  title = {An {{Improved Acceptance Procedure}} for the {{Hybrid Monte Carlo Algorithm}}},
  author = {Neal, R. M.},
  year = {1992},
  month = aug,
  abstract = {The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a state within the selected window being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve performance for a test system of uncoupled oscillators.},
  archiveprefix = {arXiv},
  eprint = {hep-lat/9208011},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/AMQA6TDS/Neal - 1992 - An Improved Acceptance Procedure for the Hybrid Mo.pdf;/home/msca8h/Zotero/storage/5QDMHZBL/9208011.html},
  journal = {ArXivhep-Lat9208011},
  keywords = {High Energy Physics - Lattice}
}

@article{neal_improved_1994,
  title = {An Improved Acceptance Procedure for the Hybrid {{Monte Carlo}} Algorithm},
  author = {Neal, Radford M.},
  year = {1994},
  month = mar,
  volume = {111},
  pages = {194--203},
  file = {/home/msca8h/Zotero/storage/9VY623A5/Neal - 1994 - An Improved Acceptance Procedure for the Hybrid Mo.pdf},
  journal = {J. Comput. Phys.},
  language = {en},
  number = {1}
}

@incollection{neal_mcmc_2011,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  booktitle = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Neal, Radford M},
  year = {2011},
  edition = {First},
  pages = {113--162},
  publisher = {{Chapman and Hall/CRC}},
  series = {Handbooks of {{Modern Statistical Methods}}}
}

@techreport{neal_mcmc_2011a,
  title = {{{MCMC Using Ensembles}} of {{States}} for {{Problems}} with {{Fast}} and {{Slow Variables}} Such as {{Gaussian Process Regression}}},
  author = {Neal, Radford M.},
  year = {2011},
  month = jan,
  institution = {{University of Toronto}},
  abstract = {I introduce a Markov chain Monte Carlo (MCMC) scheme in which sampling from a distribution with density pi(x) is done using updates operating on an "ensemble" of states. The current state x is first stochastically mapped to an ensemble, x\^\{(1)\},...,x\^\{(K)\}. This ensemble is then updated using MCMC updates that leave invariant a suitable ensemble density, rho(x\^\{(1)\},...,x\^\{(K)\}), defined in terms of pi(x\^\{(i)\}) for i=1,...,K. Finally a single state is stochastically selected from the ensemble after these updates. Such ensemble MCMC updates can be useful when characteristics of pi and the ensemble permit pi(x\^\{(i)\}) for all i in \{1,...,K\}, to be computed in less than K times the amount of computation time needed to compute pi(x) for a single x. One common situation of this type is when changes to some "fast" variables allow for quick re-computation of the density, whereas changes to other "slow" variables do not. Gaussian process regression models are an example of this sort of problem, with an overall scaling factor for covariances and the noise variance being fast variables. I show that ensemble MCMC for Gaussian process regression models can indeed substantially improve sampling performance. Finally, I discuss other possible applications of ensemble MCMC, and its relationship to the "multiple-try Metropolis" method of Liu, Liang, and Wong and the "multiset sampler" of Leman, Chen, and Lavine.},
  archiveprefix = {arXiv},
  eprint = {1101.0387},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/KX2WVRTS/Neal - 2011 - MCMC Using Ensembles of States for Problems with F.pdf;/home/msca8h/Zotero/storage/P6I37EDD/1101.html},
  keywords = {Statistics - Computation},
  number = {1011},
  type = {Dept. of {{Statistics Technical Report}}}
}

@article{neal_slice_2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  volume = {31},
  pages = {705--767},
  file = {/home/msca8h/Zotero/storage/EE77CBUZ/Neal - 2003 - Slice sampling.pdf},
  journal = {Ann. Statist.},
  language = {en},
  number = {3}
}

@techreport{Neal2011ProbabilisticIU,
  title = {Probabilistic Inference Using Markov Chain Monte Carlo Methods},
  author = {Neal, Radford M.},
  year = {1993},
  month = sep,
  institution = {{University of Toronto}},
  number = {CRG-TR-93-1}
}

@article{neklyudov_metropolishastings_2019,
  title = {Metropolis-{{Hastings}} View on Variational Inference and Adversarial Training},
  author = {Neklyudov, Kirill and Egorov, Evgenii and Shvechikov, Pavel and Vetrov, Dmitry},
  year = {2019},
  month = jun,
  abstract = {A significant part of MCMC methods can be considered as the Metropolis-Hastings (MH) algorithm with different proposal distributions. From this point of view, the problem of constructing a sampler can be reduced to the question - how to choose a proposal for the MH algorithm? To address this question, we propose to learn an independent sampler that maximizes the acceptance rate of the MH algorithm, which, as we demonstrate, is highly related to the conventional variational inference. For Bayesian inference, the proposed method compares favorably against alternatives to sample from the posterior distribution. Under the same approach, we step beyond the scope of classical MCMC methods and deduce the Generative Adversarial Networks (GANs) framework from scratch, treating the generator as the proposal and the discriminator as the acceptance test. On real-world datasets, we improve Frechet Inception Distance and Inception Score, using different GANs as a proposal distribution for the MH algorithm. In particular, we demonstrate improvements of recently proposed BigGAN model on ImageNet.},
  archiveprefix = {arXiv},
  eprint = {1810.07151},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/ZNZ52NNU/Neklyudov et al. - 2019 - Metropolis-Hastings view on variational inference .pdf;/home/msca8h/Zotero/storage/2MB7FLBH/1810.html},
  journal = {ArXiv181007151 Cs Stat},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{nelder_simplex_1965,
  title = {A Simplex Method for Function Minimization},
  author = {Nelder, J. A. and Mead, R.},
  year = {1965},
  month = jan,
  volume = {7},
  pages = {308--313},
  journal = {The Comput. J.},
  language = {en},
  number = {4}
}

@inproceedings{nelson_generating_2015,
  title = {Generating {{Efficient Tensor Contractions}} for {{GPUs}}},
  booktitle = {2015 44th {{International Conference}} on {{Parallel Processing}}},
  author = {Nelson, Thomas and Rivera, Axel and Balaprakash, Prasanna and Hall, Mary and Hovland, Paul D. and Jessup, Elizabeth and Norris, Boyana},
  year = {2015},
  month = sep,
  pages = {969--978},
  publisher = {{IEEE}},
  address = {{Beijing, China}}
}

@inproceedings{nemeth_automatic_2020,
  title = {Automatic Parallelization of Probabilistic Models with Varying Load Imbalance},
  booktitle = {Proceedings of the 20th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet}}},
  author = {Nemeth, Balazs and Haber, Tom and Liesenborgs, Jori and Lamotte, Wim},
  year = {2020},
  month = may,
  pages = {752--759},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}},
  series = {{{CCGrid}}'20}
}

@inproceedings{nemeth_distributed_2017,
  title = {Distributed {{Affine}}-{{Invariant MCMC Sampler}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Nemeth, Balazs and Haber, Tom and Liesenborgs, Jori and Lamotte, Wim},
  year = {2017},
  month = sep,
  pages = {520--524},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}}
}

@inproceedings{nemeth_relaxing_2018,
  title = {Relaxing Scalability Limits with Speculative Parallelism in {{Sequential Monte Carlo}}},
  booktitle = {Proceedings of the 2018 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Nemeth, Balazs and Haber, Tom and Liesenborgs, Jori and Lamotte, Wim},
  year = {2018},
  month = sep,
  pages = {494--503},
  publisher = {{IEEE}},
  address = {{Belfast}},
  series = {{{CLUSTER}}'18}
}

@article{nemirovski_robust_2009,
  title = {Robust {{Stochastic Approximation Approach}} to {{Stochastic Programming}}},
  author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
  year = {2009},
  month = jan,
  volume = {19},
  pages = {1574--1609},
  journal = {SIAM J Optim.},
  keywords = {complexity,minimax problems,mirror descent algorithm,Monte Carlo sampling,saddle point,sample average approximation method,stochastic approximation,stochastic programming},
  number = {4}
}

@article{nesterov_primaldual_2009,
  title = {Primal-Dual Subgradient Methods for Convex Problems},
  author = {Nesterov, Yurii},
  year = {2009},
  month = aug,
  volume = {120},
  pages = {221--259},
  journal = {Math. Program.},
  language = {en},
  number = {1}
}

@inproceedings{NEURIPS2018_1cd138d0,
  title = {Variational Inference with Tail-Adaptive f-{{Divergence}}},
  booktitle = {Advances in Neural Information Processing Systems ({{NIPS}})},
  author = {Wang, Dilin and Liu, Hao and Liu, Qiang},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2018_25db67c5,
  title = {Importance Weighting and Variational Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Domke, Justin and Sheldon, Daniel R},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2018_584b98aa,
  title = {Meta-Learning {{MCMC}} Proposals},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Tongzhou and WU, YI and Moore, Dave and Russell, Stuart J},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2019_aec851e5,
  title = {Estimating Convergence of Markov Chains with L-Lag Couplings},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Biswas, Niloy and Jacob, Pierre E and Vanetti, Paul},
  year = {2019},
  volume = {32},
  pages = {7391--7401},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_10fb6cfa,
  title = {Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Letham, Ben and Calandra, Roberto and Rai, Akshara and Bakshy, Eytan},
  year = {2020},
  volume = {33},
  pages = {1546--1558},
  publisher = {{Curran Associates, Inc.}},
  series = {{{NeurIPS}}'20}
}

@inproceedings{NEURIPS2020_7880d722,
  title = {Gibbs Sampling with People},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Harrison, Peter and Marjieh, Raja and Adolfi, Federico and {van Rijn}, Pol and {Anglada-Tort}, Manuel and Tchernichovski, Ofer and {Larrouy-Maestri}, Pauline and Jacoby, Nori},
  year = {2020},
  volume = {33},
  pages = {10659--10671},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_7cac11e2,
  title = {Robust, Accurate Stochastic Optimization for Variational Inference},
  booktitle = {Advances in Neural Information Processing Systems ({{NeurIPS}})},
  author = {Dhaka, Akash Kumar and Catalina, Alejandro and Andersen, Michael R and Magnusson, M{\aa}ns and Huggins, Jonathan and Vehtari, Aki},
  year = {2020},
  volume = {33},
  pages = {10961--10973},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_b2070693,
  title = {Markovian Score Climbing: {{Variational}} Inference with {{KL(p||q)}}},
  booktitle = {Advances in Neural Information Processing Systems ({{NeurIPS}})},
  author = {Naesseth, Christian and Lindsten, Fredrik and Blei, David},
  year = {2020},
  volume = {33},
  pages = {15499--15510},
  publisher = {{Curran Associates, Inc.}}
}

@article{nguyen_efficient_2016,
  title = {Efficient {{Sequential Monte}}-{{Carlo Samplers}} for {{Bayesian Inference}}},
  author = {Nguyen, Thi Le Thu and Septier, Francois and Peters, Gareth W. and Delignon, Yves},
  year = {2016},
  month = mar,
  volume = {64},
  pages = {1305--1319},
  journal = {IEEE Trans. Signal Process.},
  number = {5}
}

@inproceedings{NIPS2007_89d4402d,
  title = {Markov Chain Monte Carlo with People},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Sanborn, Adam and Griffiths, Thomas},
  year = {2008},
  volume = {20},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2007_b6a1085a,
  title = {Active Preference Learning with Discrete Choice Data},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Brochu, Eric and {de Freitas}, Nando and Ghosh, Abhijeet},
  year = {2008},
  volume = {20},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2009_7cce53cf,
  title = {Dual Averaging Method for Regularized Stochastic Learning and Online Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xiao, Lin},
  year = {2009},
  volume = {22},
  pages = {2116--2124},
  publisher = {{Curran Associates, Inc.}},
  series = {{{NIPS}}'09}
}

@inproceedings{NIPS2013_7fe1f8ab,
  title = {Non-Strongly-Convex Smooth Stochastic Approximation with Convergence Rate {{O}}(1/n)},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bach, Francis and Moulines, Eric},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2014_5450,
  title = {Asynchronous Anytime Sequential Monte Carlo},
  booktitle = {Advances in Neural Information Processing Systems 27},
  author = {Paige, Brooks and Wood, Frank and Doucet, Arnaud and Teh, Yee Whye},
  year = {2014},
  pages = {3410--3418},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2014_8c6744c9,
  title = {Automated Variational Inference for {{Gaussian}} Process Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Nguyen, Trung V and Bonilla, Edwin V},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2015_5888,
  title = {Variational Consensus Monte Carlo},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}. 28},
  author = {Rabinovich, Maxim and Angelino, Elaine and Jordan, Michael I},
  year = {2015},
  pages = {1207--1215},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2015_5986,
  title = {Parallelizing {{MCMC}} with Random Partition Trees},
  booktitle = {Advances in Neural Information Processing Systems 28},
  author = {Wang, Xiangyu and Guo, Fangjian and Heller, Katherine A and Dunson, David B},
  year = {2015},
  pages = {451--459},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2016_7750ca35,
  title = {R\'enyi Divergence Variational Inference},
  booktitle = {Advances in Neural Information Processing Systems ({{NIPS}})},
  author = {Li, Yingzhen and Turner, Richard E},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2017_35464c84,
  title = {Variational Inference via {$\chi$} Upper Bound Minimization},
  booktitle = {Advances in Neural Information Processing Systems ({{NIPS}})},
  author = {Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},
  year = {2017},
  volume = {30},
  pages = {2729--2738},
  publisher = {{Curran Associates, Inc.}},
  address = {{Long Beach, California, USA}}
}

@inproceedings{NIPS2017_e91068ff,
  title = {Sticking the Landing: {{Simple}}, Lower-Variance Gradient Estimators for Variational Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David K},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2019_9107,
  title = {Sample Adaptive {{MCMC}}},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Zhu, Michael},
  year = {2019},
  pages = {9066--9077},
  publisher = {{Curran Associates, Inc.}}
}

@article{nishimura_recycling_2019,
  title = {Recycling Intermediate Steps to Improve {{Hamiltonian Monte Carlo}}},
  author = {Nishimura, Akihiko and Dunson, David},
  year = {2019},
  month = oct,
  file = {/home/msca8h/Zotero/storage/5XB75UY3/Nishimura and Dunson - 2019 - Recycling Intermediate Steps to Improve Hamiltonia.pdf},
  journal = {Bayesian Anal.},
  language = {en}
}

@article{niu_hogwild_2011,
  title = {{{HOGWILD}}!: {{A Lock}}-{{Free Approach}} to {{Parallelizing Stochastic Gradient Descent}}},
  shorttitle = {{{HOGWILD}}!},
  author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
  year = {2011},
  month = jun,
  abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
  archiveprefix = {arXiv},
  eprint = {1106.5730},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/6M96EILJ/Niu et al. - 2011 - HOGWILD! A Lock-Free Approach to Parallelizing St.pdf;/home/msca8h/Zotero/storage/AN85P4TC/1106.html},
  journal = {ArXiv11065730 Cs Math},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  primaryclass = {cs, math}
}

@book{nocedal_numerical_2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  publisher = {{Springer New York}},
  abstract = {Springer Series on Operations Research and Financial Engineering},
  file = {/home/msca8h/Zotero/storage/6J42ZVT8/2006 - Numerical Optimization.pdf},
  language = {en},
  series = {Springer {{Ser}}. {{Oper}}. {{Res}}. {{Financial Eng}}.}
}

@article{olivier_openmp_2012,
  title = {{{OpenMP}} Task Scheduling Strategies for Multicore {{NUMA}} Systems},
  author = {Olivier, S. L. and Porterfield, A. K. and Wheeler, K. B. and Spiegel, M. and Prins, J. F.},
  year = {2012},
  volume = {26},
  pages = {110--124},
  file = {/home/msca8h/Zotero/storage/VUXYUD89/OMP_scheduling_NUMA_analysis_2012.pdf},
  journal = {Int. J. High Perform. Comput. Appl.},
  number = {2}
}

@book{openmparchitecturereviewboard_openmp_2008,
  title = {{{OpenMP}} Application Program Interface Version 3.0},
  author = {OpenMP Architecture Review Board},
  year = {2008},
  month = may
}

@article{outtas_subjective_2018,
  title = {Subjective and Objective Evaluations of Feature Selected Multi Output Filter for Speckle Reduction on Ultrasound Images},
  author = {Outtas, M and Zhang, L and Deforges, O and Serir, A and Hamidouche, W and Chen, Y},
  year = {2018},
  month = sep,
  volume = {63},
  pages = {185014},
  file = {/home/msca8h/Zotero/storage/8Q3JEVUH/Outtas et al. - 2018 - Subjective and objective evaluations of feature se.pdf},
  journal = {Phys. Med. Biol.},
  number = {18}
}

@misc{owen_adaptive_,
  title = {Adaptive {{Importance Sampling}}},
  author = {Owen, Art},
  file = {/home/msca8h/Zotero/storage/XHV9NTDC/Owen - Adaptive Importance Sampling.pdf}
}

@article{owen_safe_2000,
  title = {Safe and Effective Importance Sampling},
  author = {Owen, Art and Zhou, Yi},
  year = {2000},
  month = mar,
  volume = {95},
  pages = {135--143},
  file = {/home/msca8h/Zotero/storage/J9UPP34N/Owen and Zhou - 2000 - Safe and effective importance sampling.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {449}
}

@article{paananen_implicitly_2021,
  title = {Implicitly Adaptive Importance Sampling},
  author = {Paananen, Topi and Piironen, Juho and B{\"u}rkner, Paul-Christian and Vehtari, Aki},
  year = {2021},
  month = mar,
  volume = {31},
  pages = {16},
  abstract = {Abstract             Adaptive importance sampling is a class of techniques for finding good proposal distributions for importance sampling. Often the proposal distributions are standard probability distributions whose parameters are adapted based on the mismatch between the current proposal and a target distribution. In this work, we present an implicit adaptive importance sampling method that applies to complicated distributions which are not available in closed form. The method iteratively matches the moments of a set of Monte Carlo draws to weighted moments based on importance weights. We apply the method to Bayesian leave-one-out cross-validation and show that it performs better than many existing parametric adaptive importance sampling methods while being computationally inexpensive.},
  file = {/home/msca8h/Zotero/storage/QVRDSELK/Paananen et al. - 2021 - Implicitly adaptive importance sampling.pdf},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@inproceedings{pan_fast_2006,
  title = {Fast, Automatic, Procedure-Level Performance Tuning},
  booktitle = {Proceedings of the 15th International Conference on {{Parallel}} Architectures and Compilation Techniques  - {{PACT}} '06},
  author = {Pan, Zhelong and Eigenmann, Rudolf},
  year = {2006},
  pages = {173},
  publisher = {{ACM Press}},
  address = {{Seattle, Washington, USA}},
  language = {en}
}

@article{papamarkou_challenges_2019,
  title = {Challenges in {{Bayesian}} Inference via {{Markov}} Chain {{Monte Carlo}} for Neural Networks},
  author = {Papamarkou, Theodore and Hinkle, Jacob and Young, M. Todd and Womble, David},
  year = {2019},
  month = nov,
  abstract = {Markov chain Monte Carlo (MCMC) methods and neural networks are instrumental in tackling inferential and prediction problems. However, Bayesian inference based on joint use of MCMC methods and of neural networks is limited. This paper reviews the main challenges posed by neural networks to MCMC developments, including lack of parameter identifiability due to weight symmetries, prior specification effects, and consequently high computational cost and convergence failure. Population and manifold MCMC algorithms are combined to demonstrate these challenges via multilayer perceptron (MLP) examples and to develop case studies for assessing the capacity of approximate inference methods to uncover the posterior covariance of neural network parameters. Some of these challenges, such as high computational cost arising from the application of neural networks to big data and parameter identifiability arising from weight symmetries, stimulate research towards more scalable approximate MCMC methods or towards MCMC methods in reduced parameter spaces.},
  archiveprefix = {arXiv},
  eprint = {1910.06539},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/3XLF57HA/Papamarkou et al. - 2019 - Challenges in Bayesian inference via Markov chain .pdf;/home/msca8h/Zotero/storage/BT3S5QT3/1910.html},
  journal = {ArXiv191006539 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {cs, stat}
}

@misc{papp_tpapp_2020,
  title = {Tpapp/{{DynamicHMC}}.Jl: V2.1.6},
  shorttitle = {Tpapp/{{DynamicHMC}}.Jl},
  author = {Papp, Tamas K. and JackRab and Dilum Aluthge and TagBot, Julia and Piibeleht, Morten},
  year = {2020},
  month = aug,
  abstract = {DynamicHMC v2.1.6 Diff since v2.1.5 {$<$}strong{$>$}Closed issues:{$<$}/strong{$>$} Struggling to Construct a working sampler (\#99) {$<$}strong{$>$}Merged pull requests:{$<$}/strong{$>$} A typo in the worked example (\#121) (@JackRab) a small issue in linking (\#122) (@JackRab) one redundant "them" (\#123) (@JackRab) CompatHelper: bump compat for "Optim" to "0.21" (\#125) (@github-actions[bot]) Travis CI: Build documentation on Julia 1.4 (\#126) (@DilumAluthge) CompatHelper: bump compat for "Optim" to "0.22" (\#129) (@github-actions[bot])},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@article{pasarica_adaptively_2007,
  title = {Adaptively {{Scaling}} the {{Metropolis Algorithm Using Expected Squared Jumped Distance}}},
  author = {Pasarica, Cristian and Gelman, Andrew},
  year = {2007},
  file = {/home/msca8h/Zotero/storage/KXWCXDVN/Gelman and Pasarica - 2007 - Adaptively Scaling the Metropolis Algorithm Using .pdf},
  journal = {SSRN Journal},
  language = {en}
}

@inproceedings{patel_communication_2019,
  title = {Communication Trade-Offs for Synchronized Distributed {{SGD}} with Large Step Size},
  booktitle = {{{arXiv}}:1904.11325 [Cs, Math, Stat]},
  author = {Patel, Kumar Kshitij and Dieuleveut, Aymeric},
  year = {2019},
  month = apr,
  abstract = {Synchronous mini-batch SGD is state-of-the-art for large-scale distributed machine learning. However, in practice, its convergence is bottlenecked by slow communication rounds between worker nodes. A natural solution to reduce communication is to use the \textbackslash emph\{`local-SGD'\} model in which the workers train their model independently and synchronize every once in a while. This algorithm improves the computation-communication trade-off but its convergence is not understood very well. We propose a non-asymptotic error analysis, which enables comparison to \textbackslash emph\{one-shot averaging\} i.e., a single communication round among independent workers, and \textbackslash emph\{mini-batch averaging\} i.e., communicating at every step. We also provide adaptive lower bounds on the communication frequency for large step-sizes (\$ t\^\{-\textbackslash alpha\} \$, \$ \textbackslash alpha\textbackslash in (1/2 , 1 ) \$) and show that \textbackslash emph\{Local-SGD\} reduces communication by a factor of \$O\textbackslash Big(\textbackslash frac\{\textbackslash sqrt\{T\}\}\{P\^\{3/2\}\}\textbackslash Big)\$, with \$T\$ the total number of gradients and \$P\$ machines.},
  archiveprefix = {arXiv},
  eprint = {1904.11325},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/VD3I2Y3T/Patel and Dieuleveut - 2019 - Communication trade-offs for synchronized distribu.pdf;/home/msca8h/Zotero/storage/IMFJ6K8X/1904.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@article{patil_pymc_2010,
  title = {{{PyMC}}: {{Bayesian}} Stochastic Modelling in {{Python}}},
  shorttitle = {{{{\textbf{PyMC}}}}},
  author = {Patil, Anand and Huard, David and Fonnesbeck, Christopher},
  year = {2010},
  volume = {35},
  file = {/home/msca8h/Zotero/storage/TNTW2PMU/Patil et al. - 2010 - PyMC  Bayesian Stochastic Modelling in .pdf},
  journal = {J. Stat. Soft.},
  language = {en},
  number = {4}
}

@article{patterson_improvement_1983,
  title = {The Improvement and Quantitative Assessment of {{B}}-Mode Images Produced by an Annular Array/Cone Hybrid},
  author = {Patterson, M. S. and Foster, F. S.},
  year = {1983},
  month = jul,
  volume = {5},
  pages = {195--213},
  abstract = {Hybrid ultrasound imaging systems, which combine spherical focusing on transmit with axicon focusing on receive, provide excellent resolution over a useful depth of field. This paper presents a new hybrid design with improved sensitivity, in which the axicon focusing is achieved by two conical mirrors and a PZT 5A disk cut into 8 sectors. We have investigated two methods of processing the signals from the 8 sectors. In the first, phase insensitive sector addition (PISA), the B-scan is formed from the sum of the 8 demodulated signals. In the second, multiplicative processing (MP), the 8 rf waveforms are multiplied and the resultant is demodulated to form the image. Both techniques result in smoothed speckle but degraded lateral resolution. As well, MP decreases the off-axis sensitivity of the system and artifacts characteristic of axicon focusing. Quantitative assessment of the effects of PISA and MP was performed using a new approach called contrast-to-speckle ratio (CSR). The CSR data, which is a measure of the image contrast of cylindrical voids in a random scattering medium relative to contrast fluctuations due to speckle, shows the superiority of PISA and MP. This conclusion is supported by images of in vitro human breast tissue.},
  journal = {Ultrason. Imaging},
  language = {en},
  number = {3}
}

@article{paulin_error_2019,
  title = {Error Bounds for Sequential {{Monte Carlo}} Samplers for Multimodal Distributions},
  author = {Paulin, Daniel and Jasra, Ajay and Thiery, Alexandre},
  year = {2019},
  month = feb,
  volume = {25},
  pages = {310--340},
  journal = {Bernoulli},
  number = {1}
}

@article{pearce_uncertainty_2020,
  title = {Uncertainty in {{Neural Networks}}: {{Approximately Bayesian Ensembling}}},
  shorttitle = {Uncertainty in {{Neural Networks}}},
  author = {Pearce, Tim and Leibfried, Felix and Brintrup, Alexandra and Zaki, Mohamed and Neely, Andy},
  year = {2020},
  month = feb,
  abstract = {Understanding the uncertainty of a neural network's (NN) predictions is essential for many purposes. The Bayesian framework provides a principled approach to this, however applying it to NNs is challenging due to large numbers of parameters and data. Ensembling NNs provides an easily implementable, scalable method for uncertainty quantification, however, it has been criticised for not being Bayesian. This work proposes one modification to the usual process that we argue does result in approximate Bayesian inference; regularising parameters about values drawn from a distribution which can be set equal to the prior. A theoretical analysis of the procedure in a simplified setting suggests the recovered posterior is centred correctly but tends to have an underestimated marginal variance, and overestimated correlation. However, two conditions can lead to exact recovery. We argue that these conditions are partially present in NNs. Empirical evaluations demonstrate it has an advantage over standard ensembling, and is competitive with variational methods.},
  archiveprefix = {arXiv},
  eprint = {1810.05546},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/3EUM8U35/Pearce et al. - 2020 - Uncertainty in Neural Networks Approximately Baye.pdf;/home/msca8h/Zotero/storage/479WDSFT/1810.html},
  journal = {ArXiv181005546 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{penna_binlpt_2017,
  title = {{{BinLPT}}: A Novel Workload-Aware Loop Scheduler for Irregular Parallel Loops},
  booktitle = {Proc. {{Simp\'osio}} Em {{Sistemas Computacionais}} de {{Alto Desempenho}}},
  author = {Penna, Pedro Henrique and Castro, M{\'a}rcio and Plentz, Patr{\'i}cia and {Cota de Freitas}, Henrique and Broquedis, Fran{\c c}ois and M{\'e}haut, Jean-Fran{\c c}ois},
  year = {2017},
  month = oct,
  address = {{Campinas, Brazil}},
  file = {/home/msca8h/Documents/parallel_scheduling/Penna et al. - 2017 - BinLPT A Novel Workload-Aware Loop Scheduler for .pdf}
}

@article{penna_comprehensive_2019,
  title = {A Comprehensive Performance Evaluation of the {{BinLPT}} Workload-Aware Loop Scheduler},
  shorttitle = {A Comprehensive Performance Evaluation of the {{BinLPT}} Workload-Aware Loop Scheduler},
  author = {Penna, Pedro Henrique and A. Gomes, Ant{\^o}nio Tadeu and Castro, M{\'a}rcio and D.M. Plentz, Patricia and C. Freitas, Henrique and Broquedis, Fran{\c c}ois and M{\'e}haut, Jean-Fran{\c c}ois},
  year = {2019},
  month = feb,
  file = {/home/msca8h/Zotero/storage/JAVWSZBZ/Penna et al. - 2019 - A comprehensive performance evaluation of the BinL.pdf},
  journal = {Concurrency and Computation: Pract. and Experience},
  language = {en}
}

@article{perona_scalespace_1990,
  title = {Scale-Space and Edge Detection Using Anisotropic Diffusion},
  author = {Perona, Pietro and Malik, Jitendra},
  year = {1990},
  month = jul,
  volume = {12},
  pages = {629--639},
  file = {/home/msca8h/Zotero/storage/TDXHR2UW/Perona and Malik - 1990 - Scale-space and edge detection using anisotropic d.pdf},
  journal = {IEEE Trans. Pattern Anal. Machine Intell.},
  number = {7}
}

@article{peskun_optimum_1973,
  title = {Optimum {{Monte}}-{{Carlo}} Sampling Using {{Markov}} Chains},
  author = {Peskun, P. H.},
  year = {1973},
  volume = {60},
  pages = {607--612},
  journal = {Biometrika},
  language = {en},
  number = {3}
}

@article{petrovic_benchmark_2020,
  title = {A Benchmark Set of Highly-Efficient {{CUDA}} and {{OpenCL}} Kernels and Its Dynamic Autotuning with {{Kernel Tuning Toolkit}}},
  author = {Petrovi{\v c}, Filip and St{\v r}el{\'a}k, David and Hozzov{\'a}, Jana and Ol'ha, Jaroslav and Trembeck{\'y}, Richard and Benkner, Siegfried and Filipovi{\v c}, Ji{\v r}{\'i}},
  year = {2020},
  month = jul,
  volume = {108},
  pages = {161--177},
  file = {/home/msca8h/Zotero/storage/H74CASUZ/Petrovič et al. - 2020 - A benchmark set of highly-efficient CUDA and OpenC.pdf},
  journal = {Future Generation Computer Systems},
  language = {en}
}

@inproceedings{pfaffe_efficient_2019,
  title = {Efficient Hierarchical Online-Autotuning: A Case Study on Polyhedral Accelerator Mapping},
  shorttitle = {Efficient Hierarchical Online-Autotuning},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Supercomputing}}  - {{ICS}} '19},
  author = {Pfaffe, Philip and Grosser, Tobias and Tillmann, Martin},
  year = {2019},
  pages = {354--366},
  publisher = {{ACM Press}},
  address = {{Phoenix, Arizona}},
  language = {en}
}

@inproceedings{pfaffe_onlineautotuning_2017,
  title = {Online-{{Autotuning}} in the {{Presence}} of {{Algorithmic Choice}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Pfaffe, Philip and Tillmann, Martin and Walter, Sigmar and Tichy, Walter F.},
  year = {2017},
  month = may,
  pages = {1379--1388},
  publisher = {{IEEE}},
  address = {{Orlando / Buena Vista, FL, USA}}
}

@inproceedings{pfander_autotunetmp_2018,
  title = {{{AutoTuneTMP}}: {{Auto}}-{{Tuning}} in {{C}}++ {{With Runtime Template Metaprogramming}}},
  shorttitle = {{{AutoTuneTMP}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Pfander, David and Brunn, Malte and Pfluger, Dirk},
  year = {2018},
  month = may,
  pages = {1123--1132},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}}
}

@article{pfister_transfer_2001,
  title = {The Transfer Function Bake-Off},
  author = {Pfister, H. and Lorensen, B. and Bajaj, C. and Kindlmann, G. and Schroeder, W. and Avila, L.S. and Raghu, K.M. and Machiraju, R. and {Jinho Lee}},
  year = {Jan.-Feb./2001},
  volume = {21},
  pages = {16--22},
  journal = {IEEE Comput. Grap. Appl.},
  number = {1}
}

@article{phan_color_2018,
  title = {Color Orchestra: {{Ordering}} Color Palettes for Interpolation and Prediction},
  shorttitle = {Color {{Orchestra}}},
  author = {Phan, Huy Q. and Fu, Hongbo and Chan, Antoni B.},
  year = {2018},
  month = jun,
  volume = {24},
  pages = {1942--1955},
  file = {/home/msca8h/Zotero/storage/GK6CW5LU/Phan et al. - 2018 - Color Orchestra Ordering Color Palettes for Inter.pdf},
  journal = {IEEE Trans. Visual. Comput. Graphics},
  number = {6}
}

@article{piironen_sparsity_2017,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  volume = {11},
  pages = {5018--5051},
  file = {/home/msca8h/Zotero/storage/8TTYNG46/Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf},
  journal = {Electron. J. Statist.},
  language = {en},
  number = {2}
}

@article{pizurica_versatile_2003,
  title = {A Versatile Wavelet Domain Noise Filtration Technique for Medical Imaging},
  author = {Pizurica, A. and Philips, W. and Lemahieu, I. and Acheroy, M.},
  year = {2003},
  month = mar,
  volume = {22},
  pages = {323--331},
  journal = {IEEE Trans. Med. Imaging},
  language = {en},
  number = {3}
}

@inproceedings{pmlr-v108-lorraine20a,
  title = {Optimizing Millions of Hyperparameters by Implicit Differentiation},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  author = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  year = {2020},
  month = aug,
  volume = {108},
  pages = {1540--1552},
  publisher = {{PMLR}},
  address = {{Online}},
  abstract = {We propose an algorithm for inexpensive gradient-based hyperparameter optimization that combines the implicit function theorem (IFT) with efficient inverse Hessian approximations. We present results about the relationship between the IFT and differentiating through optimization, motivating our algorithm. We use the proposed approach to train modern network architectures with millions of weights and millions of hyper-parameters. For example, we learn a data-augmentation network\textemdash where every weight is a hyperparameter tuned for validation performance\textemdash outputting augmented training examples. Jointly tuning weights and hyper-parameters is only a few times more costly in memory and compute than standard training.},
  pdf = {http://proceedings.mlr.press/v108/lorraine20a/lorraine20a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v118-xu20a,
  title = {{{AdvancedHMC}}.Jl: {{A}} Robust, Modular and Efficient Implementation of Advanced {{HMC}} Algorithms},
  booktitle = {Proceedings of {{The}} 2nd {{Symposium}} on  {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Xu, Kai and Ge, Hong and Tebbutt, Will and Tarek, Mohamed and Trapp, Martin and Ghahramani, Zoubin},
  year = {2020},
  month = dec,
  volume = {118},
  pages = {1--10},
  publisher = {{Proceedings of Machine Learning Research}},
  address = {{Vancouver, BC V6C 3B5, Canada}},
  abstract = {Stan's Hamilton Monte Carlo (HMC) has demonstrated remarkable sampling robustness and efficiency in a wide range of Bayesian inference problems through carefully crafted adaption schemes to the celebrated No-U-Turn sampler (NUTS) algorithm. It is challenging to implement these adaption schemes robustly in practice, hindering wider adoption amongst practitioners who are not directly working with the Stan modelling language. AdvancedHMC.jl (AHMC) contributes a modular, well-tested, standalone implementation of NUTS that recovers and extends Stan's NUTS algorithm. AHMC is written in Julia, a modern high-level language for scientic computing, benefoting from optional hardware acceleration and interoperability with a wealth of existing software written in both Julia and other languages, such as Python. Efficacy is demonstrated empirically by comparison with Stan through a third-party Markov chain Monte Carlo benchmarking suite.},
  pdf = {http://proceedings.mlr.press/v118/xu20a/xu20a.pdf},
  series = {{{PMLR}}}
}

@inproceedings{pmlr-v119-mikkola20a,
  title = {Projective Preferential {{Bayesian}} Optimization},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Mikkola, Petrus and Todorovi{\'c}, Milica and J{\"a}rvi, Jari and Rinke, Patrick and Kaski, Samuel},
  year = {2020},
  month = jul,
  volume = {119},
  pages = {6884--6892},
  publisher = {{ML Research Press}},
  abstract = {Bayesian optimization is an effective method for finding extrema of a black-box function. We propose a new type of Bayesian optimization for learning user preferences in high-dimensional spaces. The central assumption is that the underlying objective function cannot be evaluated directly, but instead a minimizer along a projection can be queried, which we call a projective preferential query. The form of the query allows for feedback that is natural for a human to give, and which enables interaction. This is demonstrated in a user experiment in which the user feedback comes in the form of optimal position and orientation of a molecule adsorbing to a surface. We demonstrate that our framework is able to find a global minimum of a high-dimensional black-box function, which is an infeasible task for existing preferential Bayesian optimization frameworks that are based on pairwise comparisons.},
  pdf = {http://proceedings.mlr.press/v119/mikkola20a/mikkola20a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v124-ou20a,
  title = {Joint Stochastic Approximation and Its Application to Learning Discrete Latent Variable Models},
  booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence ({{UAI}})},
  author = {Ou, Zhijian and Song, Yunfu},
  year = {2020},
  month = aug,
  volume = {124},
  pages = {929--938},
  publisher = {{ML Research Press}},
  abstract = {Although with progress in introducing auxiliary amortized inference models, learning discrete latent variable models is still challenging. In this paper, we show that the annoying difficulty of obtaining reliable stochastic gradients for the inference model and the drawback of indirectly optimizing the target log-likelihood can be gracefully addressed in a new method based on stochastic approximation (SA) theory of the Robbins-Monro type. Specifically, we propose to directly maximize the target log-likelihood and simultaneously minimize the inclusive divergence between the posterior and the inference model. The resulting learning algorithm is called joint SA (JSA). To the best of our knowledge, JSA represents the first method that couples an SA version of the EM (expectation-maximization) algorithm (SAEM) with an adaptive MCMC procedure. Experiments on several benchmark generative modeling and structured prediction tasks show that JSA consistently outperforms recent competitive algorithms, with faster convergence, better final likelihoods, and lower variance of gradient estimates.},
  pdf = {http://proceedings.mlr.press/v124/ou20a/ou20a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v139-campbell21a,
  title = {A Gradient Based Strategy for {{Hamiltonian Monte Carlo}} Hyperparameter Optimization},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Campbell, Andrew and Chen, Wenlong and Stimper, Vincent and {Hernandez-Lobato}, Jose Miguel and Zhang, Yichuan},
  year = {2021},
  month = jul,
  volume = {139},
  pages = {1238--1248},
  publisher = {{PMLR}},
  abstract = {Hamiltonian Monte Carlo (HMC) is one of the most successful sampling methods in machine learning. However, its performance is significantly affected by the choice of hyperparameter values. Existing approaches for optimizing the HMC hyperparameters either optimize a proxy for mixing speed or consider the HMC chain as an implicit variational distribution and optimize a tractable lower bound that can be very loose in practice. Instead, we propose to optimize an objective that quantifies directly the speed of convergence to the target distribution. Our objective can be easily optimized using stochastic gradient descent. We evaluate our proposed method and compare to baselines on a variety of problems including sampling from synthetic 2D distributions, reconstructing sparse signals, learning deep latent variable models and sampling molecular configurations from the Boltzmann distribution of a 22 atom molecule. We find that our method is competitive with or improves upon alternative baselines in all these experiments.},
  pdf = {http://proceedings.mlr.press/v139/campbell21a/campbell21a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v33-ranganath14,
  title = {Black Box Variational Inference},
  booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics ({{AISTATS}})},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
  year = {2014},
  month = apr,
  volume = {33},
  pages = {814--822},
  publisher = {{ML Research Press}},
  address = {{Reykjavik, Iceland}},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a ``black box'' variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  pdf = {http://proceedings.mlr.press/v33/ranganath14.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v37-kandasamy15,
  title = {High Dimensional Bayesian Optimisation and Bandits via Additive Models},
  author = {Kandasamy, Kirthevasan and Schneider, Jeff and Poczos, Barnabas},
  year = {2015},
  month = jul,
  volume = {37},
  pages = {295--304},
  publisher = {{PMLR}},
  address = {{Lille, France}},
  abstract = {Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.},
  pdf = {http://proceedings.mlr.press/v37/kandasamy15.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v37-salimans15,
  title = {Markov Chain {{Monte Carlo}} and Variational Inference: {{Bridging}} the Gap},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
  year = {2015},
  month = jul,
  volume = {37},
  pages = {1218--1226},
  publisher = {{PMLR}},
  address = {{Lille, France}},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  pdf = {http://proceedings.mlr.press/v37/salimans15.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v48-sa16,
  title = {Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling},
  booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
  author = {Sa, Christopher De and Re, Chris and Olukotun, Kunle},
  year = {2016},
  month = jun,
  volume = {48},
  pages = {1567--1576},
  publisher = {{PMLR}},
  address = {{New York, New York, USA}},
  abstract = {Gibbs sampling is a Markov chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: bias and mixing time. We show experimentally that our theoretical results match practical outcomes.},
  pdf = {http://proceedings.mlr.press/v48/sa16.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v51-basse16a,
  title = {Parallel Markov Chain Monte Carlo via Spectral Clustering},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  author = {Basse, Guillaume and Smith, Aaron and Pillai, Natesh},
  year = {2016},
  month = may,
  volume = {51},
  pages = {1318--1327},
  publisher = {{PMLR}},
  address = {{Cadiz, Spain}},
  abstract = {As it has become common to use many computer cores in routine applications, finding good ways to parallelize popular algorithms has become increasingly important. In this paper, we present a parallelization scheme for Markov chain Monte Carlo (MCMC) methods based on spectral clustering of the underlying state space, generalizing earlier work on parallelization of MCMC methods by state space partitioning. We show empirically that this approach speeds up MCMC sampling for multimodal distributions and that it can be usefully applied in greater generality than several related algorithms. Our algorithm converges under reasonable conditions to an `optimal' MCMC algorithm. We also show that our approach can be asymptotically far more efficient than naive parallelization, even in situations such as completely flat target distributions where no unique optimal algorithm exists. Finally, we combine theoretical and empirical bounds to provide practical guidance on the choice of tuning parameters.},
  pdf = {http://proceedings.mlr.press/v51/basse16a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v51-murray16,
  title = {Pseudo-Marginal Slice Sampling},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Artif}}. {{Intell}}. {{Statist}}.},
  author = {Murray, Iain and Graham, Matthew},
  year = {2016},
  month = may,
  volume = {51},
  pages = {911--919},
  publisher = {{ML Research Press}},
  address = {{Cadiz, Spain}},
  abstract = {Markov chain Monte Carlo (MCMC) methods asymptotically sample from complex probability distributions. The pseudo-marginal MCMC framework only requires an unbiased estimator of the unnormalized probability distribution function to construct a Markov chain. However, the resulting chains are harder to tune to a target distribution than conventional MCMC, and the types of updates available are limited. We describe a general way to clamp and update the random numbers used in a pseudo-marginal method's unbiased estimator. In this framework we can use slice sampling and other adaptive methods. We obtain more robust Markov chains, which often mix more quickly.},
  pdf = {http://proceedings.mlr.press/v51/murray16.pdf},
  series = {Proc. of {{Mach}}. {{Learn}}. {{Res}}.}
}

@inproceedings{pmlr-v54-gardner17a,
  title = {Discovering and Exploiting Additive Structure for Bayesian Optimization},
  author = {Gardner, Jacob and Guo, Chuan and Weinberger, Kilian and Garnett, Roman and Grosse, Roger},
  year = {2017},
  month = apr,
  volume = {54},
  pages = {1311--1319},
  publisher = {{ML Research Press}},
  address = {{Fort Lauderdale, FL, USA}},
  abstract = {Bayesian optimization has proven invaluable for black-box optimization of expensive functions. Its main limitation is its exponential complexity with respect to the dimensionality of the search space using typical kernels. Luckily, many objective functions can be decomposed into additive subproblems, which can be optimized independently. We investigate how to automatically discover such (typically unknown) additive structure while simultaneously exploiting it through Bayesian optimization. We propose an efficient algorithm based on Metropolis-Hastings sampling and demonstrate its efficacy empirically on synthetic and real-world data sets. Throughout all our experiments we reliably discover hidden additive structure whenever it exists and exploit it to yield significantly faster convergence.},
  pdf = {http://proceedings.mlr.press/v54/gardner17a/gardner17a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v70-gonzalez17a,
  title = {Preferential {{Bayesian}} Optimization},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Gonz{\'a}lez, Javier and Dai, Zhenwen and Damianou, Andreas and Lawrence, Neil D.},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {1282--1291},
  publisher = {{ML Research Press}},
  abstract = {Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimize black-box functions where direct queries of the objective are expensive. We consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as A/B tests or recommender systems. We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) and that allows to find the optimum of a latent function that can only be queried through pairwise comparisons, so-called duels. PBO extend the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the the winner of each duel by means of Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of experiments in which we show how the way correlations are modeled is the key ingredient to drastically reduce the number of comparisons to find the optimum of the latent function of interest.},
  pdf = {http://proceedings.mlr.press/v70/gonzalez17a/gonzalez17a.pdf},
  series = {Proc. {{Mach}}. {{Learn}}. {{Res}}.}
}

@inproceedings{pmlr-v70-hoffman17a,
  title = {Learning Deep Latent {{Gaussian}} Models with {{Markov}} Chain {{Monte Carlo}}},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Hoffman, Matthew D.},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {1510--1519},
  publisher = {{PMLR}},
  abstract = {Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC's additional computational overhead proves to be significant, but not prohibitive.},
  pdf = {http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v89-paananen19a,
  title = {Variable Selection for {{Gaussian}} Processes via Sensitivity Analysis of the Posterior Predictive Distribution},
  booktitle = {Proceedings of the {{Twenty}}-{{Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Paananen, Topi and Piironen, Juho and Andersen, Michael Riis and Vehtari, Aki},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {1743--1752},
  publisher = {{ML Research Press}},
  abstract = {Variable selection for Gaussian process models is often done using automatic relevance determination, which uses the inverse length-scale parameter of each input variable as a proxy for variable relevance. This implicitly determined relevance has several drawbacks that prevent the selection of optimal input variables in terms of predictive performance. To improve on this, we propose two novel variable selection methods for Gaussian process models that utilize the predictions of a full model in the vicinity of the training points and thereby rank the variables based on their predictive relevance. Our empirical results on synthetic and real world data sets demonstrate improved variable selection compared to automatic relevance determination in terms of variability and predictive performance.},
  pdf = {http://proceedings.mlr.press/v89/paananen19a/paananen19a.pdf},
  series = {Proceedings of {{Machine Learning Research}}}
}

@inproceedings{pmlr-v97-cornish19a,
  title = {Scalable {{Metropolis}}-{{Hastings}} for Exact {{Bayesian}} Inference with Large Datasets},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Cornish, Rob and Vanetti, Paul and {Bouchard-Cote}, Alexandre and Deligiannidis, George and Doucet, Arnaud},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {1351--1360},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metropolis-Hastings is too computationally intensive to handle large datasets, since the cost per step usually scales like O(n) in the number of data points n. We propose the Scalable Metropolis-Hastings (SMH) kernel that only requires processing on average O(1) or even O(1/n) data points per step. This scheme is based on a combination of factorized acceptance probabilities, procedures for fast simulation of Bernoulli processes, and control variate ideas. Contrary to many MCMC subsampling schemes such as fixed step-size Stochastic Gradient Langevin Dynamics, our approach is exact insofar as the invariant distribution is the true posterior and not an approximation to it. We characterise the performance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by empirical results that demonstrate overall performance benefits over standard Metropolis-Hastings and various subsampling algorithms.},
  pdf = {http://proceedings.mlr.press/v97/cornish19a/cornish19a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v97-ruiz19a,
  title = {A Contrastive Divergence for Combining Variational Inference and {{MCMC}}},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning ({{ICML}})},
  author = {Ruiz, Francisco and Titsias, Michalis},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {5537--5545},
  publisher = {{ML Research Press}},
  abstract = {We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).},
  pdf = {http://proceedings.mlr.press/v97/ruiz19a/ruiz19a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{polyak_acceleration_1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  author = {Polyak, B. T. and Juditsky, A. B.},
  year = {1992},
  month = jul,
  volume = {30},
  pages = {838--855},
  journal = {SIAM J. Control Optim.},
  language = {en},
  number = {4}
}

@article{polychronopoulos_guided_1987,
  title = {Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers},
  author = {Polychronopoulos, Constantine D. and Kuck, David J.},
  year = {1987},
  month = dec,
  volume = {C-36},
  pages = {1425--1439},
  file = {/home/msca8h/Documents/parallel_scheduling/Polychronopoulos and Kuck - 1987 - Guided Self-Scheduling A Practical Scheduling Sch.pdf},
  journal = {IEEE Trans. Comput.},
  number = {12}
}

@inproceedings{popov_efficient_2019,
  title = {Efficient Thread/Page/Parallelism Autotuning for {{NUMA}} Systems},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Supercomputing}}  - {{ICS}} '19},
  author = {Popov, Mihail and Jimborean, Alexandra and {Black-Schaffer}, David},
  year = {2019},
  pages = {342--353},
  publisher = {{ACM Press}},
  address = {{Phoenix, Arizona}},
  file = {/home/msca8h/Zotero/storage/S6V5F46P/Popov et al. - 2019 - Efficient threadpageparallelism autotuning for N.pdf},
  language = {en}
}

@inproceedings{popov_efficient_2019a,
  title = {Efficient Thread/Page/Parallelism Autotuning for {{NUMA}} Systems},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Supercomputing}}  - {{ICS}} '19},
  author = {Popov, Mihail and Jimborean, Alexandra and {Black-Schaffer}, David},
  year = {2019},
  pages = {342--353},
  publisher = {{ACM Press}},
  address = {{Phoenix, Arizona}},
  file = {/home/msca8h/Zotero/storage/B3KEZ5XT/Popov et al. - 2019 - Efficient threadpageparallelism autotuning for N.pdf},
  language = {en}
}

@article{popov_piecewise_2017,
  title = {Piecewise Holistic Autotuning of Parallel Programs with {{CERE}}: {{Piecewise Holistic Autotuning}} of {{Parallel Programs}} with {{CERE}}},
  shorttitle = {Piecewise Holistic Autotuning of Parallel Programs with {{CERE}}},
  author = {Popov, Mihail and Akel, Chadi and Chatelain, Yohan and Jalby, William and {de Oliveira Castro}, Pablo},
  year = {2017},
  month = aug,
  volume = {29},
  pages = {e4190},
  file = {/home/msca8h/Zotero/storage/L8ZWQ7AB/Popov et al. - 2017 - Piecewise holistic autotuning of parallel programs.pdf},
  journal = {Concurrency Computat.: Pract. Exper.},
  language = {en},
  number = {15}
}

@article{PS_2001__5__183_0,
  title = {Chernoff and {{Berry}}-{{Ess\'een}} Inequalities for {{Markov}} Processes},
  author = {Lezaud, Pascal},
  year = {2001},
  volume = {5},
  pages = {183--201},
  publisher = {{EDP-Sciences}},
  journal = {ESAIM Probab. Stat.},
  language = {English},
  zmnumber = {0998.60075}
}

@article{qawasmeh_adaptive_2015,
  title = {Adaptive {{OpenMP Task Scheduling Using Runtime APIs}} and {{Machine Learning}}},
  author = {Qawasmeh, A. and Malik, A. M. and Chapman, B. M.},
  year = {2015},
  file = {/home/msca8h/Zotero/storage/4YJTTYPL/ML_scheduling_runtime_API_OMP.pdf},
  journal = {2015 IEEE 14th Int. Conf. Mach. Learn. Appl. ICMLA}
}

@article{qiao_rethinking_2019,
  title = {Rethinking {{Normalization}} and {{Elimination Singularity}} in {{Neural Networks}}},
  author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  year = {2019},
  month = nov,
  abstract = {In this paper, we study normalization methods for neural networks from the perspective of elimination singularity. Elimination singularities correspond to the points on the training trajectory where neurons become consistently deactivated. They cause degenerate manifolds in the loss landscape which will slow down training and harm model performances. We show that channel-based normalizations (e.g. Layer Normalization and Group Normalization) are unable to guarantee a far distance from elimination singularities, in contrast with Batch Normalization which by design avoids models from getting too close to them. To address this issue, we propose BatchChannel Normalization (BCN), which uses batch knowledge to avoid the elimination singularities in the training of channel-normalized models. Unlike Batch Normalization, BCN is able to run in both large-batch and micro-batch training settings. The effectiveness of BCN is verified on many tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is here: https://github.com/joe-siyuan-qiao/Batch-Channel-Normalization.},
  archiveprefix = {arXiv},
  eprint = {1911.09738},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/37AYN8YU/Qiao et al. - 2019 - Rethinking Normalization and Elimination Singulari.pdf;/home/msca8h/Zotero/storage/GXRT5VMS/1911.html},
  journal = {ArXiv191109738 Cs},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{qiao_weight_2019,
  title = {Weight {{Standardization}}},
  author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  year = {2019},
  month = mar,
  abstract = {In this paper, we propose Weight Standardization (WS) to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The micro-batch training setting is hard because small batch sizes are not enough for training networks with Batch Normalization (BN), while other normalization methods that do not rely on batch knowledge still have difficulty matching the performances of BN in large-batch training. Our WS ends this problem because when used with Group Normalization and trained with 1 image/GPU, WS is able to match or outperform the performances of BN trained with large batch sizes with only 2 more lines of code. In micro-batch training, WS significantly outperforms other normalization methods. WS achieves these superior results by standardizing the weights in the convolutional layers, which we show is able to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients. The effectiveness of WS is verified on many tasks, including image classification, object detection, instance segmentation, video recognition, semantic segmentation, and point cloud recognition. The code is available here: https://github.com/joe-siyuan-qiao/WeightStandardization.},
  archiveprefix = {arXiv},
  eprint = {1903.10520},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/PRUPGAMQ/Qiao et al. - 2019 - Weight Standardization.pdf;/home/msca8h/Zotero/storage/DDJ5L7H2/1903.html},
  journal = {ArXiv190310520 Cs},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@incollection{quinonero-candela_evaluating_2006,
  title = {Evaluating {{Predictive Uncertainty Challenge}}},
  booktitle = {Machine {{Learning Challenges}}. {{Evaluating Predictive Uncertainty}}, {{Visual Object Classification}}, and {{Recognising Tectual Entailment}}},
  author = {{Qui{\~n}onero-Candela}, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  year = {2006},
  volume = {3944},
  pages = {1--27},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Zotero/storage/7PH3YBKZ/Quiñonero-Candela et al. - 2006 - Evaluating Predictive Uncertainty Challenge.pdf}
}

@article{radivojevic_modified_2020,
  title = {Modified {{Hamiltonian Monte Carlo}} for {{Bayesian}} Inference},
  author = {Radivojevi{\'c}, Tijana and Akhmatskaya, Elena},
  year = {2020},
  month = mar,
  volume = {30},
  pages = {377--404},
  journal = {Stat Comput},
  language = {en},
  number = {2}
}

@inproceedings{radojkovic_optimal_2012,
  title = {Optimal Task Assignment in Multithreaded Processors: A Statistical Approach},
  shorttitle = {Optimal Task Assignment in Multithreaded Processors},
  booktitle = {Proceedings of the Seventeenth International Conference on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}} - {{ASPLOS}} '12},
  author = {Radojkovi{\'c}, Petar and {\v C}akarevi{\'c}, Vladimir and Moret{\'o}, Miquel and Verd{\'u}, Javier and Pajuelo, Alex and Cazorla, Francisco J. and Nemirovsky, Mario and Valero, Mateo},
  year = {2012},
  pages = {235},
  publisher = {{ACM Press}},
  address = {{London, England, UK}},
  language = {en}
}

@article{radojkovic_thread_2016,
  title = {Thread {{Assignment}} in {{Multicore}}/{{Multithreaded Processors}}: {{A Statistical Approach}}},
  shorttitle = {Thread {{Assignment}} in {{Multicore}}/{{Multithreaded Processors}}},
  author = {Radojkovic, Petar and Carpenter, Paul M. and Moreto, Miquel and Cakarevic, Vladimir and Verdu, Javier and Pajuelo, Alex and Cazorla, Francisco J. and Nemirovsky, Mario and Valero, Mateo},
  year = {2016},
  month = jan,
  volume = {65},
  pages = {256--269},
  file = {/home/msca8h/Zotero/storage/PX57CA6I/Radojkovic et al. - 2016 - Thread Assignment in MulticoreMultithreaded Proce.pdf},
  journal = {IEEE Trans. Comput.},
  number = {1}
}

@incollection{rahimi_random_2008,
  title = {Random {{Features}} for {{Large}}-{{Scale Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 20},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2008},
  pages = {1177--1184},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{ramalingam_improving_2012,
  title = {Improving {{High}}-{{Performance Sparse Libraries Using Compiler}}-{{Assisted Specialization}}: {{A PETSc Case Study}}},
  shorttitle = {Improving {{High}}-{{Performance Sparse Libraries Using Compiler}}-{{Assisted Specialization}}},
  booktitle = {2012 {{IEEE}} 26th {{International Parallel}} and {{Distributed Processing Symposium Workshops}} \& {{PhD Forum}}},
  author = {Ramalingam, Shreyas and Hall, Mary and Chen, Chun},
  year = {2012},
  month = may,
  pages = {487--496},
  publisher = {{IEEE}},
  address = {{Shanghai, China}}
}

@article{ramos-llorden_anisotropic_2015,
  title = {Anisotropic Diffusion Filter with Memory Based on Speckle Statistics for Ultrasound Images},
  author = {{Ramos-Llorden}, Gabriel and {Vegas-Sanchez-Ferrero}, Gonzalo and {Martin-Fernandez}, Marcos and {Alberola-Lopez}, Carlos and {Aja-Fernandez}, Santiago},
  year = {2015},
  month = jan,
  volume = {24},
  pages = {345--358},
  file = {/home/msca8h/Zotero/storage/D4C6GGYB/Ramos-Llorden et al. - 2015 - Anisotropic Diffusion Filter With Memory Based on .pdf},
  journal = {IEEE Trans. Image Process.},
  number = {1}
}

@inproceedings{rashid_investigating_2008,
  title = {Investigating a {{Dynamic Loop Scheduling}} with {{Reinforcement Learning Approach}} to {{Load Balancing}} in {{Scientific Applications}}},
  booktitle = {2008 {{International Symposium}} on {{Parallel}} and {{Distributed Computing}}},
  author = {Rashid, Mahbubur and Banicescu, Ioana and Carino, Ricolindo L.},
  year = {2008},
  pages = {123--130},
  publisher = {{IEEE}},
  address = {{Krakow, Poland}}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  annotation = {OCLC: ocm61285753},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  lccn = {QA274.4 .R37 2006},
  series = {Adaptive {{Comput}}. {{Mach}}. {{Learn}}.}
}

@incollection{rasmussen_occam_2001,
  title = {Occam's Razor},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}. 13},
  author = {Rasmussen, Carl Edward and Ghahramani, Zoubin},
  year = {2001},
  pages = {294--300},
  publisher = {{MIT Press}},
  series = {{{NIPS}}'13}
}

@inproceedings{regier_cataloging_2018,
  title = {Cataloging the Visible Universe through {{Bayesian}} Inference at Petascale},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}.},
  author = {Regier, Jeffrey and Pamnany, Kiran and Fischer, Keno and Noack, Andreas and Lam, Maximilian and Revels, Jarrett and Howard, Steve and Giordano, Ryan and Schlegel, David and McAuliffe, Jon and Thomas, Rollin C. and {Prabhat}},
  year = {2018},
  pages = {44--53},
  series = {{{IPDPS}}'18}
}

@inproceedings{regier_cataloging_2018a,
  title = {Cataloging the {{Visible Universe Through Bayesian Inference}} at {{Petascale}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Regier, Jeffrey and McAuliffe, Jon and Thomas, Rollin and Prabhat, . and Pamnany, Kiran and Fischer, Keno and Noack, Andreas and Lam, Maximilian and Revels, Jarrett and Howard, Steve and Giordano, Ryan and Schlegel, David},
  year = {2018},
  month = may,
  pages = {44--53},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}},
  file = {/home/msca8h/Zotero/storage/RDV8WJIS/Regier et al. - 2018 - Cataloging the Visible Universe Through Bayesian I.pdf}
}

@article{richardson_algorithm_1973,
  title = {Algorithm 454: {{The Complex Method}} for {{Constrained Optimization}} [{{E4}}]},
  author = {Richardson, Joel A. and Kuester, J. L.},
  year = {1973},
  month = aug,
  volume = {16},
  pages = {487--489},
  journal = {Commun ACM},
  keywords = {Box's algorithm,constrained optimization,optimization},
  number = {8}
}

@article{ridgeway_sequential_2003,
  title = {A {{Sequential Monte Carlo Method}} for {{Bayesian Analysis}} of {{Massive Datasets}}},
  author = {Ridgeway, Greg},
  year = {2003},
  volume = {7},
  pages = {301--319},
  file = {/home/msca8h/Zotero/storage/TL9E3XMP/Ridgeway - 2003 - [No title found].pdf},
  journal = {Data Min. Knowl. Discov.},
  number = {3}
}

@article{rindal_effect_2019,
  title = {The Effect of Dynamic Range Alterations in the Estimation of Contrast},
  author = {Rindal, Ole Marius Hoel and Austeng, Andreas and Fatemi, Ali and {Rodriguez-Molares}, Alfonso},
  year = {2019},
  month = jul,
  volume = {66},
  pages = {1198--1208},
  file = {/home/msca8h/Zotero/storage/EDKMDA9Y/Rindal et al. - 2019 - The Effect of Dynamic Range Alterations in the Est.pdf},
  journal = {IEEE Trans. Ultrason., Ferroelect., Freq. Contr.},
  number = {7}
}

@article{robbins_stochastic_1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  month = sep,
  volume = {22},
  pages = {400--407},
  file = {/home/msca8h/Zotero/storage/KZ43C2HW/Robbins and Monro - 1951 - A Stochastic Approximation Method.pdf},
  journal = {Ann. Math. Statist.},
  language = {en},
  number = {3}
}

@article{robert_accelerating_2018,
  title = {Accelerating {{MCMC}} Algorithms},
  author = {Robert, Christian P. and Elvira, V{\'i}ctor and Tawn, Nick and Wu, Changye},
  year = {2018},
  month = sep,
  volume = {10},
  pages = {e1435},
  file = {/home/msca8h/Zotero/storage/96HN8W6A/Robert et al. - 2018 - Accelerating MCMC algorithms.pdf},
  journal = {Wiley Interdisciplinary Rev: Comput. Statist.},
  language = {en},
  number = {5}
}

@book{robert_monte_2004,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  file = {/home/msca8h/Zotero/storage/3R9A4UZ5/Robert and Casella - 2004 - Monte Carlo Statistical Methods.pdf},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@article{robert_raoblackwellization_2021,
  title = {Rao-{{Blackwellization}} in the {{MCMC}} Era},
  author = {Robert, Christian P. and Roberts, Gareth O.},
  year = {2021},
  month = jan,
  abstract = {Rao-Blackwellization is a notion often occurring in the MCMC literature, with possibly different meanings and connections with the original Rao--Blackwell theorem (Rao, 1945 and Blackwell,1947), including a reduction of the variance of the resulting Monte Carlo approximations. This survey reviews some of the meanings of the term.},
  archiveprefix = {arXiv},
  eprint = {2101.01011},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/HXK42AV8/Robert and Roberts - 2021 - Rao-Blackwellization in the MCMC era.pdf;/home/msca8h/Zotero/storage/F5JZSIJZ/2101.html},
  journal = {ArXiv210101011 Math Stat},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  primaryclass = {math, stat}
}

@article{roberts_exponential_1996,
  title = {Exponential Convergence of {{Langevin}} Distributions and Their Discrete Approximations},
  author = {Roberts, Gareth O. and Tweedie, Richard L.},
  year = {1996},
  month = dec,
  volume = {2},
  pages = {341--363},
  journal = {Bernoulli},
  number = {4}
}

@article{roberts_langevin_2002,
  title = {Langevin {{Diffusions}} and {{Metropolis}}-{{Hastings Algorithms}}},
  author = {Roberts, G. O. and Stramer, O.},
  year = {2002},
  volume = {4},
  pages = {337--357},
  journal = {Methodol. Comput. Appl. Probab.},
  number = {4}
}

@article{rodriguez-molares_generalized_2020,
  title = {The Generalized Contrast-to-Noise Ratio: {{A}} Formal Definition for Lesion Detectability},
  shorttitle = {The {{Generalized Contrast}}-to-{{Noise Ratio}}},
  author = {{Rodriguez-Molares}, Alfonso and Rindal, Ole Marius Hoel and D'hooge, Jan and Masoy, Svein-Erik and Austeng, Andreas and Lediju Bell, Muyinatu A. and Torp, Hans},
  year = {2020},
  month = apr,
  volume = {67},
  pages = {745--759},
  file = {/home/msca8h/Zotero/storage/I4CXGAUW/Rodriguez-Molares et al. - 2020 - The Generalized Contrast-to-Noise Ratio A Formal .pdf},
  journal = {IEEE Trans. Ultrason., Ferroelect. Freq. Contr.},
  number = {4}
}

@incollection{rosenthal_optimal_,
  title = {Optimal Proposal Distributions and Adaptive {{MCMC}}},
  booktitle = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Rosenthal, Jeffrey S},
  publisher = {{Chapman and Hall/CRC}}
}

@article{Rosenthal99parallelcomputing,
  title = {Parallel Computing and Monte Carlo Algorithms},
  author = {Rosenthal, Jeffrey S.},
  year = {1999},
  volume = {4},
  pages = {207--236},
  journal = {Far East J. Theor. Stat.}
}

@inproceedings{roy_exploiting_2016,
  title = {Exploiting {{Performance Portability}} in {{Search Algorithms}} for {{Autotuning}}},
  booktitle = {2016 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Roy, Amit and Balaprakash, Prasanna and Hovland, Paul D. and Wild, Stefan M.},
  year = {2016},
  month = may,
  pages = {1535--1544},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}}
}

@inproceedings{ru_fast_2018,
  title = {Fast {{Information}}-Theoretic {{Bayesian Optimisation}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Ru, Binxin and Osborne, Michael A. and Mcleod, Mark and Granziol, Diego},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {4384--4392},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm Sweden}},
  abstract = {Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with information-theoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{rubin_estimation_1981,
  title = {Estimation in {{Parallel Randomized Experiments}}},
  author = {Rubin, Donald B.},
  year = {1981},
  volume = {6},
  pages = {377},
  journal = {Journal of Educational Statistics},
  number = {4}
}

@article{rudolf_error_2010,
  title = {Error Bounds for Computing the Expectation by {{Markov}} Chain {{Monte Carlo}}},
  author = {Rudolf, Daniel},
  year = {2010},
  month = jan,
  volume = {16},
  file = {/home/msca8h/Zotero/storage/GC33VBEB/Rudolf - 2010 - Error bounds for computing the expectation by Mark.pdf},
  journal = {Monte Carlo Methods Appl.},
  number = {3-4}
}

@techreport{ruppert_efficient_1988,
  title = {Efficient Estimations from a Slowly Convergent {{Robbins}}-{{Monro}} Process},
  author = {Ruppert, David},
  year = {1988},
  month = feb,
  address = {{Ithaca, New York}},
  institution = {{Cornell University School of Operations Research and Industrial Engineering}},
  number = {781},
  type = {Technical {{Report}}}
}

@inproceedings{rusira_automating_2017,
  title = {Automating {{Compiler}}-{{Directed Autotuning}} for {{Phased Performance Behavior}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Rusira, Tharindu and Hall, Mary and Basu, Protonu},
  year = {2017},
  month = may,
  pages = {1362--1371},
  publisher = {{IEEE}},
  address = {{Orlando / Buena Vista, FL, USA}}
}

@incollection{salimans_weight_2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Salimans, Tim and Kingma, Durk P},
  year = {2016},
  pages = {901--909},
  publisher = {{Curran Associates, Inc.}}
}

@article{salvatier_probabilistic_2016,
  title = {Probabilistic Programming in {{Python}} Using {{PyMC3}}},
  author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  year = {2016},
  month = apr,
  volume = {2},
  pages = {e55},
  abstract = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
  file = {/home/msca8h/Zotero/storage/EL4ML6D5/Salvatier et al. - 2016 - Probabilistic programming in Python using PyMC3.pdf},
  journal = {PeerJ Comput. Sci.},
  language = {en}
}

@article{sameni_multichannel_2007,
  title = {Multichannel {{ECG}} and Noise Modeling: {{Application}} to Maternal and Fetal {{ECG}} Signals},
  shorttitle = {Multichannel {{ECG}} and {{Noise Modeling}}},
  author = {Sameni, Reza and Clifford, Gari D and Jutten, Christian and Shamsollahi, Mohammad B},
  year = {2007},
  month = dec,
  volume = {2007},
  pages = {043407},
  file = {/home/msca8h/Zotero/storage/ANV8WCTZ/Sameni et al. - 2007 - Multichannel ECG and Noise Modeling Application t.pdf},
  journal = {EURASIP J. Adv. Signal Process.},
  language = {en},
  number = {1}
}

@article{sameni_nonlinear_2007,
  title = {A Nonlinear {{Bayesian}} Filtering Framework for {{ECG}} Denoising},
  author = {Sameni, R. and Shamsollahi, M.B. and Jutten, C. and Clifford, G.D.},
  year = {2007},
  month = dec,
  volume = {54},
  pages = {2172--2185},
  file = {/home/msca8h/Zotero/storage/I7HLG7N8/Sameni et al. - 2007 - A Nonlinear Bayesian Filtering Framework for ECG D.pdf},
  journal = {IEEE Trans. Biomed. Eng.},
  number = {12}
}

@inproceedings{sanders_aces4_2017,
  title = {Aces4: A Platform for Computational Chemistry Calculations with Extremely Large Block-Sparse Arrays},
  shorttitle = {Aces4},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Sanders, Beverly A. and Byrd, Jason N. and Jindal, Nakul and Lotrich, Victor F. and Lyakh, Dmitry and Perera, Ajith and Bartlett, Rodney J.},
  year = {2017},
  month = may,
  pages = {555--564},
  publisher = {{IEEE}},
  address = {{Orlando, FL, USA}}
}

@inproceedings{santos_bayesian_2010,
  title = {Bayesian Optimization of Perfusion and Transit Time Estimation in {{PASL}}-{{MRI}}},
  booktitle = {2010 {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology}}},
  author = {Santos, N and Sanches, J and Figueiredo, P},
  year = {2010},
  month = aug,
  pages = {4284--4287},
  publisher = {{IEEE}},
  address = {{Buenos Aires}}
}

@incollection{santurkar_how_2018,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  year = {2018},
  pages = {2483--2493},
  publisher = {{Curran Associates, Inc.}}
}

@article{sason_divergence_2016,
  title = {\$f\$ -{{Divergence Inequalities}}},
  author = {Sason, Igal and Verdu, Sergio},
  year = {2016},
  month = nov,
  volume = {62},
  pages = {5973--6006},
  file = {/home/msca8h/Zotero/storage/C25KSZ2R/Sason and Verdu - 2016 - $f$ -Divergence Inequalities.pdf},
  journal = {IEEE Trans. Inform. Theory},
  number = {11}
}

@article{sato_autotuning_2019,
  title = {An {{Autotuning Framework}} for {{Scalable Execution}} of {{Tiled Code}} via {{Iterative Polyhedral Compilation}}},
  author = {Sato, Yukinori and Yuki, Tomoya and Endo, Toshio},
  year = {2019},
  month = jan,
  volume = {15},
  pages = {1--23},
  file = {/home/msca8h/Zotero/storage/U7QYJSG2/Sato et al. - 2019 - An Autotuning Framework for Scalable Execution of .pdf},
  journal = {ACM Trans. Archit. Code Optim.},
  language = {en},
  number = {4}
}

@article{sattar_image_1997,
  title = {Image Enhancement Based on a Nonlinear Multiscale Method},
  author = {Sattar, F. and Floreby, L. and Salomonsson, G. and Lovstrom, B.},
  year = {1997},
  month = jun,
  volume = {6},
  pages = {888--895},
  journal = {IEEE Trans. Image Process.},
  number = {6}
}

@article{savage_theory_1951,
  title = {The {{Theory}} of {{Statistical Decision}}},
  author = {Savage, L. J.},
  year = {1951},
  month = mar,
  volume = {46},
  pages = {55--67},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {253}
}

@article{savitsky_variable_2011,
  title = {Variable Selection for Nonparametric {{Gaussian}} Process Priors: Models and Computational Strategies},
  shorttitle = {Variable {{Selection}} for {{Nonparametric Gaussian Process Priors}}},
  author = {Savitsky, Terrance and Vannucci, Marina and Sha, Naijun},
  year = {2011},
  month = feb,
  volume = {26},
  pages = {130--149},
  file = {/home/msca8h/Zotero/storage/UGAGTGRJ/Savitsky et al. - 2011 - Variable Selection for Nonparametric Gaussian Proc.pdf},
  journal = {Statist. Sci.},
  language = {en},
  number = {1}
}

@inproceedings{saxe_random_2011,
  title = {On {{Random Weights}} and {{Unsupervised Feature Learning}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Saxe, Andrew M. and Koh, Pang Wei and Chen, Zhenghao and Bhand, Maneesh and Suresh, Bipin and Ng, Andrew Y.},
  year = {2011},
  pages = {1089--1096},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  series = {{{ICML}}'11}
}

@article{schulman_proximal_2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/3R9RHEXE/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/msca8h/Zotero/storage/D9TET6JR/1707.html},
  journal = {ArXiv170706347 Cs},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@inproceedings{seelam_extreme_2010,
  title = {Extreme Scale Computing: {{Modeling}} the Impact of System Noise in Multicore Clustered Systems},
  shorttitle = {Extreme Scale Computing},
  booktitle = {2010 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}} ({{IPDPS}})},
  author = {Seelam, Seetharami and Fong, Liana and Tantawi, Asser and Lewars, John and Divirgilio, John and Gildea, Kevin},
  year = {2010},
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  file = {/home/msca8h/Documents/parallel_scheduling/Seelam et al. - 2010 - Extreme scale computing Modeling the impact of sy.pdf}
}

@inproceedings{sehgal_deep_2019,
  title = {Deep {{Reinforcement Learning Using Genetic Algorithm}} for {{Parameter Optimization}}},
  booktitle = {2019 {{Third IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  author = {Sehgal, Adarsh and La, Hung and Louis, Sushil and Nguyen, Hai},
  year = {2019},
  month = feb,
  pages = {596--601},
  publisher = {{IEEE}},
  address = {{Naples, Italy}}
}

@techreport{shaby_exploring_2010,
  title = {Exploring an Adaptive {{Metropolis}} Algorithm},
  author = {Shaby, Benjamin and Wells, Martin},
  year = {2010},
  institution = {{DukeUniversity Department of Stastical Science}},
  number = {1011-14},
  type = {Technical {{Report}}}
}

@inproceedings{shah_bayesian_2013,
  title = {Bayesian {{Optimization}} Using {{Student}}-t {{Processes}}},
  booktitle = {{{NIPS Workshop}} on {{Bayesian Optimisation}}},
  author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2013}
}

@inproceedings{shah_bayesian_2013a,
  title = {Bayesian Optimization Using {{Student}}-t Processes},
  booktitle = {{{NIPS Workshop}} on {{Bayesian Optim}}.},
  author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2013}
}

@inproceedings{shah_studentt_2014,
  title = {Student-t Processes as Alternatives to {{Gaussian}} Processes},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Shah, Amar and Wilson, Andrew and Ghahramani, Zoubin},
  year = {2014},
  pages = {877--885},
  file = {/home/msca8h/Zotero/storage/VCQRDIKW/Shah et al. - 2014 - Student-t processes as alternatives to Gaussian pr.pdf},
  series = {{{AISTATS}}'14}
}

@article{shahriari_taking_2016,
  title = {Taking the Human out of the Loop: A Review of {{Bayesian}} Optimization},
  author = {Shahriari, B. and Swersky, K. and Wang, Z. and Adams, R. P. and de Freitas, N.},
  year = {2016},
  month = jan,
  volume = {104},
  pages = {148--175},
  file = {/home/msca8h/Documents/bayesian_optimization/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf},
  journal = {Proc. IEEE},
  keywords = {Bayes methods,Bayesian optimization,Big data,Big Data,Big data application,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,human productivity,large-scale heterogeneous computing,Linear programming,massive complex software system,optimisation,optimization,Optimization,product quality,response surface methodology,Statistical analysis,statistical learning,storage allocation,storage architecture},
  number = {1}
}

@article{shang_vrsgd_2018,
  title = {{{VR}}-{{SGD}}: {{A Simple Stochastic Variance Reduction Method}} for {{Machine Learning}}},
  shorttitle = {{{VR}}-{{SGD}}},
  author = {Shang, Fanhua and Zhou, Kaiwen and Liu, Hongying and Cheng, James and Tsang, Ivor and Zhang, Lijun and Tao, Dacheng and Licheng, Jiao},
  year = {2018},
  pages = {1--1},
  file = {/home/msca8h/Zotero/storage/CVPA58M4/Shang et al. - 2018 - VR-SGD A Simple Stochastic Variance Reduction Met.pdf},
  journal = {IEEE Trans. Knowl. Data Eng.}
}

@article{sherlock_optimal_2009,
  title = {Optimal Scaling of the Random Walk {{Metropolis}} on Elliptically Symmetric Unimodal Targets},
  author = {Sherlock, Chris and Roberts, Gareth},
  year = {2009},
  volume = {15},
  pages = {774--798},
  abstract = {Scaling of proposals for Metropolis algorithms is an important practical problem in MCMC implementation. Criteria for scaling based on empirical acceptance rates of algorithms have been found to work consistently well across a broad range of problems. Essentially, proposal jump sizes are increased when acceptance rates are high and decreased when rates are low. In recent years, considerable theoretical support has been given for rules of this type which work on the basis that acceptance rates around 0.234 should be preferred. This has been based on asymptotic results that approximate high dimensional algorithm trajectories by diffusions. In this paper, we develop a novel approach to understanding 0.234 which avoids the need for diffusion limits. We derive explicit formulae for algorithm efficiency and acceptance rates as functions of the scaling parameter. We apply these to the family of elliptically symmetric target densities, where further illuminating explicit results are possible. Under suitable conditions, we verify the 0.234 rule for a new class of target densities. Moreover, we can characterise cases where 0.234 fails to hold, either because the target density is too diffuse in a sense we make precise, or because the eccentricity of the target density is too severe, again in a sense we make precise. We provide numerical verifications of our results.},
  journal = {Bernoulli},
  number = {3}
}

@inproceedings{shin_speeding_2010,
  title = {Speeding up {{Nek5000}} with Autotuning and Specialization},
  booktitle = {Proceedings of the 24th {{ACM International Conference}} on {{Supercomputing}} - {{ICS}} '10},
  author = {Shin, Jaewook and Hall, Mary W. and Chame, Jacqueline and Chen, Chun and Fischer, Paul F. and Hovland, Paul D.},
  year = {2010},
  pages = {253},
  publisher = {{ACM Press}},
  address = {{Tsukuba, Ibaraki, Japan}},
  language = {en}
}

@article{shlezinger_modelbased_2020,
  title = {Model-{{Based Deep Learning}}},
  author = {Shlezinger, Nir and Whang, Jay and Eldar, Yonina C. and Dimakis, Alexandros G.},
  year = {2020},
  month = dec,
  abstract = {Signal processing, communications, and control have traditionally relied on classical statistical modeling techniques. Such model-based methods utilize mathematical formulations that represent the underlying physics, prior information and additional domain knowledge. Simple classical models are useful but sensitive to inaccuracies and may lead to poor performance when real systems display complex or dynamic behavior. On the other hand, purely data-driven approaches that are model-agnostic are becoming increasingly popular as datasets become abundant and the power of modern deep learning pipelines increases. Deep neural networks (DNNs) use generic architectures which learn to operate from data, and demonstrate excellent performance, especially for supervised problems. However, DNNs typically require massive amounts of data and immense computational resources, limiting their applicability for some signal processing scenarios. We are interested in hybrid techniques that combine principled mathematical models with data-driven systems to benefit from the advantages of both approaches. Such model-based deep learning methods exploit both partial domain knowledge, via mathematical structures designed for specific problems, as well as learning from limited data. In this article we survey the leading approaches for studying and designing model-based deep learning systems. We divide hybrid model-based/data-driven systems into categories based on their inference mechanism. We provide a comprehensive review of the leading approaches for combining model-based algorithms with deep learning in a systematic manner, along with concrete guidelines and detailed signal processing oriented examples from recent literature. Our aim is to facilitate the design and study of future systems on the intersection of signal processing and machine learning that incorporate the advantages of both domains.},
  archiveprefix = {arXiv},
  eprint = {2012.08405},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/ZKFS2FZX/Shlezinger et al. - 2020 - Model-Based Deep Learning.pdf;/home/msca8h/Zotero/storage/UPIQKL7Q/2012.html},
  journal = {ArXiv201208405 Cs Eess},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  primaryclass = {cs, eess}
}

@article{Sigillito1989ClassificationOR,
  title = {Classification of Radar Returns from the Ionosphere Using Neural Networks},
  author = {Sigillito, Vincent G. and Wing, Simon and Hutton, Larrie V. and Baker, K. L.},
  year = {1989},
  volume = {10},
  pages = {262--266},
  journal = {Johns Hopkins APL Tech. Dig.}
}

@article{singh_hybrid_2017,
  title = {A Hybrid Algorithm for Speckle Noise Reduction of Ultrasound Images},
  author = {Singh, Karamjeet and Ranade, Sukhjeet Kaur and Singh, Chandan},
  year = {2017},
  month = sep,
  volume = {148},
  pages = {55--69},
  journal = {Comput. Methods Programs Biomed.},
  language = {en}
}

@article{skilling_nested_2006,
  title = {Nested Sampling for General {{Bayesian}} Computation},
  author = {Skilling, John},
  year = {2006},
  month = dec,
  volume = {1},
  file = {/home/msca8h/Zotero/storage/CM5CW82X/Skilling - 2006 - Nested sampling for general Bayesian computation.pdf},
  journal = {Bayesian Anal.},
  number = {4}
}

@article{smith_cocoa_2018,
  title = {{{CoCoA}}: {{A General Framework}} for {{Communication}}-{{Efficient Distributed Optimization}}},
  author = {Smith, Virginia and Forte, Simone and Ma, Chenxin and Tak{\'a}{\v c}, Martin and Jordan, Michael I. and Jaggi, Martin},
  year = {2018},
  volume = {18},
  pages = {1--49},
  journal = {J. Mach. Learn. Res.},
  number = {230}
}

@article{smith_susan_1997,
  title = {{{SUSAN}}\textemdash{{A}} New Approach to Low Level Image Processing},
  author = {Smith, Stephen M. and Brady, J. Michael},
  year = {1997},
  volume = {23},
  pages = {45--78},
  journal = {Int. J. Comput. Vis.},
  number = {1}
}

@article{smith_ultrasound_1984,
  title = {Ultrasound Speckle Size and Lesion Signal to Noise Ratio: Verification of Theory},
  shorttitle = {Ultrasound {{Speckle Size}} and {{Lesion Signal}} to {{Noise Ratio}}},
  author = {Smith, S.W. and Wagner, R.F.},
  year = {1984},
  month = apr,
  volume = {6},
  pages = {174--180},
  abstract = {We compare predictions from our published theory of speckle cell size with recently published experimental results and a three dimensional computer simulation for the case of Gaussian pulses from spherically focused transducers in random media. The agreement is very good. We also compare our published theoretical predictions of the signal-to-noise ratio for a circular lesion in a speckle background with published ``contrast to speckle ratio'' data for anechoic cylindrical lesions in tissue mimicking material. Again, agreement is very good. The verification of these theoretical predictions has important implications for the evaluation of B-scan image quality and the study of tissue characterization.},
  journal = {Ultrason Imaging},
  language = {en},
  number = {2}
}

@inproceedings{smith_using_1988,
  title = {Using the {{ADAP}} Learning Algorithm to Forecast the Onset of Diabetes Mellitus},
  booktitle = {Proceedings of the {{Annual Symposium}} on {{Computer Application}} in {{Medical Care}}},
  author = {Smith, Jack W and Everhart, J. E. and Dickson, W. C. and Knowler, W. C. and Johannes, R. S.},
  year = {1988},
  month = nov,
  pages = {261--265}
}

@incollection{snoek_can_2019,
  title = {Can {{You Trust Your Model}}'s {{Uncertainty}}? {{Evaluating Predictive Uncertainty Under Dataset Shift}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Snoek, Jasper and Ovadia, Yaniv and Fertig, Emily and Lakshminarayanan, Balaji and Nowozin, Sebastian and Sculley, D. and Dillon, Joshua and Ren, Jie and Nado, Zachary},
  year = {2019},
  pages = {13969--13980},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{snoek_practical_2012,
  title = {Practical {{Bayesian}} Optimization of Machine Learning Algorithms},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  year = {2012},
  volume = {25},
  pages = {2951--2959},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  series = {{{NIPS}}'12}
}

@article{sobol_distribution_1967,
  title = {On the Distribution of Points in a Cube and the Approximate Evaluation of Integrals},
  author = {Sobol', I.M},
  year = {1967},
  month = jan,
  volume = {7},
  pages = {86--112},
  journal = {USSR Comput. Math. Math. Phys.},
  language = {en},
  number = {4}
}

@article{solonen_efficient_2012,
  title = {Efficient {{MCMC}} for {{Climate Model Parameter Estimation}}: {{Parallel Adaptive Chains}} and {{Early Rejection}}},
  shorttitle = {Efficient {{MCMC}} for {{Climate Model Parameter Estimation}}},
  author = {Solonen, Antti and Ollinaho, Pirkka and Laine, Marko and Haario, Heikki and Tamminen, Johanna and J{\"a}rvinen, Heikki},
  year = {2012},
  month = sep,
  volume = {7},
  pages = {715--736},
  file = {/home/msca8h/Zotero/storage/6XTGQ3DA/Solonen et al. - 2012 - Efficient MCMC for Climate Model Parameter Estimat.pdf},
  journal = {Bayesian Anal.},
  language = {en},
  number = {3}
}

@inproceedings{song_designing_2014,
  title = {Designing and Auto-Tuning Parallel 3-{{D FFT}} for Computation-Communication Overlap},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} Symposium on {{Principles}} and Practice of Parallel Programming - {{PPoPP}} '14},
  author = {Song, Sukhyun and Hollingsworth, Jeffrey K.},
  year = {2014},
  pages = {181--192},
  publisher = {{ACM Press}},
  address = {{Orlando, Florida, USA}},
  language = {en}
}

@article{south_sequential_2018,
  title = {Sequential {{Monte Carlo Samplers}} with {{Independent Markov Chain Monte Carlo Proposals}}},
  author = {South, Leah F and Pettitt, Anthony N and Drovandi, Christopher C and others},
  year = {2018},
  file = {/home/msca8h/Zotero/storage/LKCDACSN/South et al. - 2018 - Sequential Monte Carlo Samplers with Independent M.pdf},
  journal = {Bayesian Anal.}
}

@article{souza_priorguided_2020,
  title = {Prior-Guided {{Bayesian Optimization}}},
  author = {Souza, Artur and Nardi, Luigi and Oliveira, Leonardo B. and Olukotun, Kunle and Lindauer, Marius and Hutter, Frank},
  year = {2020},
  month = jun,
  abstract = {While Bayesian Optimization (BO) is a very popular method for optimizing expensive black-box functions, it fails to leverage the experience of domain experts. This causes BO to waste function evaluations on commonly known bad regions of design choices, e.g., hyperparameters of a machine learning algorithm. To address this issue, we introduce Prior-guided Bayesian Optimization (PrBO). PrBO allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than BO's standard priors over functions which are much less intuitive for users. PrBO then combines these priors with BO's standard probabilistic model to yield a posterior. We show that PrBO is more sample efficient than state-of-the-art methods without user priors and 10,000\$\textbackslash times\$ faster than random search, on a common suite of benchmarks and a real-world hardware design application. We also show that PrBO converges faster even if the user priors are not entirely accurate and that it robustly recovers from misleading priors.},
  archiveprefix = {arXiv},
  eprint = {2006.14608},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/C4U9M6U6/Souza et al. - 2020 - Prior-guided Bayesian Optimization.pdf;/home/msca8h/Zotero/storage/LV5GTZQG/2006.html},
  journal = {ArXiv200614608 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{spall_implementation_1998,
  title = {Implementation of the Simultaneous Perturbation Algorithm for Stochastic Optimization},
  author = {Spall, J.C.},
  year = {1998},
  month = jul,
  volume = {34},
  pages = {817--823},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  number = {3}
}

@article{spall_overview_1998,
  title = {An Overview of the Simultaneous Perturbation Method for Efficient Optimization},
  author = {Spall, James C},
  year = {1998},
  volume = {19},
  pages = {482--492},
  journal = {Johns Hopkins APL Tech. Dig.},
  number = {4}
}

@article{spiegelhalter1996bugs,
  title = {{{BUGS}}: {{Bayesian}} Inference Using Gibbs Sampling},
  author = {Spiegelhalter, David J and Thomas, Andrew and Best, Nicky G and Gilks, Wally and Lunn, D},
  year = {1996},
  volume = {19},
  journal = {Version 05version Ii Httpwww Mrc-Bsu Cam Ac Ukbugs}
}

@incollection{springenberg_bayesian_2016,
  title = {Bayesian {{Optimization}} with {{Robust Bayesian Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Springenberg, Jost Tobias and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
  year = {2016},
  pages = {4134--4142},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{srinivas_gaussian_2010,
  title = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}} ({{ICML}}'10)},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  year = {2010},
  pages = {1015--1022},
  publisher = {{Omnipress}},
  address = {{USA}},
  file = {/home/msca8h/Documents/bayesian_optimization/Srinivas et al. - 2010 - Gaussian Process Optimization in the Bandit Settin.pdf}
}

@article{srinivas_informationtheoretic_2012,
  title = {Information-{{Theoretic Regret Bounds}} for {{Gaussian Process Optimization}} in the {{Bandit Setting}}},
  author = {Srinivas, N. and Krause, A. and Kakade, S. M. and Seeger, M. W.},
  year = {2012},
  month = may,
  volume = {58},
  pages = {3250--3265},
  journal = {IEEE Trans. Inf. Theory},
  keywords = {Bandit problems,bandit setting,Bayesian methods,Bayesian prediction,Convergence,cumulative regret,experimental design,Gaussian process (GP),Gaussian process optimization,Gaussian processes,Hilbert spaces,information gain,information theory,information-theoretic regret bounds,intuitive Gaussian process upper confidence bound algorithm,Kernel,multiarmed bandit problem,Noise,nonparametric statistics,online learning,Optimization,payoff function,regret bound,reproducing kernel Hilbert space,statistical learning,sublinear regret bounds,Temperature sensors},
  number = {5}
}

@inproceedings{stahl_noiseresistant_1999,
  title = {Noise-Resistant Weak-Structure Enhancement for Digital Radiography},
  booktitle = {Proc. {{SPIE Med}}. {{Imag}}.},
  author = {Stahl, Martin and Aach, Til and Buzug, Thorsten M. and Dippel, Sabine and Neitzel, Ulrich},
  year = {1999},
  month = may,
  pages = {1406--1417},
  address = {{San Diego, CA}}
}

@article{stan2020,
  title = {Stan Modeling Language Users Guide and Reference Manual, Version 2.23.0},
  author = {{Stan Development Team}},
  year = {2020}
}

@inproceedings{stich_local_2018,
  title = {Local {{SGD Converges Fast}} and {{Communicates Little}}},
  booktitle = {{{arXiv}}:1805.09767 [Cs, Math]},
  author = {Stich, Sebastian U.},
  year = {2018},
  month = may,
  abstract = {Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speedup with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while. This scheme shows promising results in practice, but eluded thorough theoretical analysis. We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speedup in the number of workers and mini-batch size. The number of communication rounds can be reduced up to a factor of T\^\{1/2\}---where T denotes the number of total steps---compared to mini-batch SGD. This also holds for asynchronous implementations. Local SGD can also be used for large scale training of deep learning models. The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.},
  archiveprefix = {arXiv},
  eprint = {1805.09767},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/W8RQ38CV/Stich - 2018 - Local SGD Converges Fast and Communicates Little.pdf;/home/msca8h/Zotero/storage/WS8MI8Z6/1805.html},
  keywords = {90C06; 68W40; 68W10,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,F.2.1,G.1.6,Mathematics - Optimization and Control},
  primaryclass = {cs, math}
}

@inproceedings{strens_evolutionary_2003,
  title = {Evolutionary {{MCMC Sampling}} and {{Optimization}} in {{Discrete Spaces}}},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Strens, Malcolm J. A.},
  year = {2003},
  pages = {736--743},
  publisher = {{AAAI Press}},
  series = {{{ICML}}'03}
}

@article{strid_efficient_2010,
  title = {Efficient Parallelisation of {{Metropolis}}\textendash{{Hastings}} Algorithms Using a Prefetching Approach},
  author = {Strid, Ingvar},
  year = {2010},
  month = nov,
  volume = {54},
  pages = {2814--2835},
  journal = {Comput. Statist. Data Anal.},
  language = {en},
  number = {11}
}

@article{suchard_manycore_2009,
  title = {Many-Core Algorithms for Statistical Phylogenetics},
  author = {Suchard, M. A. and Rambaut, A.},
  year = {2009},
  month = jun,
  volume = {25},
  pages = {1370--1376},
  file = {/home/msca8h/Zotero/storage/DJJDRPJT/Suchard and Rambaut - 2009 - Many-core algorithms for statistical phylogenetics.pdf},
  journal = {Bioinformatics},
  language = {en},
  number = {11}
}

@article{suchard_understanding_2010,
  title = {Understanding {{GPU}} Programming for Statistical Computation: Studies in Massively Parallel Massive Mixtures},
  shorttitle = {Understanding {{GPU Programming}} for {{Statistical Computation}}},
  author = {Suchard, Marc A. and Wang, Quanli and Chan, Cliburn and Frelinger, Jacob and Cron, Andrew and West, Mike},
  year = {2010},
  month = jan,
  volume = {19},
  pages = {419--438},
  file = {/home/msca8h/Zotero/storage/THFFSHLW/Suchard et al. - 2010 - Understanding GPU Programming for Statistical Comp.pdf},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {2}
}

@inproceedings{sukhija_portfoliobased_2014,
  title = {Portfolio-Based Selection of Robust Dynamic Loop Scheduling Algorithms Using Machine Learning},
  booktitle = {2014 {{IEEE International Parallel}} \& {{Distributed Processing Symposium Workshops}}},
  author = {Sukhija, Nitin and Malone, Brandon and Srivastava, Srishti and Banicescu, Ioana and Ciorba, Florina M.},
  year = {2014},
  month = may,
  pages = {1638--1647},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}}
}

@article{sun_automated_2020,
  title = {Automated {{Performance Modeling}} of {{HPC Applications Using Machine Learning}}},
  author = {Sun, Jingwei and Sun, Guangzhong and Zhan, Shiyan and Zhang, Jiepeng and Chen, Yong},
  year = {2020},
  month = may,
  volume = {69},
  pages = {749--763},
  journal = {IEEE Trans. Comput.},
  number = {5}
}

@inproceedings{sun_functional_2019,
  title = {Functional {{Variational Bayesian Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  year = {2019}
}

@inproceedings{sun_new_2019,
  title = {New {{Interpretations}} of {{Normalization Methods}} in {{Deep Learning}}},
  author = {{sun}, jiacheng and Cao, Xiangyong and Liang, Hanwen and {huang}, weiran and {chen}, zewei and {li}, zhenguo},
  year = {2019},
  month = nov
}

@article{svensson_marginalizing_2015,
  title = {Marginalizing {{Gaussian Process Hyperparameters}} Using {{Sequential Monte Carlo}}},
  author = {Svensson, Andreas and Dahlin, Johan and Sch{\"o}n, Thomas B.},
  year = {2015},
  month = dec,
  pages = {477--480},
  abstract = {Gaussian process regression is a popular method for non-parametric probabilistic modeling of functions. The Gaussian process prior is characterized by so-called hyperparameters, which often have a large influence on the posterior model and can be difficult to tune. This work provides a method for numerical marginalization of the hyperparameters, relying on the rigorous framework of sequential Monte Carlo. Our method is well suited for online problems, and we demonstrate its ability to handle real-world problems with several dimensions and compare it to other marginalization methods. We also conclude that our proposed method is a competitive alternative to the commonly used point estimates maximizing the likelihood, both in terms of computational load and its ability to handle multimodal posteriors.},
  archiveprefix = {arXiv},
  eprint = {1502.01908},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/SX4FSW2S/Svensson et al. - 2015 - Marginalizing Gaussian Process Hyperparameters usi.pdf;/home/msca8h/Zotero/storage/AE7HD5G9/1502.html},
  journal = {2015 IEEE 6th Int. Workshop Comput. Adv. Multi-Sens. Adapt. Process. CAMSAP},
  keywords = {Statistics - Computation,Statistics - Machine Learning}
}

@inproceedings{svensson_marginalizing_2015a,
  title = {Marginalizing {{Gaussian}} Process Hyperparameters Using Sequential {{Monte Carlo}}},
  booktitle = {2015 {{IEEE}} 6th {{International Workshop}} on {{Computational Advances}} in {{Multi}}-{{Sensor Adaptive Processing}} ({{CAMSAP}})},
  author = {Svensson, Andreas and Dahlin, Johan and Schon, Thomas B.},
  year = {2015},
  month = dec,
  pages = {477--480},
  publisher = {{IEEE}},
  address = {{Cancun, Mexico}},
  file = {/home/msca8h/Zotero/storage/UG6NUK9Y/Svensson et al. - 2015 - Marginalizing Gaussian process hyperparameters usi.pdf}
}

@article{swersky_freezethaw_2014,
  title = {Freeze-Thaw {{Bayesian}} Optimization},
  author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  year = {2014},
  month = jun,
  abstract = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
  archiveprefix = {arXiv},
  eprint = {1406.3896},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/5FUNWCAA/Swersky et al. - 2014 - Freeze-Thaw Bayesian Optimization.pdf;/home/msca8h/Zotero/storage/XRYAQ93P/1406.html},
  journal = {arXiv:1406.3896 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{taasti_automating_2020,
  title = {Automating Proton Treatment Planning with Beam Angle Selection Using {{Bayesian}} Optimization},
  author = {Taasti, Vicki T. and Hong, Linda and Shim, Jin Sup(Andy) and Deasy, Joseph O. and Zarepisheh, Masoud},
  year = {2020},
  month = may,
  pages = {mp.14215},
  journal = {Med. Phys.},
  language = {en}
}

@inproceedings{tabatabaee_parallel_2005,
  title = {Parallel {{Parameter Tuning}} for {{Applications}} with {{Performance Variability}}},
  booktitle = {{{ACM}}/{{IEEE SC}} 2005 {{Conference}} ({{SC}}'05)},
  author = {Tabatabaee, V. and Tiwari, A. and Hollingsworth, J.K.},
  year = {2005},
  pages = {57--57},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}}
}

@inproceedings{tabirca_feedback_2001,
  title = {Feedback Guided Dynamic Loop Scheduling; {{A}} Theoretical Approach},
  booktitle = {Proceedings {{International Conference}} on {{Parallel Processing Workshops}}},
  author = {Tabirca, T. and Freeman, L. and Tabirca, S. and Yang, L.T.},
  year = {2001},
  pages = {115--121},
  publisher = {{IEEE Comput. Soc}},
  address = {{Valencia, Spain}},
  file = {/home/msca8h/Documents/parallel_scheduling/06882d3232d6c887834fa9027a5cd25055d9.pdf;/home/msca8h/Zotero/storage/4ZVCP797/Tabirca et al. - 2001 - Feedback guided dynamic loop scheduling\; A theoret.pdf}
}

@inproceedings{tang_highthroughput_2020,
  title = {A {{High}}-{{Throughput Solver}} for {{Marginalized Graph Kernels}} on {{GPU}}},
  booktitle = {2020 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Tang, Yu-Hang and Selvitopi, Oguz and Popovici, Doru Thom and Buluc, Aydin},
  year = {2020},
  month = may,
  pages = {728--738},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  file = {/home/msca8h/Zotero/storage/FFQA53EH/Tang et al. - 2020 - A High-Throughput Solver for Marginalized Graph Ke.pdf}
}

@inproceedings{tang_processor_1986,
  title = {{Processor self-scheduling for multiple-nested parallel loops}},
  booktitle = {{Proc. Int. Conf. Parallel Process.}},
  author = {Tang, Peiyi and Yew, Pen Chung},
  year = {1986},
  month = dec,
  pages = {528--535},
  publisher = {{IEEE}},
  abstract = {Processor self-scheduling is a useful scheme in a multiprocessor system if the execution time of each iteration in a parallel loop is not known in advance and varies substantially, or if there are multiple nestings in parallel loops which makes static scheduling difficult and inefficient. By using efficient synchronization primitives, the operating system is not needed for loop scheduling. The overhead for the processor self-scheduling is small. A self-scheduling scheme is presented for a single-nested parallel loop and is extended to multiple-nested parallel loops. Barrier synchronization mechanisms for the self-scheduling schemes are also discussed.},
  language = {English (US)},
  series = {{ICPP'86}}
}

@inproceedings{tapus_active_2002,
  title = {Active {{Harmony}}: {{Towards Automated Performance Tuning}}},
  shorttitle = {Active {{Harmony}}},
  booktitle = {{{ACM}}/{{IEEE SC}} 2002 {{Conference}} ({{SC}}'02)},
  author = {Tapus, C. and {I-Hsin Chung} and Hollingsworth, J.K.},
  year = {2002},
  pages = {44--44},
  publisher = {{IEEE}},
  address = {{Baltimore, MD, USA}}
}

@inproceedings{tay_ultrasound_2006,
  title = {Ultrasound Despeckling Using an Adaptive Window Stochastic Approach},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Image Process}}.},
  author = {Tay, Peter C. and Acton, Scott T. and Hossack, John A.},
  year = {2006},
  month = oct,
  pages = {2549--2552},
  publisher = {{IEEE}},
  address = {{Atlanta, GA}}
}

@article{tensorforce,
  title = {Tensorforce: A {{TensorFlow}} Library for Applied Reinforcement Learning},
  author = {Kuhnle, Alexander and Schaarschmidt, Michael and Fricke, Kai},
  year = {2017},
  howpublished = {Web page}
}

@article{terenin_asynchronous_2020,
  title = {Asynchronous {{Gibbs Sampling}}},
  author = {Terenin, Alexander and Simpson, Daniel and Draper, David},
  year = {2020},
  month = feb,
  abstract = {Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method often used in Bayesian learning. MCMC methods can be difficult to deploy on parallel and distributed systems due to their inherently sequential nature. We study asynchronous Gibbs sampling, which achieves parallelism by simply ignoring sequential requirements. This method has been shown to produce good empirical results for some hierarchical models, and is popular in the topic modeling community, but was also shown to diverge for other targets. We introduce a theoretical framework for analyzing asynchronous Gibbs sampling and other extensions of MCMC that do not possess the Markov property. We prove that asynchronous Gibbs can be modified so that it converges under appropriate regularity conditions -- we call this the exact asynchronous Gibbs algorithm. We study asynchronous Gibbs on a set of examples by comparing the exact and approximate algorithms, including two where it works well, and one where it fails dramatically. We conclude with a set of heuristics to describe settings where the algorithm can be effectively used.},
  archiveprefix = {arXiv},
  eprint = {1509.08999},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/8ED8XA7L/Terenin et al. - 2020 - Asynchronous Gibbs Sampling.pdf;/home/msca8h/Zotero/storage/ARKY94I4/1509.html},
  journal = {ArXiv150908999 Stat},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@article{thawornwattana_designing_2018,
  title = {Designing {{Simple}} and {{Efficient Markov Chain Monte Carlo Proposal Kernels}}},
  author = {Thawornwattana, Yuttapong and Dalquen, Daniel and Yang, Ziheng},
  year = {2018},
  month = dec,
  volume = {13},
  pages = {1037--1063},
  file = {/home/msca8h/Zotero/storage/MZHXDNJQ/Thawornwattana et al. - 2018 - Designing Simple and Efficient Markov Chain Monte .pdf},
  journal = {Bayesian Anal.},
  language = {en},
  number = {4}
}

@article{thijssen_texture_1990,
  title = {Texture in Tissue Echograms. {{Speckle}} or Information?},
  author = {Thijssen, J. M. and Oosterveld, B. J.},
  year = {1990},
  month = apr,
  volume = {9},
  pages = {215--229},
  journal = {J. Ultrasound Med.},
  language = {en},
  number = {4}
}

@article{thompson_decline_2018,
  title = {The {{Decline}} of {{Computers As}} a {{General Purpose Technology}}: {{Why Deep Learning}} and the {{End}} of {{Moore}}'s {{Law}} Are {{Fragmenting Computing}}},
  shorttitle = {The {{Decline}} of {{Computers As}} a {{General Purpose Technology}}},
  author = {Thompson, Neil and Spanuth, Svenja},
  year = {2018},
  journal = {SSRN Electron. J.},
  language = {en}
}

@incollection{titsiasrcaueb_variational_2013,
  title = {Variational {{Inference}} for {{Mahalanobis Distance Metrics}} in {{Gaussian Process Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Titsias RC AUEB, Michalis and {Lazaro-Gredilla}, Miguel},
  year = {2013},
  pages = {279--287},
  publisher = {{Curran Associates, Inc.}}
}

@article{tiwari_autotuning_2011,
  title = {Auto-Tuning Full Applications: {{A}} Case Study},
  shorttitle = {Auto-Tuning Full Applications},
  author = {Tiwari, Ananta and Hollingsworth, Jeffrey K and {Chun Chen} and Hall, Mary and {Chunhua Liao} and Quinlan, Daniel J and Chame, Jacqueline},
  year = {2011},
  month = aug,
  volume = {25},
  pages = {286--294},
  file = {/home/msca8h/Zotero/storage/ITK2RPSD/Tiwari et al. - 2011 - Auto-tuning full applications A case study.pdf},
  journal = {The International Journal of High Performance Computing Applications},
  language = {en},
  number = {3}
}

@inproceedings{tiwari_scalable_2009,
  title = {A Scalable Auto-Tuning Framework for Compiler Optimization},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}},
  author = {Tiwari, Ananta and Chen, Chun and Chame, Jacqueline and Hall, Mary and Hollingsworth, Jeffrey K.},
  year = {2009},
  month = may,
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Rome, Italy}},
  file = {/home/msca8h/Zotero/storage/A29KRNU9/Tiwari et al. - 2009 - A scalable auto-tuning framework for compiler opti.pdf}
}

@article{topol_highperformance_2019,
  title = {High-Performance Medicine: The Convergence of Human and Artificial Intelligence},
  shorttitle = {High-Performance Medicine},
  author = {Topol, Eric J.},
  year = {2019},
  month = jan,
  volume = {25},
  pages = {44--56},
  journal = {Nat Med},
  language = {en},
  number = {1}
}

@article{torralba_statistics_2003,
  title = {Statistics of Natural Image Categories},
  author = {Torralba, Antonio and Oliva, Aude},
  year = {2003},
  month = jan,
  volume = {14},
  pages = {391--412},
  file = {/home/msca8h/Zotero/storage/99LNCRT2/Torralba and Oliva - 2003 - Statistics of natural image categories.pdf},
  journal = {Network: Computation in Neural Systems},
  language = {en},
  number = {3}
}

@article{tory_human_2004,
  title = {Human Factors in Visualization Research},
  author = {Tory, M. and Moller, T.},
  year = {2004},
  month = jan,
  volume = {10},
  pages = {72--84},
  journal = {IEEE Trans. Visual. Comput. Graphics},
  language = {en},
  number = {1}
}

@inproceedings{tracey_upgrading_2018,
  title = {Upgrading from {{Gaussian}} Processes to {{Student}}'st Processes},
  booktitle = {2018 {{AIAA Non}}-{{Deterministic Approaches Conference}}},
  author = {Tracey, Brendan D and Wolpert, David},
  year = {2018},
  pages = {1659}
}

@article{tran_model_2021,
  title = {Model {{Selection}} for {{Bayesian Autoencoders}}},
  author = {Tran, Ba-Hien and Rossi, Simone and Milios, Dimitrios and Michiardi, Pietro and Bonilla, Edwin V. and Filippone, Maurizio},
  year = {2021},
  month = jun,
  abstract = {We develop a novel method for carrying out model selection for Bayesian autoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by the common practice of type-II maximum likelihood optimization and its equivalence to Kullback-Leibler divergence minimization, we propose to optimize the distributional sliced-Wasserstein distance (DSWD) between the output of the autoencoder and the empirical data distribution. The advantages of this formulation are that we can estimate the DSWD based on samples and handle high-dimensional problems. We carry out posterior estimation of the BAE parameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE into a generative model by fitting a flexible Dirichlet mixture model in the latent space. Consequently, we obtain a powerful alternative to variational autoencoders, which are the preferred choice in modern applications of autoencoders for representation learning with uncertainty. We evaluate our approach qualitatively and quantitatively using a vast experimental campaign on a number of unsupervised learning tasks and show that, in small-data regimes where priors matter, our approach provides state-of-the-art results, outperforming multiple competitive baselines.},
  archiveprefix = {arXiv},
  eprint = {2106.06245},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/C5PAT2E6/Tran et al. - 2021 - Model Selection for Bayesian Autoencoders.pdf;/home/msca8h/Zotero/storage/I6PPVPIX/2106.html},
  journal = {ArXiv210606245 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{tran2016edward,
  title = {Edward: {{A}} Library for Probabilistic Modeling, Inference, and Criticism},
  author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
  year = {2016},
  archiveprefix = {arXiv},
  eprint = {1610.09787},
  eprinttype = {arxiv},
  journal = {ArXiv Prepr. ArXiv161009787}
}

@article{tran2016edward,
  title = {Edward: {{A}} Library for Probabilistic Modeling, Inference, and Criticism},
  author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
  year = {2016},
  archiveprefix = {arXiv},
  eprint = {1610.09787},
  eprinttype = {arxiv},
  journal = {ArXiv Prepr. ArXiv161009787}
}

@article{turner_how_2017,
  title = {How Well Does Your Sampler Really Work?},
  author = {Turner, Ryan and Neal, Brady},
  year = {2017},
  month = dec,
  abstract = {We present a new data-driven benchmark system to evaluate the performance of new MCMC samplers. Taking inspiration from the COCO benchmark in optimization, we view this task as having critical importance to machine learning and statistics given the rate at which new samplers are proposed. The common hand-crafted examples to test new samplers are unsatisfactory; we take a meta-learning-like approach to generate benchmark examples from a large corpus of data sets and models. Surrogates of posteriors found in real problems are created using highly flexible density models including modern neural network based approaches. We provide new insights into the real effective sample size of various samplers per unit time and the estimation efficiency of the samplers per sample. Additionally, we provide a meta-analysis to assess the predictive utility of various MCMC diagnostics and perform a nonparametric regression to combine them.},
  archiveprefix = {arXiv},
  eprint = {1712.06006},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/QRBDPF97/Turner and Neal - 2017 - How well does your sampler really work.pdf;/home/msca8h/Zotero/storage/NZSGUA3H/1712.html},
  journal = {ArXiv171206006 Stat},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  primaryclass = {stat}
}

@article{tzen_trapezoid_1993,
  title = {Trapezoid Self-Scheduling: A Practical Scheduling Scheme for Parallel Compilers},
  author = {Tzen, T. H. and Ni, L. M.},
  year = {1993},
  month = jan,
  volume = {4},
  pages = {87--98},
  file = {/home/msca8h/Documents/parallel_scheduling/Tzen and Ni - 1993 - Trapezoid self-scheduling a practical scheduling .pdf},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  keywords = {Butterfly GP-1000,chunk size,Computer science,dynamic allocation,Dynamic scheduling,load balancing,Load management,loop iterations,memory management,Memory management,Multiprocessing systems,parallel compilers,Parallel languages,parallel nested loops,Parallel processing,parallel programming,parallel programs,Processor scheduling,processor self-scheduling,program compilers,Programming profession,run-time scheduling overhead,Runtime,scheduling,shared memory systems,shared-memory multiprocessors,trapezoid self-scheduling},
  number = {1}
}

@article{ulyanov_instance_2017,
  title = {Instance {{Normalization}}: {{The Missing Ingredient}} for {{Fast Stylization}}},
  shorttitle = {Instance {{Normalization}}},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2017},
  month = nov,
  abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture\_nets. Full paper can be found at arXiv:1701.02096.},
  archiveprefix = {arXiv},
  eprint = {1607.08022},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/HRP7WMIT/Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf;/home/msca8h/Zotero/storage/SGFGJDB5/1607.html},
  journal = {ArXiv160708022 Cs},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{uribe_optimal_2017,
  title = {Optimal {{Algorithms}} for {{Distributed Optimization}}},
  author = {Uribe, C{\'e}sar A. and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
  year = {2017},
  month = dec,
  abstract = {In this paper, we study the optimal convergence rate for distributed convex optimization problems in networks. We model the communication restrictions imposed by the network as a set of affine constraints and provide optimal complexity bounds for four different setups, namely: the function \$F(\textbackslash xb) \textbackslash triangleq \textbackslash sum\_\{i=1\}\^\{m\}f\_i(\textbackslash xb)\$ is strongly convex and smooth, either strongly convex or smooth or just convex. Our results show that Nesterov's accelerated gradient descent on the dual problem can be executed in a distributed manner and obtains the same optimal rates as in the centralized version of the problem (up to constant or logarithmic factors) with an additional cost related to the spectral gap of the interaction matrix. Finally, we discuss some extensions to the proposed setup such as proximal friendly functions, time-varying graphs, improvement of the condition numbers.},
  archiveprefix = {arXiv},
  eprint = {1712.00232},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/96HMKSYN/Uribe et al. - 2017 - Optimal Algorithms for Distributed Optimization.pdf;/home/msca8h/Zotero/storage/MV9KKICE/1712.html},
  journal = {ArXiv171200232 Cs Math Stat},
  keywords = {Computer Science - Computational Complexity,Computer Science - Multiagent Systems,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@inproceedings{vanaken_automatic_2017,
  title = {Automatic {{Database Management System Tuning Through Large}}-Scale {{Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}} - {{SIGMOD}} '17},
  author = {Van Aken, Dana and Pavlo, Andrew and Gordon, Geoffrey J. and Zhang, Bohan},
  year = {2017},
  pages = {1009--1024},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  file = {/home/msca8h/Zotero/storage/34HRAM9E/Van Aken et al. - 2017 - Automatic Database Management System Tuning Throug.pdf},
  language = {en}
}

@article{vandenberg_influence_2017,
  title = {Influence of {{Bayesian}} Optimization on the Performance of Propofol Target-Controlled Infusion},
  author = {{van den Berg}, J.P. and Eleveld, D.J. and De Smet, T. and {van den Heerik}, A.V.M. and {van Amsterdam}, K. and Lichtenbelt, B.J. and Scheeren, T.W.L. and Absalom, A.R. and Struys, M M R F},
  year = {2017},
  month = nov,
  volume = {119},
  pages = {918--927},
  journal = {British Journal of Anaesthesia},
  language = {en},
  number = {5}
}

@article{vanderwerken_parallel_2013,
  title = {Parallel {{Markov Chain Monte Carlo}}},
  author = {VanDerwerken, Douglas N. and Schmidler, Scott C.},
  year = {2013},
  month = dec,
  abstract = {Markov chain Monte Carlo is an inherently serial algorithm. Although likelihood calculations for individual steps can sometimes be parallelized, the serial evolution of the process is widely viewed as incompatible with parallelization, offering no speedup for samplers which require large numbers of iterations to converge to equilibrium. We provide a methodology for parallelizing Markov chain Monte Carlo across large numbers of independent, asynchronous processors. Our approach uses a partitioning and weight estimation scheme to combine independent simulations run on separate processors into rigorous Monte Carlo estimates. The method is originally motivated by sampling multimodal target distributions, where we see an exponential speedup in running time. However we show that the approach is general-purpose and applicable to all Markov chain Monte Carlo simulations, and demonstrate speedups proportional to the number of available processors on slowly mixing chains with unimodal target distributions. The approach is simple and easy to implement, and suggests additional directions for further research.},
  archiveprefix = {arXiv},
  eprint = {1312.7479},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/B3WFTVDR/VanDerwerken and Schmidler - 2013 - Parallel Markov Chain Monte Carlo.pdf;/home/msca8h/Zotero/storage/ZMI8GW89/1312.html},
  journal = {ArXiv13127479 Stat},
  keywords = {Statistics - Computation},
  primaryclass = {stat}
}

@article{vanli_global_2018,
  title = {Global {{Convergence Rate}} of {{Proximal Incremental Aggregated Gradient Methods}}},
  author = {Vanli, N. D. and G{\"u}rb{\"u}zbalaban, M. and Ozdaglar, A.},
  year = {2018},
  month = jan,
  volume = {28},
  pages = {1282--1300},
  file = {/home/msca8h/Zotero/storage/VWMLH6PX/Vanli et al. - 2018 - Global Convergence Rate of Proximal Incremental Ag.pdf},
  journal = {SIAM J. Optim.},
  language = {en},
  number = {2}
}

@article{vanwerkhoven_kernel_2019,
  title = {Kernel {{Tuner}}: {{A}} Search-Optimizing {{GPU}} Code Auto-Tuner},
  shorttitle = {Kernel {{Tuner}}},
  author = {{van Werkhoven}, Ben},
  year = {2019},
  month = jan,
  volume = {90},
  pages = {347--358},
  file = {/home/msca8h/Zotero/storage/XG4X8N6W/van Werkhoven - 2019 - Kernel Tuner A search-optimizing GPU code auto-tu.pdf},
  journal = {Future Generation Computer Systems},
  language = {en}
}

@article{varsi_single_2019,
  title = {A {{Single SMC Sampler}} on {{MPI}} That {{Outperforms}} a {{Single MCMC Sampler}}},
  author = {Varsi, Alessandro and Kekempanos, Lykourgos and Thiyagalingam, Jeyarajan and Maskell, Simon},
  year = {2019},
  month = may,
  abstract = {Markov Chain Monte Carlo (MCMC) is a well-established family of algorithms which are primarily used in Bayesian statistics to sample from a target distribution when direct sampling is challenging. Single instances of MCMC methods are widely considered hard to parallelise in a problem-agnostic fashion and hence, unsuitable to meet both constraints of high accuracy and high throughput. Sequential Monte Carlo (SMC) Samplers can address the same problem, but are parallelisable: they share with Particle Filters the same key tasks and bottleneck. Although a rich literature already exists on MCMC methods, SMC Samplers are relatively underexplored, such that no parallel implementation is currently available. In this paper, we first propose a parallel MPI version of the SMC Sampler, including an optimised implementation of the bottleneck, and then compare it with single-core Metropolis-Hastings. The goal is to show that SMC Samplers may be a promising alternative to MCMC methods with high potential for future improvements. We demonstrate that a basic SMC Sampler with 512 cores is up to 85 times faster or up to 8 times more accurate than Metropolis-Hastings.},
  archiveprefix = {arXiv},
  eprint = {1905.10252},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/99AXQJU2/Varsi et al. - 2019 - A Single SMC Sampler on MPI that Outperforms a Sin.pdf;/home/msca8h/Zotero/storage/9HC9E6A6/1905.html},
  journal = {arXiv:1905.10252 [cs, stat]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Statistics - Computation}
}

@inproceedings{vegas-sanchez-ferrero_probabilisticdriven_2010,
  title = {Probabilistic-{{Driven Oriented Speckle Reducing Anisotropic Diffusion}} with {{Application}} to {{Cardiac Ultrasonic Images}}},
  booktitle = {Med. {{Image Comput}}. {{Comput}}.-{{Assisted Intervention}}},
  author = {{Vegas-Sanchez-Ferrero}, G. and {Aja-Fernandez}, S. and {Martin-Fernandez}, M. and Frangi, A. F. and Palencia, C.},
  year = {2010},
  volume = {6361},
  pages = {518--525},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Zotero/storage/7FCQGGNX/Hutchison et al. - 2010 - Probabilistic-Driven Oriented Speckle Reducing Ani.pdf},
  series = {{{MICCAI}}'10}
}

@article{vehtari_pareto_2021,
  title = {Pareto Smoothed Importance Sampling},
  author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
  year = {2021},
  month = feb,
  abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
  archiveprefix = {arXiv},
  eprint = {1507.02646},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/W9HIAV98/Vehtari et al. - 2021 - Pareto Smoothed Importance Sampling.pdf;/home/msca8h/Zotero/storage/L4Z2DYIR/1507.html},
  journal = {ArXiv150702646 Stat},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {stat}
}

@article{vehtari_ranknormalization_2020,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R}}-Hat for {{Assessing Convergence}} of {{MCMC}}},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2020},
  month = jul,
  file = {/home/msca8h/Zotero/storage/PNJJNDU4/Vehtari et al. - 2020 - Rank-Normalization, Folding, and Localization An .pdf},
  journal = {Bayesian Anal.},
  language = {en}
}

@article{vu_bf_2012,
  title = {\$\{\textbackslash bf \vphantom\}{{S}}\vphantom\{\}\_\{3\}\$: {{A Spectral}} and {{Spatial Measure}} of {{Local Perceived Sharpness}} in {{Natural Images}}},
  shorttitle = {\$\{\textbackslash bf \vphantom\}{{S}}\vphantom\{\}\_\{3\}\$},
  author = {Vu, C. T. and Phan, T. D. and Chandler, D. M.},
  year = {2012},
  month = mar,
  volume = {21},
  pages = {934--945},
  journal = {IEEE Trans. on Image Process.},
  number = {3}
}

@article{Vuduc_2005,
  title = {{{OSKI}}: {{A}} Library of Automatically Tuned Sparse Matrix Kernels},
  author = {Vuduc, Richard and Demmel, James W and Yelick, Katherine A},
  year = {2005},
  month = jan,
  volume = {16},
  pages = {521--530},
  publisher = {{IOP Publishing}},
  abstract = {The Optimized Sparse Kernel Interface (OSKI) is a collection of low-level primitives that provide automatically tuned computational kernels on sparse matrices, for use by solver libraries and applications. These kernels include sparse matrix-vector multiply and sparse triangular solve, among others. The primary aim of this interface is to hide the complex decisionmaking process needed to tune the performance of a kernel implementation for a particular user's sparse matrix and machine, while also exposing the steps and potentially non-trivial costs of tuning at run-time. This paper provides an overview of OSKI, which is based on our research on automatically tuned sparse kernels for modern cache-based superscalar machines.},
  journal = {J. Phys. Conf. Ser.}
}

@inproceedings{vuylsteke_multiscale_1994,
  title = {Multiscale Image Contrast Amplification ({{MUSICA}})},
  booktitle = {Proc. {{SPIE Med}}. {{Imag}}.},
  author = {Vuylsteke, Pieter and Schoeters, Emile P.},
  year = {1994},
  month = may,
  pages = {551--560},
  address = {{Newport Beach, CA}}
}

@article{wai_decentralized_2017,
  title = {Decentralized {{Frank}}\textendash{{Wolfe Algorithm}} for {{Convex}} and {{Nonconvex Problems}}},
  author = {Wai, Hoi-To and Lafond, Jean and Scaglione, Anna and Moulines, Eric},
  year = {2017},
  month = nov,
  volume = {62},
  pages = {5522--5537},
  file = {/home/msca8h/Zotero/storage/65NRNHVF/07883821.pdf;/home/msca8h/Zotero/storage/D3FG2APN/Wai et al. - 2017 - Decentralized Frank–Wolfe Algorithm for Convex and.pdf},
  journal = {IEEE Trans. Automat. Contr.},
  number = {11}
}

@inproceedings{wang_adaptive_2013,
  title = {Adaptive {{Hamiltonian}} and {{Riemann Manifold Monte Carlo Samplers}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Ziyu and Mohamed, Shakir and De Freitas, Nando},
  year = {2013},
  volume = {28},
  pages = {1462--1470},
  publisher = {{ML Research Press}},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{wang_adaptive_2018,
  title = {Adaptive {{Communication Strategies}} to {{Achieve}} the {{Best Error}}-{{Runtime Trade}}-off in {{Local}}-{{Update SGD}}},
  author = {Wang, Jianyu and Joshi, Gauri},
  year = {2018},
  month = oct,
  abstract = {Large-scale machine learning training, in particular distributed stochastic gradient descent, needs to be robust to inherent system variability such as node straggling and random communication delays. This work considers a distributed training framework where each worker node is allowed to perform local model updates and the resulting models are averaged periodically. We analyze the true speed of error convergence with respect to wall-clock time (instead of the number of iterations), and analyze how it is affected by the frequency of averaging. The main contribution is the design of AdaComm, an adaptive communication strategy that starts with infrequent averaging to save communication delay and improve convergence speed, and then increases the communication frequency in order to achieve a low error floor. Rigorous experiments on training deep neural networks show that AdaComm can take \$3 \textbackslash times\$ less time than fully synchronous SGD, and still reach the same final training loss.},
  archiveprefix = {arXiv},
  eprint = {1810.08313},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/RZ9YXSA9/Wang and Joshi - 2018 - Adaptive Communication Strategies to Achieve the B.pdf;/home/msca8h/Zotero/storage/XJQVJI9R/1810.html},
  journal = {ArXiv181008313 Cs Stat},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{wang_batched_2017,
  title = {Batched {{Large}}-Scale {{Bayesian Optimization}} in {{High}}-Dimensional {{Spaces}}},
  author = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
  year = {2017},
  month = jun,
  abstract = {Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation.},
  archiveprefix = {arXiv},
  eprint = {1706.01445},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/P8P5YF5N/Wang et al. - 2017 - Batched Large-scale Bayesian Optimization in High-.pdf;/home/msca8h/Zotero/storage/68FBBWGZ/1706.html},
  journal = {ArXiv170601445 Cs Math Stat},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@article{wang_blessings_2019,
  title = {The {{Blessings}} of {{Multiple Causes}}},
  author = {Wang, Yixin and Blei, David M.},
  year = {2019},
  month = apr,
  abstract = {Causal inference from observational data often assumes "ignorability," that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects.},
  archiveprefix = {arXiv},
  eprint = {1805.06826},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/67QM9H9R/Wang and Blei - 2019 - The Blessings of Multiple Causes.pdf;/home/msca8h/Zotero/storage/FI92HUZU/1805.html},
  journal = {ArXiv180506826 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {cs, stat}
}

@article{wang_exact_2019,
  title = {Exact {{Gaussian Processes}} on a {{Million Data Points}}},
  author = {Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R. and Tyree, Stephen and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
  year = {2019},
  month = mar,
  abstract = {Gaussian processes (GPs) are flexible models with state-of-the-art performance on many impactful applications. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points in 3 days using 8 GPUs and can compute predictive means and variances in under a second using 1 GPU at test time. Moreover, we perform the first-ever comparison of exact GPs against state-of-the-art scalable approximations on large-scale regression datasets with \$10\^4-10\^6\$ data points, showing dramatic performance improvements.},
  archiveprefix = {arXiv},
  eprint = {1903.08114},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/HIJJEKGP/Wang et al. - 2019 - Exact Gaussian Processes on a Million Data Points.pdf;/home/msca8h/Zotero/storage/XVCBFY6N/1903.html},
  journal = {ArXiv190308114 Cs Stat},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{wang_exact_2020,
  title = {Exact Convergence Rate Analysis of the Independent {{Metropolis}}-{{Hastings}} Algorithms},
  author = {Wang, Guanyang},
  year = {2020},
  month = dec,
  abstract = {A well-known difficult problem regarding Metropolis-Hastings algorithms is to get sharp bounds on their convergence rates. Moreover, a fundamental but often overlooked problem in Markov chain theory is to study the convergence rates for different initializations. In this paper, we study the two issues mentioned above of the Independent Metropolis-Hastings (IMH) algorithms on both general and discrete state spaces. We derive the exact convergence rate and prove that the IMH algorithm's different deterministic initializations have the same convergence rate. Surprisingly, under mild conditions, we get the exact convergence speed for IMH algorithms on general state spaces, which is the first `exact convergence' result for general state space MCMC algorithms to the author's best knowledge. Connections with the Random Walk Metropolis-Hastings (RWMH) algorithm are also discussed, which solve a conjecture proposed by Atchad\textbackslash '\{e\} and Perron \textbackslash cite\{atchade2007geometric\} using a counterexample.},
  archiveprefix = {arXiv},
  eprint = {2008.02455},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/5U5TFW55/Wang - 2020 - Exact Convergence Rate Analysis of the Independent.pdf;/home/msca8h/Zotero/storage/4F26VXU9/2008.html},
  journal = {ArXiv200802455 Math Stat},
  keywords = {Mathematics - Probability,Mathematics - Spectral Theory,Mathematics - Statistics Theory,Statistics - Computation},
  primaryclass = {math, stat}
}

@article{wang_image_2004,
  title = {Image {{Quality Assessment}}: {{From Error Visibility}} to {{Structural Similarity}}},
  shorttitle = {Image {{Quality Assessment}}},
  author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  volume = {13},
  pages = {600--612},
  journal = {IEEE Trans. Image Process.},
  language = {en},
  number = {4}
}

@article{wang_image_2004a,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image {{Quality Assessment}}},
  author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  volume = {13},
  pages = {600--612},
  journal = {IEEE Trans. Image Process.},
  language = {en},
  number = {4}
}

@article{wang_image_2013,
  title = {Image Denoising Using Modified {{Perona}}\textendash{{Malik}} Model Based on Directional {{Laplacian}}},
  author = {Wang, Y.Q. and Guo, Jichang and Chen, Wufan and Zhang, Wenxue},
  year = {2013},
  month = sep,
  volume = {93},
  pages = {2548--2558},
  journal = {Signal Process.},
  language = {en},
  number = {9}
}

@article{wang_integrating_2014,
  title = {Integrating Profile-Driven Parallelism Detection and Machine-Learning-Based Mapping},
  author = {Wang, Zheng and Tournavitis, Georgios and Franke, Bj{\"o}rn and O'boyle, Michael F. P.},
  year = {2014},
  month = feb,
  volume = {11},
  pages = {1--26},
  file = {/home/msca8h/Zotero/storage/VMZ8J7F2/Wang et al. - 2014 - Integrating profile-driven parallelism detection a.pdf},
  journal = {ACM Trans. Archit. Code Optim.},
  language = {en},
  number = {1}
}

@incollection{wang_knowledgebased_2012,
  title = {Knowledge-{{Based Adaptive Self}}-{{Scheduling}}},
  booktitle = {Network and {{Parallel Computing}}},
  author = {Wang, Yizhuo and Ji, Weixing and Shi, Feng and Zuo, Qi and Deng, Ning},
  year = {2012},
  volume = {7513},
  pages = {22--32},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Zotero/storage/S7GHK23E/Wang et al. - 2012 - Knowledge-Based Adaptive Self-Scheduling.pdf}
}

@inproceedings{wang_mapping_2009,
  title = {Mapping Parallelism to Multi-Cores: A Machine Learning Based Approach},
  booktitle = {Proc. 14th {{ACM SIGPLAN Symp}}. {{Princ}}. {{Pract}}. {{Parallel Program}}.},
  author = {Wang, Zheng and O'Boyle, Michael F.P.},
  year = {2009},
  pages = {75--84},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  file = {/home/msca8h/Documents/parallel_scheduling/ML_scheduling_strategy.pdf},
  keywords = {artificial neural networks,compiler optimization,machine learning,performance modeling,support vector machine},
  series = {{{PPoPP}}'09}
}

@inproceedings{wang_marginalized_2012,
  title = {A {{Marginalized Particle Gaussian Process Regression}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems}} ({{NIPS}}'12)},
  author = {Wang, Yali and {Chaib-draa}, Brahim},
  year = {2012},
  pages = {1187--1195}
}

@inproceedings{wang_maxvalue_2017,
  title = {Max-Value Entropy Search for Efficient {{Bayesian}} Optimization},
  booktitle = {Proc. 34th {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2017},
  volume = {70},
  pages = {3627--3635},
  publisher = {{JMLR.org}},
  file = {/home/msca8h/Zotero/storage/GGESBYZE/Wang and Jegelka - 2017 - Max-value Entropy Search for Efficient Bayesian Op.pdf},
  series = {{{ICML}}'17}
}

@inproceedings{wang_maxvalue_2017a,
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2017},
  pages = {3627--3635},
  publisher = {{JMLR.org}},
  series = {{{ICML}}'17}
}

@inproceedings{wang_predicting_2016,
  title = {Predicting the Memory Bandwidth and Optimal Core Allocations for Multi-Threaded Applications on Large-Scale {{NUMA}} Machines},
  booktitle = {2016 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Wang, Wei and Davidson, Jack W. and Soffa, Mary Lou},
  year = {2016},
  month = mar,
  pages = {419--431},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}}
}

@inproceedings{wang_regret_2018,
  title = {Regret Bounds for Meta {{Bayesian}} Optimization with an Unknown {{Gaussian}} Process Prior},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Zi and Kim, Beomjoon and Kaelbling, Leslie Pack},
  year = {2018},
  pages = {10477--10488}
}

@article{weare_efficient_2007,
  title = {Efficient {{Monte Carlo}} Sampling by Parallel Marginalization},
  author = {Weare, J.},
  year = {2007},
  month = jul,
  volume = {104},
  pages = {12657--12662},
  file = {/home/msca8h/Zotero/storage/E8U3ZZCM/Weare - 2007 - Efficient Monte Carlo sampling by parallel margina.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {31}
}

@book{weickert_anisotropic_1998,
  title = {Anisotropic {{Diffusion}} in {{Image Processing}}},
  author = {Weickert, Joachim},
  year = {1998},
  publisher = {{Teubner-Verlag,}},
  address = {{Stuttgart, Germany}},
  series = {{{ECMI}}}
}

@article{wenzel_how_2020,
  title = {How {{Good}} Is the {{Bayes Posterior}} in {{Deep Neural Networks Really}}?},
  author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S. and {\'S}wi{\k{a}}tkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  year = {2020},
  month = feb,
  abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a "cold posterior" that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
  archiveprefix = {arXiv},
  eprint = {2002.02405},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/6Y6FM6E9/Wenzel et al. - 2020 - How Good is the Bayes Posterior in Deep Neural Net.pdf;/home/msca8h/Zotero/storage/QHGJS7NV/2002.html},
  journal = {ArXiv200202405 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@incollection{wilkinson_parallel_2005,
  title = {Parallel {{Bayesian}} Computation},
  booktitle = {Handbook of {{Parallel Computing}} and {{Statistics}} ({{Statistics}}, {{Textbooks}} and {{Monographs}})},
  author = {Wilkinson, Darren},
  year = {2005},
  publisher = {{Chapman \& Hall/CRC}},
  abstract = {The use of Bayesian inference for the analysis of complex statistical models has increased dramatically in recent years, in part due to the increasing availability of computing power. There are a range of techniques available for carrying out Bayesian inference, but the lack of analytic tractability for the vast majority of models of interest means that most of the techniques are numeric, and many are computationally demanding. Indeed, for high-dimensional nonlinear models, the only practical methods for analysis are based on Markov chain Monte Carlo (MCMC) techniques, and these are notoriously computation intensive, with some analyses requiring weeks of CPU time on powerful computers. It is clear therefore that the use of parallel computing technology in the context of Bayesian computation is of great interest to many who analyze complex models using Bayesian techniques. Of particular interest in the context of Bayesian inference are techniques for parallelization of a computation utilizing the conditional independence structure of the underlying model, as well as strategies for parallelization of Monte Carlo and MCMC algorithms. There are two obvious approaches to parallelization of an MCMC algorithm: one is based on the idea of running different chains in parallel, and the other is based on parallelization of a single chain. It is a subtle and problem-dependent question as to which of these strategies (or combination of the two) is likely to be most appropriate and there are also important issues relating to parallel random number generation which need to be addressed.}
}

@article{williams_bayesian_1998,
  title = {Bayesian Classification with {{Gaussian}} Processes},
  author = {Williams, Christopher K.I. and Barber, David},
  year = {Dec./1998},
  volume = {20},
  pages = {1342--1351},
  file = {/home/msca8h/Zotero/storage/R6R4FWHE/Williams and Barber - 1998 - Bayesian classification with Gaussian processes.pdf},
  journal = {IEEE Trans. Pattern Anal. Machine Intell.},
  number = {12}
}

@article{wilson_bayesian_2020,
  title = {Bayesian {{Deep Learning}} and a {{Probabilistic Perspective}} of {{Generalization}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  year = {2020},
  month = mar,
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
  archiveprefix = {arXiv},
  eprint = {2002.08791},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/C9ME5GTU/Wilson and Izmailov - 2020 - Bayesian Deep Learning and a Probabilistic Perspec.pdf;/home/msca8h/Zotero/storage/Q22FTK8E/2002.html},
  journal = {ArXiv200208791 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{wilson_case_2019,
  title = {The {{Case}} for {{Bayesian Deep Learning}}},
  author = {Wilson, Andrew Gordon},
  year = {2019},
  journal = {NYU Courant Tech. Rep.}
}

@inproceedings{wilson_maximizing_2018,
  title = {Maximizing Acquisition Functions for {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wilson, James and Hutter, Frank and Deisenroth, Marc},
  year = {2018},
  pages = {9884--9895}
}

@inproceedings{wilson_maximizing_2018a,
  title = {Maximizing {{Acquisition Functions}} for {{Bayesian Optimization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
  year = {2018},
  pages = {9906--9917},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  series = {{{NIPS}}'18}
}

@article{wolberg_multisurface_1990,
  title = {Multisurface Method of Pattern Separation for Medical Diagnosis Applied to Breast Cytology.},
  author = {Wolberg, W. H. and Mangasarian, O. L.},
  year = {1990},
  month = dec,
  volume = {87},
  pages = {9193--9196},
  file = {/home/msca8h/Zotero/storage/8W7JPKJV/Wolberg and Mangasarian - 1990 - Multisurface method of pattern separation for medi.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {23}
}

@article{wolstenhulme_agreement_2015,
  title = {Agreement between Objective and Subjective Assessment of Image Quality in Ultrasound Abdominal Aortic Aneurism Screening},
  author = {Wolstenhulme, S and Davies, A G and Keeble, C and Moore, S and Evans, J A},
  year = {2015},
  month = feb,
  volume = {88},
  pages = {20140482},
  file = {/home/msca8h/Zotero/storage/YLJGVLM5/Wolstenhulme et al. - 2015 - Agreement between objective and subjective assessm.pdf},
  journal = {BJR},
  language = {en},
  number = {1046}
}

@article{wong_monte_2012,
  title = {Monte {{Carlo}} Despeckling of Transrectal Ultrasound Images of the Prostate},
  author = {Wong, Alexander and Scharcanski, Jacob},
  year = {2012},
  month = sep,
  volume = {22},
  pages = {768--775},
  journal = {Digit. Signal Process.},
  language = {en},
  number = {5}
}

@inproceedings{wood-aistats-2014,
  title = {A New Approach to Probabilistic Programming Inference},
  booktitle = {Proc. 17th {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Wood, Frank and {van de Meent}, Jan Willem and Mansinghka, Vikash},
  year = {2014},
  pages = {1024--1032},
  series = {{{ICML}}'14}
}

@article{wu_parallel_2012,
  title = {Parallel {{Markov}} Chain {{Monte Carlo}} - Bridging the Gap to High-Performance {{Bayesian}} Computation in Animal Breeding and Genetics},
  author = {Wu, Xiao-Lin and Sun, Chuanyu and Beissinger, Timothy M and Rosa, Guilherme JM and Weigel, Kent A and Gatti, Natalia de Leon and Gianola, Daniel},
  year = {2012},
  month = dec,
  volume = {44},
  pages = {29},
  file = {/home/msca8h/Zotero/storage/7WNJB3YC/Wu et al. - 2012 - Parallel Markov chain Monte Carlo - bridging the g.pdf},
  journal = {Genet Sel Evol},
  language = {en},
  number = {1}
}

@article{xiaohuihao_novel_1999,
  title = {A Novel Multiscale Nonlinear Thresholding Method for Ultrasonic Speckle Suppressing},
  author = {{Xiaohui Hao} and {Shangkai Gao} and {Xiaorong Gao}},
  year = {Sept./1999},
  volume = {18},
  pages = {787--794},
  journal = {IEEE Trans. Med. Imaging},
  number = {9}
}

@inproceedings{xie_lightercommunication_2016,
  title = {Lighter-{{Communication Distributed Machine Learning}} via {{Sufficient Factor Broadcasting}}.},
  booktitle = {{{UAI}}},
  author = {Xie, Pengtao and Kim, Jin Kyu and Zhou, Yi and Ho, Qirong and Kumar, Abhimanu and Yu, Yaoliang and Xing, Eric P},
  year = {2016},
  file = {/home/msca8h/Zotero/storage/YPQM6P7H/Xie et al. - 2016 - Lighter-Communication Distributed Machine Learning.pdf}
}

@inproceedings{xu_empirical_2016,
  title = {An {{Empirical Study}} of {{ADMM}} for {{Nonconvex Problems}}},
  booktitle = {{{NIPS}} 2016 {{Workshop}} on {{Nonconvex Optimization}} for {{Machine Learning}}: {{Theory}} and {{Practice}}},
  author = {Xu, Zheng and De, Soham and Figueiredo, Mario and Studer, Christoph and Goldstein, Tom},
  year = {2016},
  month = dec,
  abstract = {The alternating direction method of multipliers (ADMM) is a common optimization tool for solving constrained and non-differentiable problems. We provide an empirical study of the practical performance of ADMM on several nonconvex applications, including l0 regularized linear regression, l0 regularized image denoising, phase retrieval, and eigenvector computation. Our experiments suggest that ADMM performs well on a broad class of non-convex problems. Moreover, recently proposed adaptive ADMM methods, which automatically tune penalty parameters as the method runs, can improve algorithm efficiency and solution quality compared to ADMM with a non-tuned penalty.},
  archiveprefix = {arXiv},
  eprint = {1612.03349},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/PYJYBFR3/Xu et al. - 2016 - An Empirical Study of ADMM for Nonconvex Problems.pdf;/home/msca8h/Zotero/storage/HA444VF8/1612.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@article{xue_doubleparallel_2019,
  title = {Double-{{Parallel Monte Carlo}} for {{Bayesian}} Analysis of Big Data},
  author = {Xue, Jingnan and Liang, Faming},
  year = {2019},
  month = jan,
  volume = {29},
  pages = {23--32},
  file = {/home/msca8h/Zotero/storage/X8G24AK4/Xue and Liang - 2019 - Double-Parallel Monte Carlo for Bayesian analysis .pdf},
  journal = {Stat Comput},
  language = {en},
  number = {1}
}

@article{xulizong_speckle_1998,
  title = {Speckle Reduction and Contrast Enhancement of Echocardiograms via Multiscale Nonlinear Processing},
  author = {{Xuli Zong} and Laine, A.F. and Geiser, E.A.},
  year = {Aug./1998},
  volume = {17},
  pages = {532--540},
  file = {/home/msca8h/Zotero/storage/9VABFKRJ/Xuli Zong et al. - 1998 - Speckle reduction and contrast enhancement of echo.pdf},
  journal = {IEEE Trans. Med. Imaging},
  number = {4}
}

@article{yan_bayesian_2018,
  title = {Bayesian {{Optimization Based}} on {{K}}-{{Optimality}}},
  author = {Yan, Liang and Duan, Xiaojun and Liu, Bowen and Xu, Jin},
  year = {2018},
  volume = {20},
  pages = {594},
  journal = {Entropy},
  number = {8}
}

@inproceedings{yan_stabilizing_2020,
  title = {Towards {{Stabilizing Batch Statistics}} in {{Backward Propagation}} of {{Batch Normalization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yan, Junjie and Wan, Ruosi and Zhang, Xiangyu and Zhang, Wei and Wei, Yichen and Sun, Jian},
  year = {2020}
}

@article{yang_parallelizable_2018,
  title = {On Parallelizable {{Markov}} Chain {{Monte Carlo}} Algorithms with Waste-Recycling},
  author = {Yang, Shihao and Chen, Yang and Bernton, Espen and Liu, Jun S.},
  year = {2018},
  month = sep,
  volume = {28},
  pages = {1073--1081},
  journal = {Stat Comput},
  language = {en},
  number = {5}
}

@inproceedings{yangyang_rumr_2003,
  title = {{{RUMR}}: Robust Scheduling for Divisible Workloads},
  booktitle = {High {{Performance Distributed Computing}}, 2003. {{Proceedings}}. 12th {{IEEE International Symposium}} On},
  author = {{Yang Yang} and Casanova, H.},
  year = {2003},
  month = jun,
  pages = {114--123},
  keywords = {Application software,Clustering algorithms,computer networks,Computer science,Delay,Dictionaries,factoring-based scheduling,Image segmentation,multiround divisible workload scheduling,parallel algorithms,performance evaluation,performance prediction errors,prediction theory,processor scheduling,Processor scheduling,real-world applications,real-world performance,robust scheduling,robust uniform multiround,Robustness,RUMR,scheduling algorithm,Signal processing algorithms,Supercomputers}
}

@inproceedings{ye_active_2014,
  title = {Active {{Sampling}} for {{Subjective Image Quality Assessment}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ye, Peng and Doermann, David},
  year = {2014},
  month = jun,
  pages = {4249--4256},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}}
}

@article{yilmaz_autotuning_2016,
  title = {Autotuning {{Runtime Specialization}} for {{Sparse Matrix}}-{{Vector Multiplication}}},
  author = {Yilmaz, Buse and Aktemur, Bari{\c s} and Garzar{\'a}n, Mar{\'i}A J. and Kamin, Sam and Kira{\c c}, Furkan},
  year = {2016},
  month = mar,
  volume = {13},
  pages = {1--26},
  file = {/home/msca8h/Zotero/storage/WMYGGYEJ/Yilmaz et al. - 2016 - Autotuning Runtime Specialization for Sparse Matri.pdf},
  journal = {ACM Trans. Archit. Code Optim.},
  language = {en},
  number = {1}
}

@inproceedings{yongjianyu_generalized_2004,
  title = {Generalized Speckle Reducing Anisotropic Diffusion for Ultrasound Imagery},
  booktitle = {Proc. {{IEEE Symp}}. {{Comput}}.-{{Based Med}}. {{Syst}}.},
  author = {{Yongjian Yu} and Molloy, J.A. and Acton, S.T.},
  year = {2004},
  pages = {279--284},
  publisher = {{IEEE}},
  address = {{Bethesda, MD, USA}},
  series = {{{CBMS}}'04}
}

@article{yongjianyu_speckle_2002,
  title = {Speckle Reducing Anisotropic Diffusion},
  author = {{Yongjian Yu} and Acton, S.T.},
  year = {2002},
  month = nov,
  volume = {11},
  pages = {1260--1270},
  file = {/home/msca8h/Zotero/storage/LQYNIKXQ/Yongjian Yu and Acton - 2002 - Speckle reducing anisotropic diffusion.pdf},
  journal = {IEEE Trans. Image Process.},
  language = {en},
  number = {11}
}

@article{yongyue_nonlinear_2006,
  title = {Nonlinear Multiscale Wavelet Diffusion for Speckle Suppression and Edge Enhancement in Ultrasound Images},
  author = {{Yong Yue} and Croitoru, M.M. and Bidani, A. and Zwischenberger, J.B. and Clark, J.W.},
  year = {2006},
  month = mar,
  volume = {25},
  pages = {297--311},
  journal = {IEEE Trans. Med. Imaging},
  number = {3}
}

@inproceedings{you_imagenet_2018,
  title = {{{ImageNet Training}} in {{Minutes}}},
  booktitle = {Proceedings of the 47th {{International Conference}} on {{Parallel Processing}}  - {{ICPP}} 2018},
  author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  year = {2018},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{Eugene, OR, USA}},
  file = {/home/msca8h/Zotero/storage/SUGW4MTR/You et al. - 2018 - ImageNet Training in Minutes.pdf},
  language = {en}
}

@inproceedings{yu_parallel_2019,
  title = {Parallel {{Asynchronous Stochastic Coordinate Descent}} with {{Auxiliary Variables}}},
  booktitle = {Proceedings of {{Machine Learning Research}}},
  author = {Yu, Hsiang-Fu and Hsieh, Cho-Jui and Dhillon, Inderjit S.},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {2641--2649},
  publisher = {{PMLR}},
  abstract = {The key to the recent success of coordinate descent (CD) in many applications is to maintain a set of auxiliary variables to facilitate efficient single variable updates. For example, the vector of residual/primal variables has to be maintained when CD is applied for Lasso/linear SVM, respectively. An implementation without maintenance is \$O(n)\$ times slower than the one with maintenance, where n is the number of variables. In serial implementations, maintaining auxiliary variables is only a computing trick without changing the behavior of coordinate descent. However, maintenance of auxiliary variables is non-trivial when there are multiple threads/workers which read/write the auxiliary variables concurrently. Thus, most existing theoretical analysis of parallel CD either assumes vanilla CD without auxiliary variables (which ends up being extremely slow in practice) or limits to a small class of problems. In this paper, we consider a rich family of objective functions where AUX-PCD can be applied. We also establish global linear convergence for AUX-PCD with atomic operations for a general family of functions and perform a complete backward error analysis of AUX-PCD with wild updates, where some updates are not just delayed but lost because of memory conflicts. Our results enable us to provide theoretical guarantees for many practical parallel coordinate descent implementations, which currently lack guarantees (such as the implementation of Shotgun by Bradley et al. 2011, which uses auxiliary variables)},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{yu_stochastic_2017,
  title = {Stochastic {{Variational Inference}} for {{Bayesian Sparse Gaussian Process Regression}}},
  author = {Yu, Haibin and Hoang, Trong Nghia and Low, Kian Hsiang and Jaillet, Patrick},
  year = {2017},
  month = nov,
  abstract = {This paper presents a novel variational inference framework for deriving a family of Bayesian sparse Gaussian process regression (SGPR) models whose approximations are variationally optimal with respect to the full-rank GPR model enriched with various corresponding correlation structures of the observation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat both the distributions of the inducing variables and hyperparameters as variational parameters, which enables the decomposability of the variational lower bound that in turn can be exploited for stochastic optimization. Such a stochastic optimization involves iteratively following the stochastic gradient of the variational lower bound to improve its estimates of the optimal variational distributions of the inducing variables and hyperparameters (and hence the predictive distribution) of our VBSGPR models and is guaranteed to achieve asymptotic convergence to them. We show that the stochastic gradient is an unbiased estimator of the exact gradient and can be computed in constant time per iteration, hence achieving scalability to big data. We empirically evaluate the performance of our proposed framework on two real-world, massive datasets.},
  archiveprefix = {arXiv},
  eprint = {1711.00221},
  eprinttype = {arxiv},
  file = {/home/msca8h/Zotero/storage/3QZVLUIV/Yu et al. - 2017 - Stochastic Variational Inference for Bayesian Spar.pdf;/home/msca8h/Zotero/storage/63GBFM8G/1711.html},
  journal = {ArXiv171100221 Cs Stat},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{yu_ultrasound_2010,
  title = {Ultrasound Speckle Reduction by a {{SUSAN}}-Controlled Anisotropic Diffusion Method},
  author = {Yu, Jinhua and Tan, Jinglu and Wang, Yuanyuan},
  year = {2010},
  month = sep,
  volume = {43},
  pages = {3083--3092},
  journal = {Pattern Recognition},
  language = {en},
  number = {9}
}

@incollection{yue_designing_1997,
  title = {Designing {{Multiprocessor Scheduling Algorithms Using}} a {{Distributed Genetic Algorithm System}}},
  booktitle = {Evolutionary {{Algorithms}} in {{Engineering Applications}}},
  author = {Yue, Kelvin K. and Lilja, David J.},
  year = {1997},
  pages = {207--222},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  language = {en}
}

@article{yue_parallel_1995,
  title = {Parallel Loop Scheduling for High Performance Computers},
  author = {Yue, Kelvin K. and Lilja, David J.},
  year = {1995},
  volume = {10},
  pages = {243--264},
  file = {/home/msca8h/Documents/parallel_scheduling/Yue and Lilja - 1995 - Parallel Loop Scheduling for High Performance Comp.pdf},
  journal = {Adv. Parallel Comput.},
  keywords = {Analytical modeling,Parallel loop scheduling,Performance analysis,Scalability,Shared memory multiprocessor}
}

@article{zeng_nonconvex_2018,
  title = {On {{Nonconvex Decentralized Gradient Descent}}},
  author = {Zeng, Jinshan and Yin, Wotao},
  year = {2018},
  month = jun,
  volume = {66},
  pages = {2834--2848},
  file = {/home/msca8h/Zotero/storage/FZVRSM9D/Zeng and Yin - 2018 - On Nonconvex Decentralized Gradient Descent.pdf},
  journal = {IEEE Trans. Signal Process.},
  number = {11}
}

@article{zhang_advances_2019,
  title = {Advances in Variational Inference},
  author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
  year = {2019},
  month = aug,
  volume = {41},
  pages = {2008--2026},
  file = {/home/msca8h/Zotero/storage/TVNGVPC9/Zhang et al. - 2019 - Advances in Variational Inference.pdf},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  number = {8}
}

@article{zhang_autogeneration_2013,
  title = {Autogeneration and {{Autotuning}} of {{3D Stencil Codes}} on {{Homogeneous}} and {{Heterogeneous GPU Clusters}}},
  author = {Zhang, Yongpeng and Mueller, Frank},
  year = {2013},
  month = mar,
  volume = {24},
  pages = {417--427},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {3}
}

@article{zhang_clutter_2020,
  title = {Clutter Suppression in Ultrasound: Performance Evaluation and Review of Low-Rank and Sparse Matrix Decomposition Methods},
  shorttitle = {Clutter Suppression in Ultrasound},
  author = {Zhang, Naiyuan and Ashikuzzaman, Md and Rivaz, Hassan},
  year = {2020},
  month = may,
  volume = {19},
  pages = {37},
  abstract = {Vessel diseases are often accompanied by abnormalities related to vascular shape and size. Therefore, a clear visualization of vasculature is of high clinical significance. Ultrasound color flow imaging (CFI) is one of the prominent techniques for flow visualization. However, clutter signals originating from slow-moving tissue are one of the main obstacles to obtain a clear view of the vascular network. Enhancement of the vasculature by suppressing the clutters is a significant and irreplaceable step for many applications of ultrasound CFI. Currently, this task is often performed by singular value decomposition (SVD) of the data matrix. This approach exhibits two well-known limitations. First, the performance of SVD is sensitive to the proper manual selection of the ranks corresponding to clutter and blood subspaces. Second, SVD is prone to failure in the presence of large random noise in the dataset. A potential solution to these issues is using decomposition into low-rank and sparse matrices (DLSM) framework. SVD is one of the algorithms for solving the minimization problem under the DLSM framework. Many other algorithms under DLSM avoid full SVD and use approximated SVD or SVD-free ideas which may have better performance with higher robustness and less computing time. In practice, these models separate blood from clutter based on the assumption that steady clutter represents a low-rank structure and that the moving blood component is sparse. In this paper, we present a comprehensive review of ultrasound clutter suppression techniques and exploit the feasibility of low-rank and sparse decomposition schemes in ultrasound clutter suppression. We conduct this review study by adapting 106 DLSM algorithms and validating them against simulation, phantom, and in vivo rat datasets. Two conventional quality metrics, signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR), are used for performance evaluation. In addition, computation times required by different algorithms for generating clutter suppressed images are reported. Our extensive analysis shows that the DLSM framework can be successfully applied to ultrasound clutter suppression.},
  journal = {BioMedical Engineering OnLine},
  number = {1}
}

@inproceedings{zhang_cyclical_2020,
  title = {Cyclical {{Stochastic Gradient MCMC}} for {{Bayesian Deep Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
  year = {2020}
}

@inproceedings{zhang_extreme_2019,
  title = {Extreme {{Stochastic Variational Inference}}: {{Distributed Inference}} for {{Large Scale Mixture Models}}},
  booktitle = {Proceedings of {{Machine Learning Research}}},
  author = {Zhang, Jiong and Raman, Parameswaran and Ji, Shihao and Yu, Hsiang-Fu and Vishwanathan, S.V.N. and Dhillon, Inderjit},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {935--943},
  publisher = {{PMLR}},
  abstract = {Mixture of exponential family models are among the most fundamental and widely used statistical models. Stochastic variational inference (SVI), the state-of-the-art algorithm for parameter estimation in such models is inherently serial. Moreover, it requires the parameters to fit in the memory of a single processor; this poses serious limitations on scalability when the number of parameters is in billions. In this paper, we present extreme stochastic variational inference (ESVI), a distributed, asynchronous and lock-free algorithm to perform variational inference for mixture models on massive real world datasets. ESVI overcomes the limitations of SVI by requiring that each processor only access a subset of the data and a subset of the parameters, thus providing data and model parallelism simultaneously. Our empirical study demonstrates that ESVI not only outperforms VI and SVI in wallclock-time, but also achieves a better quality solution. To further speed up computation and save memory when fitting large number of topics, we propose a variant ESVI-TOPK which maintains only the top-k important topics. Empirically, we found that using top 25\% topics suffices to achieve the same accuracy as storing all the topics.},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{zhang_inconsistent_2004a,
  title = {Inconsistent Estimation and Asymptotically Equal Interpolations in Model-Based Geostatistics},
  author = {Zhang, Hao},
  year = {2004},
  month = mar,
  volume = {99},
  pages = {250--261},
  journal = {J. American Statist. Association},
  language = {en},
  number = {465}
}

@inproceedings{zhang_multiscale_2006,
  title = {Multiscale Nonlinear Diffusion and Shock Filter for Ultrasound Image Enhancement},
  booktitle = {Proc. {{IEEE Comput}}. {{Soc}}. {{Conf}}. {{Comput}}. {{Vision Pattern Recognit}}.},
  author = {Zhang, Fan and Yoo, Yang Mo and Kim, Yongmin and Zhang, Lichen and Koh, Liang Mong},
  year = {2006},
  volume = {2},
  pages = {1972--1977},
  publisher = {{IEEE}},
  address = {{New York, NY, USA}},
  series = {{{CVPR}}'06}
}

@article{zhang_nonlinear_2007,
  title = {Nonlinear {{Diffusion}} in {{Laplacian Pyramid Domain}} for {{Ultrasonic Speckle Reduction}}},
  author = {Zhang, Fan and Yoo, Yang Mo and Koh, Liang Mong and Kim, Yongmin},
  year = {2007},
  month = feb,
  volume = {26},
  pages = {200--211},
  journal = {IEEE Trans. Med. Imaging},
  number = {2}
}

@article{zhang_nonparametric_1996,
  title = {Nonparametric Importance Sampling},
  author = {Zhang, Ping},
  year = {1996},
  month = sep,
  volume = {91},
  pages = {1245--1253},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {435}
}

@article{zhang_optimizing_2020,
  title = {Optimizing {{Streaming Parallelism}} on {{Heterogeneous Many}}-{{Core Architectures}}},
  author = {Zhang, Peng and Fang, Jianbin and Yang, Canqun and Huang, Chun and Tang, Tao and Wang, Zheng},
  year = {2020},
  month = aug,
  volume = {31},
  pages = {1878--1896},
  file = {/home/msca8h/Zotero/storage/M4S6NCVQ/Zhang et al. - 2020 - Optimizing Streaming Parallelism on Heterogeneous .pdf},
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  number = {8}
}

@inproceedings{zhang_rectified_2015,
  title = {Rectified Linear Neural Networks with Tied-Scalar Regularization for {{LVCSR}}},
  booktitle = {{{INTERSPEECH}} 2015, 16th {{Annual Conference}} of the {{International Speech Communication Association}}, {{Dresden}}, {{Germany}}, {{September}} 6-10, 2015},
  author = {Zhang, Shiliang and Jiang, Hui and Wei, Si and Dai, Li-Rong},
  year = {2015},
  pages = {2635--2639},
  publisher = {{ISCA}}
}

@article{zhang_variational_2018,
  title = {Variational {{Hamiltonian Monte Carlo}} via Score Matching},
  author = {Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai},
  year = {2018},
  month = jun,
  volume = {13},
  file = {/home/msca8h/Zotero/storage/ZMJ29LGF/Zhang et al. - 2018 - Variational Hamiltonian Monte Carlo via Score Matc.pdf},
  journal = {Bayesian Anal.},
  number = {2}
}

@article{zhou_automatic_2016,
  title = {Toward {{Automatic Model Comparison}}: {{An Adaptive Sequential Monte Carlo Approach}}},
  shorttitle = {Toward {{Automatic Model Comparison}}},
  author = {Zhou, Yan and Johansen, Adam M. and Aston, John A.D.},
  year = {2016},
  month = jul,
  volume = {25},
  pages = {701--726},
  file = {/home/msca8h/Zotero/storage/CVLPSC38/Zhou et al. - 2016 - Toward Automatic Model Comparison An Adaptive Seq.pdf},
  journal = {J. Comput. Graphical Statist.},
  language = {en},
  number = {3}
}

@incollection{zhou_deconstructing_2019,
  title = {Deconstructing {{Lottery Tickets}}: {{Zeros}}, {{Signs}}, and the {{Supermask}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  year = {2019},
  pages = {3597--3607},
  publisher = {{Curran Associates, Inc.}}
}

@article{zhu_automatic_2020,
  title = {Automatic Multilabel Electrocardiogram Diagnosis of Heart Rhythm or Conduction Abnormalities with Deep Learning: A Cohort Study},
  shorttitle = {Automatic Multilabel Electrocardiogram Diagnosis of Heart Rhythm or Conduction Abnormalities with Deep Learning},
  author = {Zhu, Hongling and Cheng, Cheng and Yin, Hang and Li, Xingyi and Zuo, Ping and Ding, Jia and Lin, Fan and Wang, Jingyi and Zhou, Beitong and Li, Yonge and Hu, Shouxing and Xiong, Yulong and Wang, Binran and Wan, Guohua and Yang, Xiaoyun and Yuan, Ye},
  year = {2020},
  month = jul,
  volume = {2},
  pages = {e348-e357},
  file = {/home/msca8h/Zotero/storage/VGZHSA3B/Zhu et al. - 2020 - Automatic multilabel electrocardiogram diagnosis o.pdf},
  journal = {The Lancet Digital Health},
  language = {en},
  number = {7}
}

@article{zhu_big_2017,
  title = {Big {{Learning}} with {{Bayesian}} Methods},
  author = {Zhu, Jun and Chen, Jianfei and Hu, Wenbo and Zhang, Bo},
  year = {2017},
  month = jul,
  volume = {4},
  pages = {627--651},
  abstract = {Abstract             The explosive growth in data volume and the availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems and applications with Big Data. Bayesian methods represent one important class of statistical methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including non-parametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications. We also provide various new perspectives on the large-scale Bayesian modeling and inference.},
  file = {/home/msca8h/Zotero/storage/FGGZ9RHM/Zhu et al. - 2017 - Big Learning with Bayesian methods.pdf},
  journal = {Natl. Sci. Rev.},
  language = {en},
  number = {4}
}

@inproceedings{zhu_nonlocal_2017,
  title = {A Non-Local Low-Rank Framework for Ultrasound Speckle Reduction},
  booktitle = {{{IEEE Conf}}. {{Comput}}. {{Vision Pattern Recognit}}.},
  author = {Zhu, Lei and Fu, Chi-Wing and Brown, Michael S. and Heng, Pheng-Ann},
  year = {2017},
  month = jul,
  pages = {493--501},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  series = {{{CVPR}}'07}
}

@article{zierke_fpga_2010,
  title = {{{FPGA}} Acceleration of the Phylogenetic Likelihood Function for {{Bayesian MCMC}} Inference Methods},
  author = {Zierke, Stephanie and Bakos, Jason D},
  year = {2010},
  month = dec,
  volume = {11},
  pages = {184},
  file = {/home/msca8h/Zotero/storage/4FQYKFL8/Zierke and Bakos - 2010 - FPGA acceleration of the phylogenetic likelihood f.pdf},
  journal = {BMC Bioinformatics},
  language = {en},
  number = {1}
}

@inproceedings{zima_modelguided_2009,
  title = {Model-Guided Autotuning of High-Productivity Languages for Petascale Computing},
  booktitle = {Proceedings of the 18th {{ACM}} International Symposium on {{High}} Performance Distributed Computing - {{HPDC}} '09},
  author = {Zima, Hans and Hall, Mary and Chen, Chun and Chame, Jaqueline},
  year = {2009},
  pages = {151--166},
  publisher = {{ACM Press}},
  address = {{Garching, Germany}},
  language = {en}
}


